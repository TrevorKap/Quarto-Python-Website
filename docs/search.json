[
  {
    "objectID": "SpatialOptimization/p_median_with_pulp.html",
    "href": "SpatialOptimization/p_median_with_pulp.html",
    "title": "P-Median with PULP",
    "section": "",
    "text": "&lt;from https://soumenatta.medium.com/solving-the-p-median-problem-using-pulp-in-python-31d9bc13cc2d&gt;\nThe p-median problem is a well-known facility location problem that seeks to determine the optimal locations for a fixed number of facilities to serve a set of customers, with the goal of minimizing the total distance or cost of serving the customers.\nThis problem arises in many practical applications, such as logistics, transportation, and public services.\n\n\n\nimage.png\n\n\nIn this tutorial, we will present a Python implementation of the p-median problem using the PuLP modeling language and the branch-and-bound algorithm. PuLP is an open-source library for modeling and solving optimization problems in Python, and it provides a user-friendly syntax for defining decision variables, objective functions, and constraints.\nWe will begin by loading a sample dataset of the p-median problem from a text file and defining the problem using PuLP. We will then introduce the key components of the problem, such as the decision variables for selecting facility locations and the constraints for assigning customers to facilities.\nNext, we show how to solve the problem using the branch-and-bound algorithm, which is a powerful exact method for solving mixed-integer linear programs. We will discuss some important solver parameters, such as the gap tolerance and time limit, and demonstrate how to interpret the results of the optimization.\nBy the end of this article, readers will have a better understanding of how to use PuLP to model and solve facility location problems, and how to apply these techniques to their own practical applications.\n\nProblem Formulation\nThe p-median problem can be formulated as a mixed-integer linear program (MILP) as follows:\n\n\n\nimage.png\n\n\nwhere m is the number of customers, n is the number of potential facility locations, p is the number of facilities to be opened, cᵢⱼ is the cost of serving customer i from facility j, xᵢⱼ is a binary decision variable indicating whether customer i is served by facility j, and yⱼ is a binary decision variable indicating whether facility j is opened. The goal of the problem is to find p locations (facilities) among a set of n potential locations that minimize the total distance between each customer and the nearest facility. The first constraint set ensures that each customer is served by exactly one facility. The second constraint set ensures that if a customer is assigned to a facility, then the facility must be selected. The third constraint ensures that exactly p facilities are opened. The fourth and fifth constraints specify the domain of the decision variables.\n\n\nDataset\nIn this tutorial, we will use synthetic data for the p-median problem. The data file can be downloaded from the following link (https://gist.github.com/SoumenAtta/fc4fef0053a41dc0dd7eadae53a13df0)\n\nThe input data is in a text file name ‘pmedian_data.txt’. The text file contains three integers on the first line: m, n, and p. The first integer, m, represents the number of customers. The second integer, n, represents the number of potential facility locations. The third integer, p, represents the number of facilities we are required to locate. The remaining lines of the text file represent the cost of serving each customer from each potential location.\n\n\nImplementation using PuLP in Python\nNow, we will discuss the implementation of the above formulation of the p-median problem using PuLP in Python.\n\n!pip install pulp\nfrom pulp import *\n\nCollecting pulp\n  Downloading PuLP-2.8.0-py3-none-any.whl (17.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.7/17.7 MB 35.3 MB/s eta 0:00:00\nInstalling collected packages: pulp\nSuccessfully installed pulp-2.8.0\n\n\nStep 1: Import the required module\nThe statement from pulp import * imports all the classes and functions defined in the pulp library, allowing the user to create and solve optimization problems using the library’s objects and methods. This statement is typically used at the beginning of an optimization program to make all the pulp classes and functions available throughout the code.\nStep 2: Load input data\n\n# Load data\nwith open(\"pmedian_data.txt\") as file:\n    m, n, p = map(int, file.readline().split())\n    c = [[int(x) for x in file.readline().split()] for i in range(m)]\n\nThe above code snippet loads the problem data from the file pmedian_data.txt and stores it in the variables m, n, p, and c.\nThe file pmedian_data.txt presumably contains the following data on its first line: m, n, and pwhere m, n, and p are integers. These are the number of facilities, customers, and facilities to locate, respectively.\nThe remaining lines of the file contain the cost of serving each customer by each facility. The c variable is a two-dimensional list that stores these costs. Each row i of the c matrix represents a facility, and each column j represents a customer. Thus, c[i][j] is the cost of serving customer j by facility i.\nThe with statement opens the file pmedian_data.txt in read mode and automatically closes it once the block of code inside the with statement is executed. The map function reads the first line of the file, splits it into a list of strings, and converts each element to an integer. The resulting list of integers is assigned to the variables m, n, and p. The remaining lines of the file are then read and parsed into the two-dimensional list c using a list comprehension.\nStep 3: Define the problem\n\n# Define the problem\nprob = LpProblem(\"p-median\", LpMinimize)\n\nIn the given code, LpProblem is a function provided by the PuLP library that is used to define an optimization problem. The function takes two arguments:\nname (string): a user-defined name for the problem.\nsense (int): the optimization sense of the problem, which can be either LpMinimize or LpMaximize.\nIn the given code, a minimization problem is defined and named “p-median” using LpProblem. This creates an empty optimization problem that can be filled with decision variables, constraints, and an objective function.\nStep 4: Define decision variables\n\n# Define decision variables\nx = LpVariable.dicts(\"x\", [(i,j) for i in range(m) for j in range(n)], 0, 1, LpBinary)\ny = LpVariable.dicts(\"y\", [j for j in range(n)], 0, 1, LpBinary)\n\nIn the above code snippet, the decision variables are defined using the LpVariable.dicts method from the PuLP library. The first line defines a dictionary of binary decision variables x, where each variable represents whether a customer i is assigned to a facility j (i.e., whether facility j serves customer i). The dictionary is indexed by tuples (i,j) for all combinations of i and j, and each variable is initialized with a lower bound of 0 and an upper bound of 1, indicating that it can take on binary values only.\nThe second line defines another dictionary of binary decision variables y, where each variable represents whether a facility j is open or closed. The dictionary is indexed by integers from 0 to n-1, which represent the n facilities in the problem. Like x, each variable is initialized with a lower bound of 0 and an upper bound of 1.\nStep 5: Define objective function\n\n# Define objective function\nprob += lpSum([c[i][j] * x[(i,j)] for i in range(m) for j in range(n)])\n\n\nprob\n\nIn the above line of code, we are defining the objective function of our optimization problem. The objective function is the mathematical function that we want to minimize or maximize. In the p-median problem, we want to minimize the total cost of opening facilities and serving customers.\nThe objective function is defined using the lpSum() function, which takes a list of terms that make up the objective function. Each term is a product of a decision variable and a coefficient. In this case, the coefficient is the cost c[i][j] of serving customer i by facility j, and the decision variable is x[(i,j)], which is binary and takes a value of 1 if facility j serves customer i, and 0 otherwise.\nThe lpSum() function takes care of summing up all the terms in the list and returning the total value of the objective function. Finally, we add the objective function to our problem using the += operator.\nStep 6: Define constraints\n\n# Define constraints\nfor i in range(m):\n    prob += lpSum([x[(i,j)] for j in range(n)]) == 1\n\nfor i in range(m):\n    for j in range(n):\n        prob += x[(i,j)] &lt;= y[j]\n\nprob += lpSum([y[j] for j in range(n)]) == 2\n\n\nprob\n\nThese lines of code define the constraints that the problem must satisfy.\nThe first constraint, for i in range(m): prob += lpSum([x[(i,j)] for j in range(n)]) == 1, ensures that each customer is assigned to exactly one facility. It is expressed by summing over all facilities j and requiring that each customer i is served by exactly one of them.\nThe second constraint, for i in range(m): for j in range(n): prob += x[(i,j)] &lt;= y[j], ensures that if a facility is open, then the customers served by that facility should be assigned to it. This is expressed by saying that if x[i,j] = 1 (facility j serves customer i), then y[j] must also be 1 (facility j is open).\nThe third constraint, prob += lpSum([y[j] for j in range(n)]) == p, limits the number of facilities that can be open to p. It is expressed by summing over all facilities j and requiring that the total number of open facilities is exactly equal to p.\nTogether, these constraints ensure that the objective function is minimized subject to the conditions that each customer is served by exactly one facility, each facility can serve a limited number of customers, and the total number of open facilities is limited top.\nStep 7: Solve the problem\n\n# Solve the problem using branch-and-bound algorithm\nprob.solve(PULP_CBC_CMD(gapRel=0.0, threads=1, timeLimit=600))\n\n1\n\n\nIn this line, we are instructing PuLP to solve the optimization problem using the branch-and-bound algorithm provided by the PULP_CBC_CMD solver. We pass several parameters to the solver, including:\ngapRel=0.0: This sets the relative optimality gap tolerance to 0, meaning the solver will continue searching for the optimal solution until it finds the best possible solution within the given time limit.\nthreads=1: This specifies the number of threads the solver can use for parallel computing. Here, we set it to 1, meaning that the solver will run in single-threaded mode.\n\ntimeLimit=600: This sets the maximum time (in seconds) that the solver can use to find the solution. Here, we set it to 600 seconds (or 10 minutes). If the solver fails to find the optimal solution within the time limit, it will return the best solution it has found so far.\nNote that the solver options and parameters may differ depending on the specific solver being used.\nStep 8: Print the result\n\n# Print results\nprint(\"Optimal objective value:\", value(prob.objective))\nfor j in range(n):\n    if y[j].value() &gt; 0.5:\n        print(\"Facility\", j, \"is located.\")\n        for i in range(m):\n            if x[(i,j)].value() &gt; 0.5:\n                print(\"- Customer\", i, \"is served.\")\n\nOptimal objective value: 81.0\nFacility 3 is located.\n- Customer 2 is served.\n- Customer 4 is served.\n- Customer 5 is served.\n- Customer 6 is served.\n- Customer 9 is served.\n- Customer 10 is served.\n- Customer 13 is served.\n- Customer 14 is served.\n- Customer 17 is served.\n- Customer 18 is served.\n- Customer 21 is served.\n- Customer 23 is served.\n- Customer 24 is served.\n- Customer 25 is served.\n- Customer 28 is served.\nFacility 8 is located.\n- Customer 0 is served.\n- Customer 1 is served.\n- Customer 3 is served.\n- Customer 7 is served.\n- Customer 8 is served.\n- Customer 11 is served.\n- Customer 12 is served.\n- Customer 15 is served.\n- Customer 16 is served.\n- Customer 19 is served.\n- Customer 20 is served.\n- Customer 22 is served.\n- Customer 26 is served.\n- Customer 27 is served.\n- Customer 29 is served.\n\n\nAfter the problem is solved, this code block prints the results of the optimization.\nThe first line prints the optimal objective value, which is the minimum cost of opening p facilities and serving all the customers. The value() method is used to retrieve the optimal objective value from the LpProblem object prob.\nThen, for each facility, the code checks if it was opened or not. If the value of the binary variable y[j] is greater than 0.5, it means that the j-th facility was opened. For this facility, the code prints its index and the list of customers that are served by it. To do so, the code checks the value of the binary variable x[(i,j)] for all customers i. If the value is greater than 0.5, it means that the customer i is served by the j-th facility.\nOverall, this code block provides a way to interpret the results of the optimization problem and to understand which facilities are opened and which customers are served by them.\nThe output is shown below:\n\n\nConclusion\nIn this tutorial, we learned how to solve the p-median problem using Python and the PuLP library. The p-median problem is a classic optimization problem that involves finding the optimal location of p facilities to minimize the sum of distances between customers and facilities.\nWe first loaded the data from a text file and defined the problem, decision variables, objective function, and constraints using PuLP. We then used the branch-and-bound algorithm to solve the problem and obtain the optimal objective value. Finally, we printed the results, which included the optimal objective value and the locations of the facilities and customers.\nThis tutorial provides a useful example of how to apply optimization techniques to solve real-world problems using Python and PuLP. The concepts and techniques learned in this tutorial can be applied to other optimization problems in various fields, including operations research, engineering, and economics."
  },
  {
    "objectID": "SpatialOptimization/index.html",
    "href": "SpatialOptimization/index.html",
    "title": "Spatial Optimization",
    "section": "",
    "text": "Spatial Optimization\nThis section includes examples of spatial optimization. Each sub-section highlights different types of analyses and visualizations."
  },
  {
    "objectID": "SpatialOptimization/capacitatedPmedian.html",
    "href": "SpatialOptimization/capacitatedPmedian.html",
    "title": "Trevor Kapuvari Python Demonstrations",
    "section": "",
    "text": "############   capacitated P-median.py #######################\n## Goal:    Computes capacitated p-median on CMS dataset\n## Created: November 1 2016 / Revised November 7 2018\n## Author:  eric delmelle\n## Inputs:  demand nodes & actual demand\n##          schools & capacities\n## Outputs: CPLEX file\n###########################################################\n\nimport string, os,  math, time, sys\nimport arcpy\nfrom arcpy import env\narcpy.OverWriteOutput = True\n\ndef distance(x1, y1, x2, y2):\n    dx = x2 - x1\n    dy = y2 - y1\n    d = math.sqrt(dx**2 + dy**2)\n    return d\n\n#########################################################\n#### STEP 1 ######### INPUTS  ###########################\n#########################################################\n\ndemand = \"demand.shp\"\nschools = \"schools.shp\"\np = 16\nmaxCapacity = .75 #85% of its capacity\noutputFile = open('capacitatedPmedian.txt', 'w')\n\n# reading demand information\noriginFID=[]\npopInfo=[]\noriginX=[]\noriginY=[]\nfor ori in arcpy.SearchCursor(demand):\n    originFID.append(ori.getValue(\"FID\"))\n    popInfo.append(ori.getValue(\"Weight\"))\n\nfor row in arcpy.da.SearchCursor(demand, [\"SHAPE@XY\"]):\n    originX.append(row[0][0])\n    originY.append(row[0][1])\n\nprint(\"...origin information read....\")\n\n# reading destination information\ndestinationFID = []\ndestinationX=[]\ndestinationY=[]\ndestinationCapacity=[]\nfor dest in arcpy.SearchCursor(schools):\n    destinationFID.append(dest.getValue(\"FID\"))\n    destinationCapacity.append(dest.getValue(\"Capacity\"))\n\nfor row in arcpy.da.SearchCursor(schools, [\"SHAPE@XY\"]):\n    destinationX.append(row[0][0])\n    destinationY.append(row[0][1])\n\nprint(\"...destination information read....\")\n\n\nOD = []\ni=0\nwhile i&lt;len(originX):\n    j=0\n    while j&lt;len(destinationX):\n        OD.append(str(distance(originX[i], originY[i], destinationX[j], destinationY[j])))\n        j=j+1\n    i=i+1\n\narcpy.AddMessage(\"euclidean OD matrix built......\")\n\n\n#  WRITING L-P file\n#  Model headings\n\n# Inputs for objective function\n# CPLEX Heading Requirements\n# Objective function headings\nprint(\"writing model output...\")\noutputFile.write(\"Minimize\\n\")\n\ni=0\nwhile i&lt; len(originX):\n    j=0\n    while j &lt; len(destinationX):\n        b = float(OD[(i*len(destinationX))+j])\n        outputFile.write(\"+\")\n        outputFile.write(str(b))\n        outputFile.write(\"X\")\n        outputFile.write(str(int(originFID[i])) + \"_\"+ str(int(destinationFID[j])))\n        j=j+1\n    i=i+1\n\n#  CONSTRAINTS  #\n\n# demand node assigned if school is open.\noutputFile.write(\"\\nSubject to\")\noutputFile.write(\"\\n\")\ni=0\nwhile i&lt; len(originFID):\n    j=0\n    while j &lt; len(destinationFID):\n        outputFile.write(\"+\")\n        outputFile.write(\"X\" + str(int(originFID[i])) + \"_\"+ str(int(destinationFID[j])))\n        outputFile.write(\"-Y\" + str(int(destinationFID[j])))\n        outputFile.write(\"&lt;=0\\n\")\n        j=j+1\n    i=i+1\nprint(\"constraint a demand node is assigned only if a facility is open...done\")\n\n\n# Constraint each demand node is assigned -\noutputFile.write(\"\\n\")\ni=0\nwhile i&lt; len(originFID):\n    j=0\n    while j &lt; len(destinationFID):\n        outputFile.write(\"+\")\n        outputFile.write(\"X\" + str(int(originFID[i])) + \"_\"+ str(int(destinationFID[j])))\n        j=j+1\n    outputFile.write(\"=1\\n\")\n    i=i+1\nprint(\"constraint that each demand node is assigned ...done\")\n\n###  MAXIMAL CAPACITY !Maximal Capacity Constraint!\noutputFile.write(\"\\n\")\n##\nCapacityField = destinationCapacity\nj=0\nwhile j&lt;len(destinationFID):\n    outputFile.write(str(maxCapacity * float(CapacityField[j]))+ \" Y\"+str(destinationFID[j]))\n    i=0\n    while i&lt;len(originFID):\n        a = popInfo[i]\n        if a&gt;0:\n            outputFile.write(\" -\")\n            outputFile.write(str(int(a)))\n            outputFile.write(\" X\")\n            outputFile.write(str(int(originFID[i]))+\"_\"+str(destinationFID[j]))\n        i=i+1\n    outputFile.write(\"&gt;=0\\n\")\n    j=j+1\noutputFile.write(\"\\n\\n\")\nprint(\"capacity constraint....done\")\n\n###  p-facilities\n\nj=0\nwhile j &lt; len(destinationFID):\n    outputFile.write(\"+\")\n    outputFile.write(\" Y\")\n    outputFile.write(str(int(destinationFID[j])))\n    j=j+1\noutputFile.write(\"&lt;=\" +str(p))\nprint(\"keep track of how many schools\")\noutputFile.write(\"\\n\\n\")\n\n\n\n### INTEGER CONSTRAINTS\noutputFile.write(\"\\n\") ##REMOVE LATER\noutputFile.write(\"Binary\\n\")\ni=0\nwhile i&lt;len(originFID):\n    j=0\n    while j&lt;len(destinationFID):\n        outputFile.write(\"X\")\n        outputFile.write(str(int(originFID[i]))+\"_\"+str(int(destinationFID[j]))+ \"\\n\")\n        j=j+1\n    i=i+1\noutputFile.write(\"\\n\")\nprint(\"X integers all set\")\n\nj=0\nwhile j&lt;len(destinationFID):\n    outputFile.write(\"Y\")\n    outputFile.write(str(int(destinationFID[j]))+\"\\n\")\n    j=j+1\nprint(\"Y integers all set\")\n\n\noutputFile.write(\"END\\n\")\noutputFile.close()\n\n\n\nModuleNotFoundError: No module named 'arcpy'"
  },
  {
    "objectID": "RemoteSensing/land_surface_temperature.html",
    "href": "RemoteSensing/land_surface_temperature.html",
    "title": "Analyzing Land Surface Temperature (LST) with Landsat 8 Data in Google Earth Engine",
    "section": "",
    "text": "Note: this code is adapted from Muhammad Ridho’s excellent Medium tutorial. I have converted his JavaScript code to Python using the geemap package, with only minor changes. I am quoting his writeup almost verbatim.\nThis notebook is exclusively for educational purposes. All credit for the work goes to Muhammad.\nLand Surface Temperature (LST) is a crucial environmental parameter with applications ranging from climate change monitoring to urban planning and agriculture management. This summary provides a concise overview of how to calculate and analyze LST using Landsat 8 imagery and the powerful toolset of Google Earth Engine (GEE).\nLandsat 8, a satellite operated by the United States Geological Survey (USGS) and NASA, provides a wealth of multispectral data. Its thermal infrared band, known as the Thermal Infrared Sensor (TIRS), is particularly valuable for estimating LST. Google Earth Engine is a cloud-based platform for geospatial analysis, making it an ideal choice for processing Landsat data and deriving LST information.\n\nStep 1: Accessing Google Earth Engine\n\n1.1 Sign up for an Earth Engine account if you don’t have one already.\n\n\n1.2 Load the ee and geemap libraries in Google Colab.\n\nimport ee\nimport geemap\n\n\n\n1.3 Authenticate your Earth Engine account in Google Colab.\n\nee.Authenticate()\n\n\n            \n            \n\n\n\n\n1.4 Initialize your session.\n\nee.Initialize(project='remotesensing-musa6500')\n\n\n            \n            \n\n\n\n\n\nStep 2: Define the Area of Interest (AOI)\nIn this part of the script, an Area of Interest (AOI) is defined using the ee.FeatureCollection function. The AOI is obtained from the “FAO/GAUL_SIMPLIFIED_500m/2015/level1” dataset, specifically filtering for the administrative division (ADM1_NAME) named ‘Daerah Istimewa Yogyakarta.’ This effectively creates a region of interest for subsequent analysis.\n\naoi = ee.FeatureCollection(\"FAO/GAUL_SIMPLIFIED_500m/2015/level1\") \\\n        .filter(ee.Filter.eq('ADM1_NAME', 'Daerah Istimewa Yogyakarta'))\n\n\n            \n            \n\n\n\n\nStep 3: Import Landsat 8 Data\n\n3.1 Define the start and end dates.\nThese two variables, startDate and endDate, can be used in various geospatial analyses, particularly in Google Earth Engine, to specify a specific time range for data retrieval, processing, or visualization. In this case, the time period defined by these dates spans from June 1, 2023, to September 21, 2023.\n\nfrom datetime import datetime\n\nstartDate = datetime(2023, 6, 1)\nendDate = datetime(2023, 9, 21)\n\n# Format dates into strings that Earth Engine expects (\"YYYY-MM-DD\")\nstartDate= startDate.strftime('%Y-%m-%d')\nendDate_str = endDate.strftime('%Y-%m-%d')\n\n\n            \n            \n\n\n\n\n3.2 Apply scaling factors\nApplying a scale factor involves adjusting pixel values in satellite imagery to ensure they represent physical properties accurately. For example, it’s used to convert digital numbers into physical units, like radiance or reflectance, making the data more meaningful for analysis.\n\n# Function to apply scaling factors.\ndef apply_scale_factors(image):\n    # Scale and offset values for optical bands\n    optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n\n    # Scale and offset values for thermal bands\n    thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n\n    # Add scaled bands to the original image\n    return image.addBands(optical_bands, None, True) \\\n                .addBands(thermal_bands, None, True)\n\n\n            \n            \n\n\n\n\n3.3 Apply cloud masking\nCloud masking is a process to identify and remove cloud-affected pixels from satellite imagery. It’s essential for ensuring that the analysis focuses on clear and cloud-free areas, minimizing the impact of clouds and their shadows on the results.\n\n# Function to mask clouds and cloud shadows in Landsat 8 imagery\ndef cloud_mask(image):\n    # Define cloud shadow and cloud bitmask (Bits 3 and 5)\n    cloud_shadow_bit_mask = 1 &lt;&lt; 3\n    cloud_bit_mask = 1 &lt;&lt; 5\n\n    # Select the Quality Assessment (QA) band for pixel quality information\n    qa = image.select('QA_PIXEL')\n\n    # Create a binary mask to identify clear conditions (both cloud and cloud shadow bits set to 0)\n    mask = qa.bitwiseAnd(cloud_shadow_bit_mask).eq(0) \\\n                .And(qa.bitwiseAnd(cloud_bit_mask).eq(0))\n\n    # Update the original image, masking out cloud and cloud shadow-affected pixels\n    return image.updateMask(mask)\n\n\n            \n            \n\n\n\n\n3.4 Import Landsat 8 imagery\nImporting images involves fetching satellite or remote sensing data into a geospatial analysis platform like Google Earth Engine. After data import, creating visualization involves defining how the data should be visually represented, and specifying parameters like color bands and brightness levels for effective interpretation and analysis.\n\n# Import and preprocess Landsat 8 imagery\nimage = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n            .filterBounds(aoi) \\\n            .filterDate(startDate, endDate) \\\n            .map(apply_scale_factors) \\\n            .map(cloud_mask) \\\n            .median() \\\n            .clip(aoi)\n\n\n            \n            \n\n\n\n\n\nStep 4: Calculate NDVI, Proportion of Vegetation, and Emissivity\n\nStep 4.1 Calculate NDVI\nNDVI (Normalized Difference Vegetation Index) is a widely used vegetation index in remote sensing and environmental science. It quantifies the presence and health of vegetation based on the reflectance of visible and near-infrared light. NDVI values typically range from -1 to 1, with higher values indicating healthier or denser vegetation.\nNDVI is calculated using the following formula:\nNDVI=(NIR+Red)/(NIR−Red)​\n\nNIR (Near-Infrared) is the reflectance in the near-infrared part of the electromagnetic spectrum (typically Landsat band 5 or similar).\nRed is the reflectance in the red part of the spectrum (typically Landsat band 4 or similar).\n\n\n# Calculate Normalized Difference Vegetation Index (NDVI)\nndvi = image.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n\n\n            \n            \n\n\n\n\nStep 4.2. Calculate Minimum and Maximum NDVI Values\nThe minimum (min) and maximum (max) values of the Normalized Difference Vegetation Index (NDVI) are used to calculate the proportion of vegetation cover (PV) in the region of interest.\nMinimum NDVI Calculation (ndviMin):\n\nThe script uses the reduceRegion method to calculate the minimum NDVI value within the specified AOI.\nThe reducer parameter is set to ee.Reducer.min(), indicating that we want to find the minimum value.\nThe geometry parameter is set to aoi, which defines the area of interest.\nThe scale parameter sets the pixel size for the analysis to 30 meters.\nThe maxPixels parameter specifies the maximum number of pixels to include in the calculation (1e9 represents one billion pixels, a large number to cover extensive areas).\n\nMaximum NDVI Calculation (ndviMax):\n\nSimilarly, this part of the script calculates the maximum NDVI value within the same AOI.\nIt uses ee.Reducer.max() as the reducer to find the maximum value.\n\n\n# Calculate the minimum and maximum NDVI values within the AOI\nndvi_min = ndvi.reduceRegion(\n    reducer=ee.Reducer.min(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI').getInfo()\n\nndvi_max = ndvi.reduceRegion(\n    reducer=ee.Reducer.max(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI').getInfo()\n\n\n            \n            \n\n\n\n\nStep 4.3. Calculate Proportion of Vegetation (PV) and Emissivity (EM)\nThe proportion of vegetation cover is calculated using the following formula:\nProportion of Vegetation (PV) = (NDVI — NDVI_min) / (NDVI_max — NDVI_min)\nWhere:\n\nNDVI represents the pixel’s NDVI value.\nNDVI_min is the minimum NDVI value specified.\nNDVI_max is the maximum NDVI value specified.\n\nThe Proportion of Vegetation (PV) is a metric used to quantify the relative abundance of vegetation within a specified area by analyzing Normalized Difference Vegetation Index (NDVI) values. It provides valuable insights into land cover and ecosystem health, with higher PV values indicating a greater presence of vegetation. On the other hand, Emissivity (EM) is a critical parameter for accurate Land Surface Temperature (LST) calculations. In the provided code, EM is computed as a function of PV, reflecting how efficiently a surface emits thermal radiation. EM values near 1.0 are typical for natural surfaces like soil and vegetation, while lower values are often associated with water bodies or urban areas. Both PV and EM play essential roles in remote sensing and environmental studies, contributing to a more comprehensive understanding of land characteristics and thermal behavior.\n\n# Convert NDVI_min and NDVI_max to ee.Number for operations\nndvi_min = ee.Number(ndvi_min)\nndvi_max = ee.Number(ndvi_max)\n\n# Fraction of Vegetation (FV) Calculation\n# Formula: ((NDVI - NDVI_min) / (NDVI_max - NDVI_min))^2\nfv = ndvi.subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).pow(2).rename('FV')\n\n# Emissivity Calculation\n# Formula: 0.004 * FV + 0.986\nem = fv.multiply(0.004).add(0.986).rename('EM')\n\n\n            \n            \n\n\n\n\n\nStep 5: Estimate Land Surface Temperature (LST)\nSelect Thermal Band: In this section, the script selects the thermal band (Band 10) from the input image and renames it as ‘thermal’. This band contains thermal infrared information.\n\n# Select Thermal Band (Band 10) and Rename It\nthermal = image.select('ST_B10').rename('thermal')\n\n\n            \n            \n\n\nThe script calculates Land Surface Temperature (LST) using Planck’s Law. The formula is provided as a mathematical expression. It involves using the thermal band (TB) and emissivity (em) values to compute LST.\n\n# Land Surface Temperature (LST) Calculation\n# Formula: (TB / (1 + (λ * (TB / 1.438)) * log(em))) - 273.15\nlst = thermal.expression(\n    '(TB / (1 + (0.00115 * (TB / 1.438)) * log(em))) - 273.15',\n    {\n        'TB': thermal.select('thermal'),  # Thermal band as the temperature in Kelvin\n        'em': em  # Emissivity\n    }\n).rename('LST Yogyakarta 2023')\n\n\n            \n            \n\n\n\n\nStep 6: Map Results with Custom Title and Legend\nNow, we can combine our results with custom visualizations to view them on the map. (This can be done step-by-step throughout the script or at the end.)\nFirst, we initialize a map using the geemap package. We add our AOI to it as a black line using an empty palette and center the map on our AOI.\n\nMap = geemap.Map() # initialize a map\nMap.addLayer(aoi, {}, 'AOI - Yogyakarta')\nMap.centerObject(aoi, 10) # center the map on the aoi\n\n\n            \n            \n\n\nNext, we add our true color imagery to the map.\n\n# Define visualization parameters for True Color imagery (bands 4, 3, and 2)\nvisualization = {\n  'bands': ['SR_B4', 'SR_B3', 'SR_B2'],\n  'min': 0.0,\n  'max': 0.15,\n}\n\n# Add the processed image to the map with the specified visualization\nMap.addLayer(image, visualization, 'True Color 432')\n\n\n            \n            \n\n\nWe add the NDVI layer, too.\n\n# Define NDVI Visualization Parameters\nndviPalette = {\n 'min': -1,\n 'max': 1,\n 'palette': ['blue', 'white', 'green']\n}\n\nMap.addLayer(ndvi, ndviPalette, 'NDVI Yogyakarta')\n\n\n            \n            \n\n\nFinally, we add the LST layer with a custom pallete and visualize the map with a custom title and legend.\n\n# Visualization parameters for LST\nlst_vis_params = {\n    'min': 18.47,\n    'max': 42.86,\n    'palette': [\n        '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n        '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n        '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n        'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n        'ff0000', 'de0101', 'c21301', 'a71001', '911003'\n    ]\n}\n\n# Add the LST layer to the map with custom visualization parameters\nMap.addLayer(lst, lst_vis_params, 'Land Surface Temperature 2023')\n\n# Define the color palette for the legend (updated to include the missing color '210300')\npalette = [\n    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n    'ff0000', 'de0101', 'c21301', 'a71001', '911003', '210300'\n]\n\n# Convert hex colors to the format required by the add_legend method\nlegend_colors = ['#' + color for color in palette]\n\n# Calculate the legend keys based on the number of colors\n# Assuming minLST and maxLST represent the range of your data\nminLST, maxLST = 15, 45\nlegend_keys = [str(round(minLST + i * (maxLST - minLST) / (len(palette) - 1), 2)) for i in range(len(palette))]\n\n# Add the custom legend to the map\nMap.add_legend(keys=legend_keys, colors=legend_colors, position='bottomright')\n\n# Set the center of the map (Example coordinates for Yogyakarta)\nMap.set_center(110.3668, -7.8032, 10)\n\n# Display the map\nMap"
  },
  {
    "objectID": "RemoteSensing/Forest_buildings_removed_dem_error.html",
    "href": "RemoteSensing/Forest_buildings_removed_dem_error.html",
    "title": "Trevor Kapuvari Python Demonstrations",
    "section": "",
    "text": "import ee\nimport colorcet\nfrom matplotlib.cm import get_cmap\nimport geemap\nfrom datetime import datetime, timedelta\n\n\nee.Authenticate()\n\n\n            \n            \n\n\n\nee.Initialize(project='remotesensing-musa6500')\n\n\n            \n            \n\n\n\nbbox = ee.Geometry.Polygon([\n    [\n        [-85.9, 8.0],\n        [-85.9, 11.2],\n        [-82.5, 11.2],\n        [-82.5, 8.0],\n        [-85.9, 8.0]\n    ]\n])\n\nstart_date = '2021-07-22'\nend_date = '2021-07-28'\nstart_date = datetime.strptime(start_date, \"%Y-%m-%d\")\nend_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n\n            \n            \n\n\n\n# Load the datasets\n\n# with the FABDEM layer, the output probability raster 1) is missing tiles and\n# 2) shows up semi-transparent in the geemap map for no apparent reason\ndem = ee.ImageCollection(\"projects/sat-io/open-datasets/GLO-30\").mosaic().clip(bbox)\n\n# with the hydrosheds DEM, however, this is not the issue\n# (uncomment this line to replace the FABDEM layer)\n# dem = ee.Image('WWF/HydroSHEDS/03VFDEM').clip(bbox)\n\nslope = ee.Terrain.slope(dem)\nlandcover = ee.Image(\"ESA/WorldCover/v100/2020\").select('Map').clip(bbox)\n\n\n            \n            \n\n\n\nstream_dist_proximity_collection = ee.ImageCollection(\"projects/sat-io/open-datasets/HYDROGRAPHY90/stream-outlet-distance/stream_dist_proximity\")\\\n    .filterBounds(bbox)\\\n    .mosaic()\nstream_dist_proximity = stream_dist_proximity_collection.clip(bbox).rename('stream_distance')\n\nhydro_proj = stream_dist_proximity.projection()\n\n## set time frame\nbefore_start= '2023-09-25'\nbefore_end='2023-10-05'\n\nafter_start='2023-10-05'\nafter_end='2023-10-15'\n\n# SET SAR PARAMETERS (can be left default)\n\n# Polarization (choose either \"VH\" or \"VV\")\npolarization = \"VH\"  # or \"VV\"\n\n# Pass direction (choose either \"DESCENDING\" or \"ASCENDING\")\npass_direction = \"DESCENDING\"  # or \"ASCENDING\"\n\n# Difference threshold to be applied on the difference image (after flood - before flood)\n# It has been chosen by trial and error. Adjust as needed.\ndifference_threshold = 1.25\n\n# Relative orbit (optional, if you know the relative orbit for your study area)\n# relative_orbit = 79\n\n# Rename the selected geometry feature\naoi = bbox\n\n# Load and filter Sentinel-1 GRD data by predefined parameters\ncollection = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', polarization)) \\\n    .filter(ee.Filter.eq('orbitProperties_pass', pass_direction)) \\\n    .filter(ee.Filter.eq('resolution_meters', 10)) \\\n    .filterBounds(aoi) \\\n    .select(polarization)\n\n# Select images by predefined dates\nbefore_collection = collection.filterDate(before_start, before_end)\nafter_collection = collection.filterDate(after_start, after_end)\n\n# Create a mosaic of selected tiles and clip to the study area\nbefore = before_collection.mosaic().clip(aoi)\nafter = after_collection.mosaic().clip(aoi)\n\n# Apply radar speckle reduction by smoothing\nsmoothing_radius = 50\nbefore_filtered = before.focal_mean(smoothing_radius, 'circle', 'meters')\nafter_filtered = after.focal_mean(smoothing_radius, 'circle', 'meters')\n\n# Calculate the difference between the before and after images\ndifference = after_filtered.divide(before_filtered)\n\n# Apply the predefined difference-threshold and create the flood extent mask\nthreshold = difference_threshold\ndifference_binary = difference.gt(threshold)\n\n# Refine the flood result using additional datasets\nswater = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').select('seasonality')\nswater_mask = swater.gte(10).updateMask(swater.gte(10))\nflooded_mask = difference_binary.where(swater_mask, 0)\nflooded = flooded_mask.updateMask(flooded_mask)\nconnections = flooded.connectedPixelCount()\nflooded = flooded.updateMask(connections.gte(8))\n\n\n            \n            \n\n\n\n# Mask out areas with more than 5 percent slope using a Digital Elevation Model\nterrain = ee.Algorithms.Terrain(dem)\nflooded = flooded.updateMask(slope.lt(5))\n\n# Set the default projection from the hydrography dataset\nflooded = flooded.setDefaultProjection(hydro_proj)\n\n# Now, reduce the resolution\nflooded_mode = flooded.reduceResolution(\n    reducer=ee.Reducer.mode(),\n    maxPixels=10000\n).reproject(\n    crs=hydro_proj\n)\n\n\n            \n            \n\n\n\n# Create a full-area mask, initially marking everything as non-flooded (value 0)\nfull_area_mask = ee.Image.constant(0).clip(aoi)\n\n# Update the mask to mark flooded areas (value 1)\n# Assuming flooded_mode is a binary image with 1 for flooded areas and 0 elsewhere\nflood_labeled_image = full_area_mask.where(flooded, 1)\n\n# Now flood_labeled_image contains 1 for flooded areas and 0 for non-flooded areas\n\n\n            \n            \n\n\n\ncombined = (dem.rename(\"elevation\")\n        .addBands(landcover.select('Map').rename(\"landcover\"))\n        .addBands(slope)\n        .addBands(flood_labeled_image.rename(\"flooded_mask\"))\n        )\n\n\n            \n            \n\n\n\n\n# Get all band names from the combined image\nallBandNames = combined.bandNames()\n\n# Remove the class band name ('flooded_full_mask') to get input properties\ninputProperties = allBandNames.filter(ee.Filter.neq('item', 'flooded_mask'))\n\n# Perform stratified sampling\nstratifiedSample = combined.stratifiedSample(\n    numPoints=500,  # Total number of points\n    classBand='flooded_mask',  # Band to stratify by\n    region=bbox,\n    scale=90,\n    seed=0\n).randomColumn()\n\n# Split into training and testing\ntraining = stratifiedSample.filter(ee.Filter.lt('random', 0.7))\ntesting = stratifiedSample.filter(ee.Filter.gte('random', 0.7))\n\n# Set up the Random Forest classifier for flood prediction\nclassifier = ee.Classifier.smileRandomForest(10).train(\n    features=training,\n    classProperty='flooded_mask',  # Use 'flooded_full_mask' as the class property\n    inputProperties=inputProperties  # Dynamically generated input properties\n)\n\n# Classify the image\nclassified = combined.select(inputProperties).classify(classifier)\n\n# Assess accuracy\ntestAccuracy = testing.classify(classifier).errorMatrix('flooded_mask', 'classification')\n\n# Calculate accuracy\naccuracy = testAccuracy.accuracy().getInfo()\n\n# Convert the confusion matrix to an array\nconfusionMatrixArray = testAccuracy.array().getInfo()\n\n# Calculate recall for the positive class (assuming '1' represents the positive class for flooding)\ntrue_positives = confusionMatrixArray[1][1]  # True positives\nfalse_negatives = confusionMatrixArray[1][0]  # False negatives\nfalse_positives = confusionMatrixArray[0][1]  # False positives (non-flooded incorrectly identified as flooded)\ntrue_negatives = confusionMatrixArray[0][0]  # True negatives (non-flooded correctly identified as non-flooded)\nrecall = true_positives / (true_positives + false_negatives)\nfalse_positive_rate = false_positives / (false_positives + true_negatives)\n\nprint('Confusion Matrix:', confusionMatrixArray)\nprint('Accuracy:', accuracy)\nprint('Recall:', recall)\nprint('False Positive Rate:', false_positive_rate)\n\n\n            \n            \n\n\nConfusion Matrix: [[84, 55], [16, 130]]\nAccuracy: 0.7508771929824561\nRecall: 0.8904109589041096\nFalse Positive Rate: 0.39568345323741005\n\n\n\n# Set up the Random Forest classifier for flood prediction with probability output\nclassifier = ee.Classifier.smileRandomForest(10).setOutputMode('PROBABILITY').train(\n        features=training,\n        classProperty='flooded_mask',\n        inputProperties=inputProperties\n    )\n\n# Classify the image to get probabilities\nprobabilityImage = combined.select(inputProperties).classify(classifier)\n\n# Visualization parameters for probability with white at the midpoint\nvizParamsProbability = {\n    'min': 0.,\n    'max': 1,\n    'palette': colorcet.bmw\n}\n\n\n            \n            \n\n\n\nm = geemap.Map()\nm.add(\"basemap_selector\")\nm.add(\"layer_manager\")\n\n# Center the map on San Jose, Costa Rica\nm.setCenter(-84.0833, 9.9333, 10)\nm.addLayer(landcover, {}, 'ESA WorldCover 2020')\nm.addLayer(probabilityImage, vizParamsProbability, 'Flood Probability')\nm.addLayer(swater_mask, {'palette': 'black'}, 'Permanent Surface Water')\n# Display the map\nm"
  },
  {
    "objectID": "analysis/streetsandwebscrap.html",
    "href": "analysis/streetsandwebscrap.html",
    "title": "Street Networks & Web Scraping",
    "section": "",
    "text": "Part 1: Visualizing crash data in Philadelphia\nIn this section, you will use osmnx to analyze the crash incidence in Center City.\nPart 2: Scraping Craigslist\nIn this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia’s Craigslist portal."
  },
  {
    "objectID": "analysis/streetsandwebscrap.html#part-1-visualizing-crash-data-in-philadelphia",
    "href": "analysis/streetsandwebscrap.html#part-1-visualizing-crash-data-in-philadelphia",
    "title": "Street Networks & Web Scraping",
    "section": "Part 1: Visualizing crash data in Philadelphia",
    "text": "Part 1: Visualizing crash data in Philadelphia\n1.1 Load the geometry for the region being analyzed We’ll analyze crashes in the “Central” planning district in Philadelphia, a rough approximation for Center City. Planning districts can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\nhttp://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\nSelect the “Central” district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type shapely.geometry.polygon.Polygon\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport hvplot.pandas \nimport altair as alt\nimport matplotlib.pyplot as plt\nimport rasterio as rio\nimport osmnx as ox\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\nCPD = gpd.read_file(\"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\")\n#CPD = ox.geocode_to_gdf(\"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\")\nCPD = CPD[CPD[\"ABBREV\"] == \"CTR\"]\nCPD\n\n\n\n\n\n\n\n\nOBJECTID_1\nOBJECTID\nDIST_NAME\nABBREV\nShape__Area\nShape__Length\nPlanningDist\nDaytimePop\ngeometry\n\n\n\n\n3\n4\n9\nCentral\nCTR\n1.782880e+08\n71405.14345\nNaN\nNaN\nPOLYGON ((-75.14791 39.96733, -75.14715 39.967...\n\n\n\n\n\n\n\n\nPhillyox = ox.geocode_to_gdf(\"Philadelphia, PA\")\nPhillyox.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n1.2 Get the street network graph\nUse OSMnx to create a network graph (of type ‘drive’) from your polygon boundary in 1.1.\n\nax = ox.project_gdf(CPD).plot(fc=\"lightgreen\", ec=\"black\")\nax.set_axis_off()\n\n\n\n\n\nax = CPD.to_crs(epsg=2272).plot(facecolor=\"none\", edgecolor=\"black\")\nax.set_axis_off()\n\n\n\n\n\nstreets = ox.features_from_place(\"Philadelphia, PA\", tags={\"highway\": True})\n\n\nstreets.head(6)\n\n\n\n\n\n\n\n\n\nhighway\ngeometry\ntraffic_signals\ntraffic_signals:direction\nrailway\ncrossing\nref\nnoref\nnoexit\nref:left\nref:right\nold_ref\ndisused:railway\nstop\nname\ncrossing:markings\ndirection\ntraffic_calming\ntactile_paving\naccess\nsource\ncrossing_ref\nbicycle\ndescription\nfoot\nhorse\nmotor_vehicle\naddr:city\naddr:housenumber\naddr:postcode\naddr:state\naddr:street\nwebsite\nnote\ncrossing:island\nproposed:junction\nsupervised\nexcept\nproposed:highway\nford\nele\ngnis:feature_id\nbench\nbus\ncovered\nnetwork\nnetwork:wikidata\noperator\npublic_transport\nshelter\nkerb\njunction\ngate\nparking\nwheelchair\nnetwork:wikipedia\nindoor\nlevel\nfixme\nroute_ref\nlocal_ref\ndesignation\nleisure\naeroway\nmaxheight\nheritage\nheritage:operator\nref:nrhp\nmaterial\nbin\nlit\ntraffic_signals:sound\nlamp_type\ndepartures_board\ncapacity\nbutton_operated\nlayer\nopening_hours\nsurface\ntraffic_signals:vibration\ninternet_access\nroute_ref_1\nlanduse\nalt_name\nlamp_mount\ntraffic_sign\noperator:wikidata\nshort_name\nbrand\nbrand:wikidata\ntram\nmotor_vehicle:conditional\nhgv\ntourism\ntrolleybus\nnetwork:short\nman_made\nflashing_lights\nabandoned\nnot:network:wikidata\nsupport\ntraffic_signals:countdown\nhistoric\ncycleway\ninformal\nadvertising\ncheck_date:crossing\nnodes\noneway\ntiger:cfcc\ntiger:name_base\ntiger:name_type\ntiger:reviewed\ntiger:zip_left\ntiger:zip_right\nlane_markings\nlanes\ntiger:name_direction_prefix\ntiger:separated\ntiger:source\ntiger:tlid\ntiger:zip_left_1\nservice\nparking:lane:both\nsidewalk\ncycleway:right\nold_ref_legislative\nref:penndot\ntiger:name_direction_suffix\ndestination:ref\nturn:lanes\ndestination\ntoll\ndestination:ref:to\nmaxspeed\nsource:hgv\nsource:ref:penndot\nsmoothness\nbridge\njunction:ref\ndestination:lanes\nwidth\ncycleway:both\ncycleway:both:buffer\nNHS\ncycleway:right:buffer\ncheck_date:smoothness\ndestination:street\nwikipedia\nlanes:backward\nlanes:forward\nturn:lanes:backward\ntracktype\nturn\nmaxspeed:advisory\npsv\ndestination:symbol\ntunnel\nnoname\nmaxweight:signed\ntiger:name_base_1\nsource:width\nrcn_ref\nabandoned:railway\nparking:lane:left\nparking:lane:left:parallel\nparking:lane:right\ncycleway:left\nembedded_rails\nname_1\ntiger:name_base_2\ntiger:name_type_1\ndestination:ref:lanes\nold_ref:legislative\ndestination:to\ntiger:zip_left_2\ntiger:zip_right_1\ntiger:zip_right_2\ncycleway:left:buffer\ncycleway:right:lane\nsource:oneway\nsidewalk:both\nlcn\nparking:lane:right:parallel\nmaxspeed:type\nparking:both\nparking:orientation\nsidewalk:both:surface\ncentre_turn_lane\ntiger:zip_left_3\ntiger:zip_left_4\nparking:left\nparking:right\nparking:right:orientation\nsidewalk:left\nsidewalk:right\nmaxwidth\nold_name\nmaxspeed:backward\nold_railway_operator\nbicycle:backward:conditional\nbicycle:forward:conditional\nname:etymology:wikidata\noneway:bicycle\nconstruction\nwikidata\nincline\ncheck_date:surface\ndestination:street:lanes\nparking:both:orientation\naccess:hgv\nmaxweight\nFIXME\nlanes:both_ways\nturn:lanes:both_ways\nturn:lanes:forward\ntiger:zip_right_3\nsource:lanes\nrestriction:hgv\nmaxspeed:forward\ntiger:name_direction_prefix_1\nparking:lane:both:parallel\nnote:lanes\nname_2\ntiger:name_direction_prefix_2\ntiger:name_type_2\ntiger:zip_right_4\nmotorcar\ntiger:name_direction\nnote:old_railway_operator\ntoll:backward\ntoll:forward\nbicycle:forward\nparking:condition:right\nbicycle:backward\nmaxwidth:physical\ntiger:name_base_3\ntiger:name_direction_prefix_3\ntiger:name_type_3\nsource:boundary\nsegregated\nbridge:name\nstart_date\nname2\ncreated_by\nmotorroad\nfootway\nmaxspeed:variable\ntiger:county\ndestination:ref:to:lanes\narea\nhandrail\nramp\nparking:left:markings\nparking:left:orientation\nstep_count\nsac_scale\ntrail_visibility\ntrolley_wire\nvoltage\nhighspeed\ncheck_date\nbase_material_values\nbridge:structure\nembankment\ndestination:lanes:forward\ndestination:ref:to:lanes:forward\ndestination:ref:lanes:forward\ndestination:ref:to:lanes:backward\ndisused:cycleway:both\ndestination:lanes:backward\nopening_date\nlanes:bus\nmtb:scale\naccess:lanes:forward\npsv:lanes:forward\nplacement:backward\ncanoe\nsurface:condition\ndestination:ref:forward\ndestination:street:forward\ntiger:zip\nunsigned_ref\nhgv:lanes\nshoulder\nplacement\nabandoned:highway\ndestination:ref:lanes:backward\nproposed\nbicycle:lane\nloc_name\nramp:wheelchair\nmtb:scale:imba\nmtb:scale:uphill\ndisused:cycleway:right\ngoods\ncutting\ncycleway:left:oneway\nparking:condition:left\nparking:condition:left:time_interval\nski\nsnowmobile\nemergency\nsource:name\nname:en\nHFCS\nNJDOT_SRI\nbridge:movable\nhgv:state_network\nsource:hgv:state_network\nchange:lanes:backward\nchange:lanes:forward\nparking:lane:right:diagonal\nlevels\npostal_code\nlocation\nlength\nsport\nclosed\ncondition\ndanger\ndanger_1\nhazard\ncycleway:both:width\nparking:lane:left:diagonal\nPARK_NAME\nSHAPE_LENG\nTRL_LENGTH\nparking:condition:both\nproposed:lanes\nproposed:lanes:forward\nproposed:turn:lanes:forward\nproposed:lanes:backward\nproposed:turn:lanes:backward\nplacement:forward\ndestination:street:backward\ndestination:street:lanes:forward\ndestination:street:lanes:backward\nname:backward\nname:forward\ncycleway:buffer\nturn:backward\ngolf\ngauge\nparking:condition:both:time_interval\ntrain\nhorse_scale\ndog\nturn:forward\nsurface:lanes:backward\nsurface:lanes:forward\nbus:backward\nhgv:backward\nmotorcar:backward\nmotorroad:backward\nartist:website\nartist:wikidata\nartist_name\nnot:name\ngolf_cart\nsteps\ncomment\nhistoric:railway\ndisused:cycleway:left\noneway:bus\nstairs\nsidewalk:left:surface\nsidewalk:right:surface\ntruck\nstroller\nconveying\ncheck_date:handrail\nways\ntype\n\n\nelement_type\nosmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnode\n109727831\ntraffic_signals\nPOINT (-75.15328 40.02996)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727914\ntraffic_signals\nPOINT (-75.13837 40.02803)\nsignal\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727992\ntraffic_signals\nPOINT (-75.14616 40.02904)\nsignal\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727997\ntraffic_signals\nPOINT (-75.14686 40.02914)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109728089\nturning_circle\nPOINT (-74.99562 40.10542)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109728114\ntraffic_signals\nPOINT (-75.13897 40.03285)\nsignal\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nCPDoutline = CPD.squeeze().geometry\nCPDoutline\n\n\n\n\n\nG_CPD = ox.graph_from_polygon(CPDoutline, network_type=\"drive\")\n\n\nox.plot_graph(ox.project_graph(G_CPD), node_size=0);\n\n\n\n\n\n\n1.3 Convert your network graph edges to a GeoDataFrame\nUse OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network.\n\nCPD_edges = ox.graph_to_gdfs(G_CPD, edges=True, nodes=False)\n\n\nCPD_edges\n\n\n\n\n\n\n\n\n\n\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\n\n\nu\nv\nkey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n109727439\n109911666\n0\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727448\n109727439\n0\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n110034229\n0\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727507\n110024052\n0\n193364514\nTrue\nCarpenter Street\nresidential\nFalse\n53.208\nLINESTRING (-75.17196 39.93973, -75.17134 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109728761\n110274344\n0\n672312336\nTrue\nBrown Street\nresidential\nFalse\n58.270\nLINESTRING (-75.17317 39.96951, -75.17250 39.9...\n25 mph\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11176163640\n11176163648\n1\n1206065012\nFalse\nNaN\nliving_street\nFalse\n57.963\nLINESTRING (-75.16655 39.95896, -75.16633 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5808113442\n0\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n37.086\nLINESTRING (-75.16655 39.95896, -75.16662 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11176163648\n5808113443\n0\n613950538\nFalse\nAlexander Court\nliving_street\nFalse\n31.592\nLINESTRING (-75.16652 39.95909, -75.16646 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11176163640\n0\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n14.738\nLINESTRING (-75.16652 39.95909, -75.16655 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1206065012\nFalse\nNaN\nliving_street\nTrue\n57.963\nLINESTRING (-75.16652 39.95909, -75.16630 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3896 rows × 16 columns\n\n\n\n\n\n1.4 Load PennDOT crash data\nData for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n./data/CRASH_PHILADELPHIA_XXXX.csv\nYou should see three separate files in the data/ folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using pd.concat().\nThe data was downloaded for Philadelphia County from here.\n\ncrash2020 = pd.read_csv(\"./Data/CRASH_PHILADELPHIA_2020.csv\")\ncrash2021 = pd.read_csv(\"./Data/CRASH_PHILADELPHIA_2021.csv\")\ncrash2022 = pd.read_csv(\"./Data/CRASH_PHILADELPHIA_2022.csv\")\ncrashDf = pd.concat([crash2020, crash2021, crash2022])\ncrashDf\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2020036617\n1842.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n4\n2020\n1\n39.9809\n-75.2065\n1840.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n18\n1\n0\n0\nNaN\n1\n39 58:51.367\n3.0\n0\n75 12:23.366\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n0\nNaN\n2\n9\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nN\n1838\n0\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2020035717\n2000.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\nNaN\n67\n4\n2020\n1\n39.9269\n-75.1691\n2000.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n14\n1\n1\n1\nNaN\n9\n39 55:36.660\nNaN\n0\n75 10:08.677\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n0\nNaN\n1\n0\n1\n3\n3\nU\n1457\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n4.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2020034378\n1139.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n4\n2020\n4\n39.9237\n-75.1924\n1131.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n11\n1\n1\n0\nNaN\n0\n39 55:25.183\nNaN\n0\n75 11:32.758\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n1\nNaN\n1\n0\n0\n0\n0\nNaN\n1128\n1\n3\n0\n0\n0\n0\n0\n3\n4\n0\n3\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2020025511\n345.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2020\n1\n39.8826\n-75.2450\n329.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n3\n3\n2\n0\nNaN\n0\n39 52:57.717\nNaN\n0\n75 14:41.931\n3\n0\n0\n0\n67301\n0\n0\n0\nY\n0\n0\n0\n2\n68K01\n0\nNaN\n4\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nNaN\n328\n2\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8746\n2022016289\n2245.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n4\n40.0239\n-75.1425\n2242.0\n6\n0\n0\n0\n1\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n1\n1\nNaN\n0\n40 01:25.870\nNaN\n0\n75 08:33.165\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n2230\n1\n4\n0\n0\n0\n0\n0\n3\n4\n0\n4\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8748\n2022014729\n1721.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n1\n40.0189\n-75.0495\n1707.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n3\n1\n1\nNaN\n0\n40 01:08.120\nNaN\n0\n75 02:58.216\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n1701\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8749\n2022014753\n713.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\nNaN\n67\n2\n2022\n4\n39.9291\n-75.1687\n707.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n7\n1\n2\n0\nN\n0\n39 55:44.636\nNaN\n0\n75 10:07.421\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67301\n0\nNaN\n3\n1\nNaN\nN\nN\nN\n1\nNaN\n1\n0\n1\n0\n0\nNaN\n700\n2\n5\n0\n0\n0\n1\n0\n3\n4\n0\n5\n5\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n29463 rows × 100 columns\n\n\n\n\n\n1.5 Convert the crash data to a GeoDataFrame\nYou will need to use the DEC_LAT and DEC_LONG columns for latitude and longitude.\nThe full data dictionary for the data is available here\n\ncrashGDF = gpd.GeoDataFrame(crashDf, geometry=gpd.points_from_xy(crashDf[\"DEC_LONG\"], crashDf[\"DEC_LAT\"], crs=\"2272\"))\ncrashGDF\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n1\n2020036617\n1842.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n4\n2020\n1\n39.9809\n-75.2065\n1840.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n18\n1\n0\n0\nNaN\n1\n39 58:51.367\n3.0\n0\n75 12:23.366\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n0\nNaN\n2\n9\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nN\n1838\n0\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.207 39.981)\n\n\n2\n2020035717\n2000.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\nNaN\n67\n4\n2020\n1\n39.9269\n-75.1691\n2000.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n14\n1\n1\n1\nNaN\n9\n39 55:36.660\nNaN\n0\n75 10:08.677\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n0\nNaN\n1\n0\n1\n3\n3\nU\n1457\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n4.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.169 39.927)\n\n\n3\n2020034378\n1139.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n4\n2020\n4\n39.9237\n-75.1924\n1131.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n11\n1\n1\n0\nNaN\n0\n39 55:25.183\nNaN\n0\n75 11:32.758\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n1\nNaN\n1\n0\n0\n0\n0\nNaN\n1128\n1\n3\n0\n0\n0\n0\n0\n3\n4\n0\n3\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.192 39.924)\n\n\n4\n2020025511\n345.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2020\n1\n39.8826\n-75.2450\n329.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n3\n3\n2\n0\nNaN\n0\n39 52:57.717\nNaN\n0\n75 14:41.931\n3\n0\n0\n0\n67301\n0\n0\n0\nY\n0\n0\n0\n2\n68K01\n0\nNaN\n4\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nNaN\n328\n2\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.245 39.883)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8746\n2022016289\n2245.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n4\n40.0239\n-75.1425\n2242.0\n6\n0\n0\n0\n1\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n1\n1\nNaN\n0\n40 01:25.870\nNaN\n0\n75 08:33.165\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n2230\n1\n4\n0\n0\n0\n0\n0\n3\n4\n0\n4\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.142 40.024)\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.958)\n\n\n8748\n2022014729\n1721.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n1\n40.0189\n-75.0495\n1707.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n3\n1\n1\nNaN\n0\n40 01:08.120\nNaN\n0\n75 02:58.216\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n1701\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.049 40.019)\n\n\n8749\n2022014753\n713.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\nNaN\n67\n2\n2022\n4\n39.9291\n-75.1687\n707.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n7\n1\n2\n0\nN\n0\n39 55:44.636\nNaN\n0\n75 10:07.421\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67301\n0\nNaN\n3\n1\nNaN\nN\nN\nN\n1\nNaN\n1\n0\n1\n0\n0\nNaN\n700\n2\n5\n0\n0\n0\n1\n0\n3\n4\n0\n5\n5\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.169 39.929)\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.947)\n\n\n\n\n29463 rows × 101 columns\n\n\n\n\n\n1.6 Trim the crash data to Center City\n\nGet the boundary of the edges data frame (from part 1.3). Accessing the .geometry.unary_union.convex_hull property will give you a nice outer boundary region.\nTrim the crashes using the within() function of the crash GeoDataFrame to find which crashes are within the boundary.\n\nThere should be about 3,750 crashes within the Central district.\n\nCPD_boundary = CPD_edges.geometry.unary_union.convex_hull\nCC_crashes = crashGDF[crashGDF.within(CPD_boundary)]\nCC_crashes\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n4\n1\nNaN\n67\n3\n2020\n2\n39.9700\n-75.1631\n1250.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n12\n1\n2\n1\nNaN\n0\n39 58:11.995\nNaN\n0\n75 09:47.193\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67508\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n3\nN\n1250\n2\n2\n0\n0\n0\n2\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.163 39.970)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n2\n2020\n7\n39.9523\n-75.1878\n800.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n8\n1\n2\n0\nNaN\n2\n39 57:08.258\n4.0\n0\n75 11:16.213\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67504\n0\nNaN\n6\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nN\n800\n2\n2\n0\n0\n2\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.188 39.952)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n5\n1\nNaN\n67\n3\n2020\n2\n39.9558\n-75.1484\n1024.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n1\n0.0\n10\n1\n0\n0\nNaN\n0\n39 57:21.042\nNaN\n2\n75 08:54.233\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67501\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1024\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.148 39.956)\n\n\n18\n2020000481\n1737.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n5\n1\nNaN\n67\n1\n2020\n4\n39.9534\n-75.1547\n1737.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n4\n0\n1\nNaN\n0\n39 57:12.069\nNaN\n0\n75 09:16.934\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n2\nNaN\n1735\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.155 39.953)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8719\n2022019573\n2210.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n1\n2022\n5\n39.9556\n-75.1728\n2205.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n21\n3\n1\n1\nNaN\n0\n39 57:20.163\nNaN\n0\n75 10:21.961\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n2\nNaN\n2145\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.173 39.956)\n\n\n8723\n2022037448\n252.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n2\n2022\n1\n39.9537\n-75.1816\n237.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n2\n3\n1\n0\nN\n1\n39 57:13.320\n3.0\n0\n75 10:53.702\n4\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n1\nNaN\n2\n1\n246.0\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nN\n236\n1\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.182 39.954)\n\n\n8744\n2022037464\n2225.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2022\n5\n39.9600\n-75.1798\n2210.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n0\n6\nNaN\n0\n39 57:36.017\nNaN\n2\n75 10:47.219\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n1\n0\n0\nNaN\n2205\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.180 39.960)\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.958)\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.947)\n\n\n\n\n3751 rows × 101 columns\n\n\n\n\n\n1.7 Re-project our data into an approriate CRS\nWe’ll need to find the nearest edge (street) in our graph for each crash. To do this, osmnx will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude\nWe’ll convert the local state plane CRS for Philadelphia, EPSG=2272\n\nTwo steps:\n\nProject the graph object (G) using the ox.project_graph. Run ox.project_graph? to see the documentation for how to convert to a specific CRS.\nProject the crash data using the .to_crs() function.\n\n\nG_projected = ox.project_graph(G_CPD, to_crs=2272)\nCC_crashproj = CC_crashes.to_crs(2272)\nCC_crashproj\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n4\n1\nNaN\n67\n3\n2020\n2\n39.9700\n-75.1631\n1250.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n12\n1\n2\n1\nNaN\n0\n39 58:11.995\nNaN\n0\n75 09:47.193\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67508\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n3\nN\n1250\n2\n2\n0\n0\n0\n2\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.163 39.970)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n2\n2020\n7\n39.9523\n-75.1878\n800.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n8\n1\n2\n0\nNaN\n2\n39 57:08.258\n4.0\n0\n75 11:16.213\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67504\n0\nNaN\n6\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nN\n800\n2\n2\n0\n0\n2\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.188 39.952)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n5\n1\nNaN\n67\n3\n2020\n2\n39.9558\n-75.1484\n1024.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n1\n0.0\n10\n1\n0\n0\nNaN\n0\n39 57:21.042\nNaN\n2\n75 08:54.233\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67501\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1024\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.148 39.956)\n\n\n18\n2020000481\n1737.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n5\n1\nNaN\n67\n1\n2020\n4\n39.9534\n-75.1547\n1737.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n4\n0\n1\nNaN\n0\n39 57:12.069\nNaN\n0\n75 09:16.934\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n2\nNaN\n1735\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.155 39.953)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8719\n2022019573\n2210.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n1\n2022\n5\n39.9556\n-75.1728\n2205.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n21\n3\n1\n1\nNaN\n0\n39 57:20.163\nNaN\n0\n75 10:21.961\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n2\nNaN\n2145\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.173 39.956)\n\n\n8723\n2022037448\n252.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n2\n2022\n1\n39.9537\n-75.1816\n237.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n2\n3\n1\n0\nN\n1\n39 57:13.320\n3.0\n0\n75 10:53.702\n4\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n1\nNaN\n2\n1\n246.0\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nN\n236\n1\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.182 39.954)\n\n\n8744\n2022037464\n2225.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2022\n5\n39.9600\n-75.1798\n2210.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n0\n6\nNaN\n0\n39 57:36.017\nNaN\n2\n75 10:47.219\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n1\n0\n0\nNaN\n2205\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.180 39.960)\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.958)\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.947)\n\n\n\n\n3751 rows × 101 columns\n\n\n\n\nox.plot_graph(G_projected);\n\n\n\n\n\n\n\n1.8 Find the nearest edge for each crash\nSee: ox.distance.nearest_edges(). It takes three arguments:\n\nthe network graph\nthe longitude of your crash data (the x attribute of the geometry column)\nthe latitude of your crash data (the y attribute of the geometry column)\n\nYou will get a numpy array with 3 columns that represent (u, v, key) where each u and v are the node IDs that the edge links together. We will ignore the key value for our analysis.\n\ndef find_nearest_edge(CC_crashproj, G_CPD):\n    crash_longitudes = CC_crashproj.geometry.x\n    crash_latitudes = CC_crashproj.geometry.y\n\n    nearest_edges = ox.distance.nearest_edges(G_CPD, crash_longitudes, crash_latitudes)\n    #CC_crashproj[\"nearest_edge\"] = nearest_edges\n    return nearest_edges\n\ncrashes_nearest_edge = find_nearest_edge(CC_crashproj, G_CPD)\n#crashes_nearest_edge\n\n\n\n1.9 Calculate the total number of crashes per street\n\nMake a DataFrame from your data from part 1.7 with three columns, u, v, and key (we will only use the u and v columns)\nGroup by u and v and calculate the size\nReset the index and name your size() column as crash_count\n\nAfter this step you should have a DataFrame with three columns: u, v, and crash_count.\n\nCC_crashproj.head(4)\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n4\n1\nNaN\n67\n3\n2020\n2\n39.9700\n-75.1631\n1250.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n12\n1\n2\n1\nNaN\n0\n39 58:11.995\nNaN\n0\n75 09:47.193\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67508\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n3\nN\n1250\n2\n2\n0\n0\n0\n2\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.163 39.970)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n2\n2020\n7\n39.9523\n-75.1878\n800.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n8\n1\n2\n0\nNaN\n2\n39 57:08.258\n4.0\n0\n75 11:16.213\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67504\n0\nNaN\n6\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nN\n800\n2\n2\n0\n0\n2\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.188 39.952)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n5\n1\nNaN\n67\n3\n2020\n2\n39.9558\n-75.1484\n1024.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n1\n0.0\n10\n1\n0\n0\nNaN\n0\n39 57:21.042\nNaN\n2\n75 08:54.233\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67501\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1024\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.148 39.956)\n\n\n\n\n\n\n\n\nnearest_edge_column = crashes_nearest_edge\n\nNearestEdgedf = pd.DataFrame(nearest_edge_column, columns=[\"u\", \"v\", \"key\"])\n\nNearestEdgedf \n\n\n\n\n\n\n\n\nu\nv\nkey\n\n\n\n\n0\n8482829382\n7065714513\n0\n\n\n1\n109848091\n109848089\n0\n\n\n2\n1903608761\n109775193\n0\n\n\n3\n775424610\n775424581\n0\n\n\n4\n110416511\n110417392\n0\n\n\n...\n...\n...\n...\n\n\n3746\n110053344\n110053369\n0\n\n\n3747\n110453046\n110318202\n0\n\n\n3748\n8482829382\n7065714513\n0\n\n\n3749\n109791270\n109783164\n0\n\n\n3750\n110338911\n110000903\n0\n\n\n\n\n3751 rows × 3 columns\n\n\n\n\ngrouped_NearestEdgedf = NearestEdgedf.groupby([\"u\", \"v\"])\n\n\ncrash_count = grouped_NearestEdgedf.size().to_frame('size')\ncrash_count_df = crash_count.reset_index()\ncrash_count_df.rename(columns={\"size\": \"crash_count\"}, inplace=True)\ncrash_count_df\n\n\n\n\n\n\n\n\nu\nv\ncrash_count\n\n\n\n\n0\n109729474\n3425014859\n2\n\n\n1\n109729486\n110342146\n4\n\n\n2\n109729673\n109729699\n3\n\n\n3\n109729699\n109811674\n6\n\n\n4\n109729709\n109729731\n3\n\n\n...\n...\n...\n...\n\n\n875\n10270051289\n5519334546\n5\n\n\n876\n10660521817\n10660521823\n1\n\n\n877\n10674041689\n10674041689\n22\n\n\n878\n11144117753\n109729699\n3\n\n\n879\n11162290432\n110329835\n1\n\n\n\n\n880 rows × 3 columns\n\n\n\n\n\n1.10 Merge your edges GeoDataFrame and crash count DataFrame\nYou can use pandas to merge them on the u and v columns. This will associate the total crash count with each edge in the street network.\nTips: - Use a left merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge. - Use the fillna(0) function to fill in missing crash count values with zero.\n\nmerged_CPD = pd.merge(CPD_edges, crash_count_df, how=\"left\", on=[\"u\",\"v\"]).fillna(0)\nmerged_CPD\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\ncrash_count\n\n\n\n\n0\n109727439\n109911666\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n1\n109727448\n109727439\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n2\n109727448\n110034229\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3\n109727507\n110024052\n193364514\nTrue\nCarpenter Street\nresidential\nFalse\n53.208\nLINESTRING (-75.17196 39.93973, -75.17134 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n4\n109728761\n110274344\n672312336\nTrue\nBrown Street\nresidential\nFalse\n58.270\nLINESTRING (-75.17317 39.96951, -75.17250 39.9...\n25 mph\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3891\n11176163640\n11176163648\n1206065012\nFalse\n0\nliving_street\nFalse\n57.963\nLINESTRING (-75.16655 39.95896, -75.16633 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3892\n11176163640\n5808113442\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n37.086\nLINESTRING (-75.16655 39.95896, -75.16662 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3893\n11176163648\n5808113443\n613950538\nFalse\nAlexander Court\nliving_street\nFalse\n31.592\nLINESTRING (-75.16652 39.95909, -75.16646 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3894\n11176163648\n11176163640\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n14.738\nLINESTRING (-75.16652 39.95909, -75.16655 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3895\n11176163648\n11176163640\n1206065012\nFalse\n0\nliving_street\nTrue\n57.963\nLINESTRING (-75.16652 39.95909, -75.16630 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n\n\n3896 rows × 19 columns\n\n\n\n\n\n1.11 Calculate a “Crash Index”\nLet’s calculate a “crash index” that provides a normalized measure of the crash frequency per street. To do this, we’ll need to:\n\nCalculate the total crash count divided by the street length, using the length column\nPerform a log transformation of the crash/length variable — use numpy’s log10() function\nNormalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n\nNote: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero.\nAfter this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9.\n\nmerged_CPD['crash_count_real']= merged_CPD['crash_count'].where(merged_CPD['crash_count'] &gt; 0)\nmerged_CPD = merged_CPD.dropna(axis=0)\n\n\nmerged_CPD[\"crash_per_length\"] = merged_CPD['crash_count'] / merged_CPD[\"length\"]\nmerged_CPD[\"crash_index\"] = np.log10(merged_CPD[\"crash_per_length\"]) \ncrash_index = merged_CPD[\"crash_index\"]\nmerged_CPD[\"crash_index_normalized\"] = (crash_index - crash_index.min()) / (crash_index.max() - crash_index.min())\nmerged_CPD \n\nC:\\Users\\Owner\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\geopandas\\geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\ncrash_count\ncrash_count_real\ncrash_per_length\ncrash_index\ncrash_index_normalized\n\n\n\n\n14\n109729474\n3425014859\n62154356\nTrue\nArch Street\nsecondary\nFalse\n126.087\nLINESTRING (-75.14847 39.95259, -75.14859 39.9...\n25 mph\n2\n0\n0\n0\n0\n0\n0\n0\n2.0\n2.0\n0.015862\n-1.799640\n0.253014\n\n\n15\n109729486\n110342146\n[12169305, 1052694387]\nTrue\nNorth Independence Mall East\nsecondary\nFalse\n123.116\nLINESTRING (-75.14832 39.95333, -75.14813 39.9...\n0\n[2, 3]\n0\n0\n0\n0\n0\n0\n0\n4.0\n4.0\n0.032490\n-1.488255\n0.344052\n\n\n22\n109729673\n109729699\n30908050\nTrue\nNorth 5th Street\nsecondary\nFalse\n106.498\nLINESTRING (-75.14749 39.95686, -75.14748 39.9...\n0\n2\n0\n0\n0\n0\n0\n0\n0\n3.0\n3.0\n0.028170\n-1.550220\n0.325935\n\n\n25\n109729699\n109811674\n[1047787360, 424804073, 1047787359]\nTrue\nCallowhill Street\ntrunk\nFalse\n135.769\nLINESTRING (-75.14724 39.95779, -75.14739 39.9...\n35 mph\n5\n0\n0\n0\n0\n0\n0\n0\n6.0\n6.0\n0.044193\n-1.354649\n0.383113\n\n\n26\n109729709\n109811681\n12166069\nFalse\nWillow Street\nresidential\nFalse\n135.689\nLINESTRING (-75.14714 39.95860, -75.14788 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4.0\n4.0\n0.029479\n-1.530485\n0.331705\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3880\n10660521817\n10660521823\n1145560245\nFalse\n0\nresidential\nFalse\n78.888\nLINESTRING (-75.18307 39.94021, -75.18315 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1.0\n1.0\n0.012676\n-1.897011\n0.224546\n\n\n3882\n10674041689\n10674041689\n1116747275\nFalse\nArch Street\ntertiary\nFalse\n59.612\nLINESTRING (-75.17821 39.95627, -75.17826 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22.0\n22.0\n0.369053\n-0.432911\n0.652595\n\n\n3883\n10674041689\n10674041689\n1116747275\nFalse\nArch Street\ntertiary\nTrue\n59.612\nLINESTRING (-75.17821 39.95627, -75.17827 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22.0\n22.0\n0.369053\n-0.432911\n0.652595\n\n\n3886\n11144117753\n109729699\n[367831920, 50725362, 61756380, 12189165]\nTrue\nNorth 5th Street\nsecondary\nFalse\n470.874\nLINESTRING (-75.14814 39.95362, -75.14807 39.9...\n25 mph\n1\n0\n0\nyes\n0\n0\n0\n0\n3.0\n3.0\n0.006371\n-2.195783\n0.137196\n\n\n3888\n11162290432\n110329835\n30908343\nTrue\nNorth 19th Street\nresidential\nFalse\n9.734\nLINESTRING (-75.17044 39.95855, -75.17045 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1.0\n1.0\n0.102733\n-0.988291\n0.490223\n\n\n\n\n884 rows × 23 columns\n\n\n\n\n\n1.12 Plot a histogram of the crash index values\nUse matplotlib’s hist() function to plot the crash index values from the previous step.\nYou should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!\n\nfig, ax = plt.subplots()\nx = merged_CPD[\"crash_index_normalized\"]\nplt.hist(x, bins=100,),\nplt.xlabel(\"Normalized Crash Index\"),\nplt.ylabel(\"Total Number of Roads\"),\nplt.title(\"Crash Index Value\"),\nplt.grid(True),\nplt.show()\n\n\n\n\n\n\n1.13 Plot an interactive map of the street networks, colored by the crash index\nYou can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\nTip: if you use the viridis color map, try using a “dark” tile set for better constrast of the colors.\n\nimport folium\n\n\nm = merged_CPD.explore(\n    column=\"crash_index_normalized\",\n    tiles=\"Cartodb dark matter\",\n)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/streetsandwebscrap.html#part-2-scraping-craigslist",
    "href": "analysis/streetsandwebscrap.html#part-2-scraping-craigslist",
    "title": "Street Networks & Web Scraping",
    "section": "Part 2: Scraping Craigslist",
    "text": "Part 2: Scraping Craigslist\nIn this part, we’ll be extracting information on apartments from Craigslist search results. You’ll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text.\nFor reference on CSS selectors, please see the notes from Week 6.\n\nPrimer: the Craigslist website URL\nWe’ll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist.\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThere are three components to this URL.\n\nThe base URL: http://philadelphia.craigslist.org/search/apa\nThe user’s search parameters: ?min_price=1&min_bedrooms=1&minSqft=1\n\n\nWe will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n\n\nThe URL hash: #search=1~gallery~0~0\n\n\nAs we will see later, this part will be important because it contains the search page result number.\n\nThe Craigslist website requires Javascript, so we’ll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want.\n\n\n2.1 Initialize a selenium driver and open Craigslist\nAs discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n\nInitialize the selenium driver\nUse the driver.get() function to open the following URL:\n\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThis will give you the search results for 1-bedroom apartments in Philadelphia.\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\n\n\ndriver = webdriver.Chrome()\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_25652\\537431734.py:1: ResourceWarning: unclosed file &lt;_io.BufferedWriter name='nul'&gt;\n  driver = webdriver.Chrome()\n\n\n\nurl = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\"\ndriver.get(url)\n\n\n\n2.2 Initialize your “soup”\nOnce selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver’s page source\n\npropertySoup = BeautifulSoup(driver.page_source, \"html.parser\")\n\n\n\n2.3 Parsing the HTML\nNow that we have our “soup” object, we can use BeautifulSoup to extract out the elements we need:\n\nUse the Web Inspector to identify the HTML element that holds the information on each apartment listing.\nUse BeautifulSoup to extract these elements from the HTML.\n\nAt the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page.\n\nelement = propertySoup.select(\"li.cl-search-result\")\n\n\n#element\n\n\nlen(element)\n\n120\n\n\n\nprint(element[0].prettify())\nprint(len(element))\n\n&lt;li class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7678961565\" title=\"A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces.\"&gt;\n &lt;div class=\"gallery-card\"&gt;\n  &lt;div class=\"cl-gallery\"&gt;\n   &lt;div class=\"gallery-inner\"&gt;\n    &lt;a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/north-wales-fresh-take-on-living/7678961565.html\"&gt;\n     &lt;div class=\"swipe\" style=\"visibility: visible;\"&gt;\n      &lt;div class=\"swipe-wrap\" style=\"width: 6768px;\"&gt;\n       &lt;div data-index=\"0\" style=\"width: 376px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\"&gt;\n        &lt;span class=\"loading icom-\"&gt;\n        &lt;/span&gt;\n        &lt;img alt=\"A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces. 1\" src=\"https://images.craigslist.org/00Z0Z_g553W757i6Y_0ew09G_300x300.jpg\"/&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"1\" style=\"width: 376px; left: -376px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"2\" style=\"width: 376px; left: -752px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"3\" style=\"width: 376px; left: -1128px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"4\" style=\"width: 376px; left: -1504px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"5\" style=\"width: 376px; left: -1880px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"6\" style=\"width: 376px; left: -2256px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"7\" style=\"width: 376px; left: -2632px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"8\" style=\"width: 376px; left: -3008px; transition-duration: 0ms; transform: translateX(-376px);\"&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-back-arrow icom-\"&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-forward-arrow icom-\"&gt;\n     &lt;/div&gt;\n    &lt;/a&gt;\n   &lt;/div&gt;\n   &lt;div class=\"dots\"&gt;\n    &lt;span class=\"dot selected\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n   &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/north-wales-fresh-take-on-living/7678961565.html\" tabindex=\"0\"&gt;\n   &lt;span class=\"label\"&gt;\n    A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces.\n   &lt;/span&gt;\n  &lt;/a&gt;\n  &lt;div class=\"meta\"&gt;\n   14 mins ago\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   &lt;span class=\"housing-meta\"&gt;\n    &lt;span class=\"post-bedrooms\"&gt;\n     1br\n    &lt;/span&gt;\n    &lt;span class=\"post-sqft\"&gt;\n     680ft\n     &lt;span class=\"exponent\"&gt;\n      2\n     &lt;/span&gt;\n    &lt;/span&gt;\n   &lt;/span&gt;\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   North Wales, Lansdale, Horsham, Doylestown, Ambler\n  &lt;/div&gt;\n  &lt;span class=\"priceinfo\"&gt;\n   $1,722\n  &lt;/span&gt;\n  &lt;button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n   &lt;/span&gt;\n  &lt;/button&gt;\n  &lt;button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n    hide\n   &lt;/span&gt;\n  &lt;/button&gt;\n &lt;/div&gt;\n&lt;/li&gt;\n\n120\n\n\n\n\n2.4 Find the relevant pieces of information\nWe will now focus on the first element in the list of 120 apartments. Use the prettify() function to print out the HTML for this first element.\nFrom this HTML, identify the HTML elements that hold:\n\nThe apartment price\nThe number of bedrooms\nThe square footage\nThe apartment title\n\nFor the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n\napt1 = element[0]\nspans = apt1.select('span')\nprint(f\"apartment price: {apt1.find('span',{'class' : 'priceinfo'}).text}\",end=\"\\n\")\n\nprint(f\"number of bedrooms: {apt1.find('span',{'class' : 'post-bedrooms'}).text[:1]}\",end=\"\\n\")\n\nprint(f\"square footage: {apt1.find('span',{'class' : 'post-sqft'}).text[:3]}\",end=\"\\n\")\n\nprint(f\"apartment title: {apt1.find('span',{'class' : 'label'}).text}\",end=\"\\n\")\n\napartment price: $1,722\nnumber of bedrooms: 1\nsquare footage: 680\napartment title: A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces.\n\n\n\n\n2.5 Functions to format the results\nIn this section, you’ll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\nI’ve started the functions to format the values. You should finish theses functions in this section.\nHints - You can use string formatting functions like string.replace() and string.strip() - The int() and float() functions can convert strings to numbers\n\ndef format_bedrooms(bedrooms_string):\n    # Format the bedrooms string and return an int\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    \n    return int(bedrooms_string.replace(\"br\",\"\"))\n\n\ndef format_size(size_string):\n    # Format the size string and return a float\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    \n    return float(f\"{size_string[:-3]}\") \n\n\ndef format_price(price_string):\n    # Format the price string and return a float\n    # \n    # This will involve using the string.strip() function to \n    # remove unwanted characters\n    return float(price_string.replace(\"$\",\"\").replace(\",\",\"\"))\n\n\n\n2.6 Putting it all together\nIn this part, you’ll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments.\nWe can get a specific page by changing the search=PAGE part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2gallery0~0\nIn the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\nFill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment.\nAfter filling in the missing pieces and executing the code cell, you should have a Data Frame called results that holds the data for 600 apartment listings.\n\nNotes\nBe careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I’ve added a sleep() function to the for loop to wait 30 seconds between scraping requests.\nIf the for loop gets stuck at the “Processing page X…” step for more than a minute or so, your IP address is probably banned temporarily, and you’ll have to wait a few minutes before trying again.\n\nfrom time import sleep\n\n\nresults = []\n\n# search in batches of 120 for 5 pages\n# NOTE: you will get temporarily banned if running more than ~5 pages or so\n# the API limits are more leninient during off-peak times, and you can try\n# experimenting with more pages\nmax_pages = 5\n\n# The base URL we will be using\nbase_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n\n# loop over each page of search results\nfor page_num in range(1, max_pages + 1):\n    print(f\"Processing page {page_num}...\")\n\n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"#search={page_num}~gallery~0~0\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n    apts = soup.select(\"li.cl-search-result\")\n    print(\"Number of apartments = \", len(apts))\n\n    # loop over each apartment in the list\n    page_results = []\n    for apt in apts:\n\n        # YOUR CODE: the bedrooms string\n        bedrooms = format_bedrooms(apt.find('span',{'class' : 'post-bedrooms'}).text)\n\n        # YOUR CODE: the size string\n        size = format_size(apt.find('span',{'class' : 'post-sqft'}).text)\n\n        # YOUR CODE: the title string\n        title = apt.find('span',{'class' : 'label'}).text\n\n        # YOUR CODE: the price string\n        price = format_price(apt.find('span',{'class' : 'priceinfo'}).text)\n\n\n        # Format using functions from Part 1.5\n        # bedrooms = format_bedrooms(bedrooms)\n        # size = format_size(size)\n        # price = format_price(price)\n\n        # Save the result\n        page_results.append([price, size, bedrooms, title])\n\n    # Create a dataframe and save\n    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\nProcessing page 1...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 2...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 3...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 4...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 5...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\n\n\n\nresults.head(4)\n\n\n\n\n\n\n\n\nprice\nsize\nbedrooms\ntitle\n\n\n\n\n0\n1722.0\n680.0\n1\nA fresh take on living: Explore our 1 BR, 680 ...\n\n\n1\n850.0\n400.0\n1\nCozy and clean apartment\n\n\n2\n1905.0\n926.0\n2\n2 bedroom, 9' Ceilings w/ Crown Molding, West ...\n\n\n3\n1788.0\n521.0\n1\n1/bd, Conference Room, Fully Equipped Kitchens\n\n\n\n\n\n\n\n\n\n\n2.7 Plotting the distribution of prices\nUse matplotlib’s hist() function to make two histograms for:\n\nApartment prices\nApartment prices per square foot (price / size)\n\nMake sure to add labels to the respective axes and a title describing the plot.\n\nfig,ax = plt.subplots(1,1)\nax.hist(\n    results[\"price\"],\n    rwidth=0.65,\n)\nax.set_title(\"Philadelphia Rental Apartment Price Frequencies - Source: Craigslist\")\nax.set_xlabel(\"Listing Price ($)\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\n\nSide note: rental prices per sq. ft. from Craigslist\nThe histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia’s rents compare to the other most populous cities:\n\nSource\n\nfig,ax = plt.subplots(1,1)\nax.hist(\n    results[\"price\"]/results[\"size\"],\n    rwidth=0.65\n)\n\nax.set_title(\"Price per Sq. Ft. of Philadelphia Rental Apartments - Source: Craigslist\")\nax.set_xlabel(\"Listing Price ($/sq. ft.)\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\n\n\n\n2.8 Comparing prices for different sizes\nUse altair to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms.\nMake sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\nWith this sort of plot, you can quickly see the outlier apartments in terms of size and price.\n\nimport altair as alt\n\ncolormap = alt.Scale(\n    domain=[0,1,2,3,4,5],\n    range=[\n        \"steelblue\",\n        \"cornflowerblue\",\n        \"chartreuse\",\n        \"#F4D03F\",\n        \"#D35400\",\n        \"red\",\n    ])\n\n# Step 1: Initialize the chart with the data\nscatter = alt.Chart(results).mark_circle(size=50).encode(\n    x=alt.X('price:Q'),\n    y=alt.Y('size:Q'),\n    color=alt.Color('bedrooms:Q',sort=\"ascending\",scale=colormap),\n    tooltip=[\"title\", \"price\",\"size\",\"bedrooms\"],\n).properties(width=350,height=150)\n\nscatter.interactive()"
  },
  {
    "objectID": "analysis/predictionmodelling.html",
    "href": "analysis/predictionmodelling.html",
    "title": "Predictive Modeling of Housing Prices in Philadelphia",
    "section": "",
    "text": "Due date: Wednesday, 12/6 by the end of the day\nLectures 12B and 13A will cover predictive modeling of housing prices in Philadelphia. We’ll extend that analysis in this section by:"
  },
  {
    "objectID": "analysis/predictionmodelling.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "href": "analysis/predictionmodelling.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "title": "Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness",
    "text": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness\n\n2.1 Load data from the Office of Property Assessment\nUse the requests package to query the CARTO API for single-family property assessment data in Philadelphia for properties that had their last sale during 2022.\nSources: - OpenDataPhilly - Metadata\n\nimport geopandas as gpd\nimport holoviews as hv\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport requests\nimport missingno as msno\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Show all columns\npd.options.display.max_columns = 999\n\n\n\n\n\n\n\n\n\n\n\n\n# The API endpoint\ncarto_url = \"https://phl.carto.com/api/v2/sql\"\n\n# Only pull 2022 sales for single family residential properties\nwhere = \"sale_date &gt;= '2022-01-01' and sale_date &lt;= '2022-12-31'\"\nwhere = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n\n\n# Create the query\nquery = f\"SELECT * FROM opa_properties_public WHERE {where}\"\n\n# Make the request\nparams = {\"q\": query, \"format\": \"geojson\", \"where\": where}\nresponse = requests.get(carto_url, params=params)\n\n# Make the GeoDataFrame\nsalesRaw = gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\n\n\nsalesRaw.head()\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\ncensus_tract\ncentral_air\ncross_reference\ndate_exterior_condition\ndepth\nexempt_building\nexempt_land\nexterior_condition\nfireplaces\nfrontage\nfuel\ngarage_spaces\ngarage_type\ngeneral_construction\ngeographic_ward\nhomestead_exemption\nhouse_extension\nhouse_number\ninterior_condition\nlocation\nmailing_address_1\nmailing_address_2\nmailing_care_of\nmailing_city_state\nmailing_street\nmailing_zip\nmarket_value\nmarket_value_date\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_of_rooms\nnumber_stories\noff_street_open\nother_building\nowner_1\nowner_2\nparcel_number\nparcel_shape\nquality_grade\nrecording_date\nregistry_number\nsale_date\nsale_price\nseparate_utilities\nsewer\nsite_type\nstate_code\nstreet_code\nstreet_designation\nstreet_direction\nstreet_name\nsuffix\ntaxable_building\ntaxable_land\ntopography\ntotal_area\ntotal_livable_area\ntype_heater\nunfinished\nunit\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\n\n\n\n\n0\nPOINT (-75.13389 40.03928)\n1056\n2022-05-24T00:00:00Z\nH\n241' N OF CHEW ST\n54230133\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n275\nN\nNone\nNone\n95.0\n0.0\n0.0\n7\n0.0\n15.0\nNone\n1.0\nNone\nB\n61\n0\nNone\n5732\n4\n5732 N 7TH ST\nWALKER MICHAEL\nNone\nNone\nSICKLERVILLE NJ\n44 FARMHOUSE RD\n08081\n133400\nNone\n1.0\n3.0\nNaN\n2.0\n1920.0\nNone\nWALKER MICHAEL\nNone\n612234600\nE\nC\n2023-10-04T00:00:00Z\n135N7 61\n2022-08-21T00:00:00Z\n21000\nNone\nNone\nNone\nNJ\n87930\nST\nN\n7TH\nNone\n106720.0\n26680.0\nF\n1425.0\n1164.0\nH\nNone\nNone\nNone\nI\n1925\nY\n19120\nRSA5\n1001602509\n24\nROW PORCH FRONT\n401090517\n\n\n1\nPOINT (-75.14337 40.00957)\n1145\n2022-05-24T00:00:00Z\nD\n415' N OF ERIE AVE\n54230032\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n198\nN\nNone\nNone\n45.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n0.0\nNone\nA\n43\n0\nNone\n3753\n4\n3753 N DELHI ST\nNone\nNone\nNone\nDELRAY BEACH FL\n4899 NW 6TH STREET\n33445\n73800\nNone\n1.0\n3.0\nNaN\n2.0\n1683.0\nNone\nRJ SIMPLE SOLUTION LLC\nNone\n432345900\nE\nC\n2023-10-04T00:00:00Z\n100N040379\n2022-06-13T00:00:00Z\n35000\nNone\nNone\nNone\nFL\n28040\nST\nN\nDELHI\nNone\n59040.0\n14760.0\nF\n720.0\n960.0\nH\nNone\nNone\nNone\nI\n1942\nY\n19140\nRM1\n1001175031\n24\nROW PORCH FRONT\n401090494\n\n\n2\nPOINT (-75.07249 40.01381)\n1357\n2022-05-24T00:00:00Z\nD\n119'11 1/2\" NE\n54228837\nH30\nSEMI/DET 2 STY MASONRY\n1\nSINGLE FAMILY\n299\nN\nNone\nNone\n76.0\n0.0\n0.0\n4\n0.0\n20.0\nNone\n0.0\nNone\nB\n62\n0\nNone\n5033\n4\n5033 DITMAN ST\nCSC INGEO\nNone\nNone\nPHILADELPHIA PA\n5033 DITMAN ST\n19124-2230\n119100\nNone\n1.0\n3.0\nNaN\n2.0\n698.0\nNone\nLISHANSKY MARINA\nNone\n622444400\nE\nC\n2023-10-02T00:00:00Z\n89N17 208\n2022-12-28T00:00:00Z\n1\nNone\nNone\nNone\nPA\n28660\nST\nNone\nDITMAN\nNone\n95280.0\n23820.0\nF\n1523.0\n1190.0\nB\nNone\nNone\nNone\nI\n1945\nNone\n19124\nRSA5\n1001181518\n32\nTWIN CONVENTIONAL\n401090706\n\n\n3\nPOINT (-75.12854 40.03916)\n1766\n2022-05-24T00:00:00Z\nNone\n71'8\" E LAWRENCE ST\n54226519\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n274\nNone\nNone\nNone\n88.0\n0.0\n0.0\n4\n0.0\n14.0\nNone\n0.0\nNone\nA\n61\n0\nNone\n416\n4\n416 W GRANGE AVE\nNone\nNone\nNone\nPHILADELPHIA PA\n416 W GRANGE AVE\n19120-1854\n124100\nNone\n1.0\n3.0\nNaN\n2.0\n957.0\nNone\nWALLACE DANE\nNone\n612061100\nE\nC\n2023-09-25T00:00:00Z\n122N2 150\n2022-10-26T00:00:00Z\n1\nNone\nNone\nNone\nPA\n38040\nAVE\nW\nGRANGE\nNone\n99280.0\n24820.0\nF\n1241.0\n1104.0\nNone\nNone\nNone\nNone\nI\n1953\nY\n19120\nRSA5\n1001249126\n24\nROW PORCH FRONT\n401091115\n\n\n4\nPOINT (-75.17362 39.99887)\n2005\n2022-05-24T00:00:00Z\nD\n261'4\" N OF SOMERSET\n54217081\nO50\nROW 3 STY MASONRY\n1\nSINGLE FAMILY\n172\nN\nNone\nNone\n56.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n0.0\nNone\nB\n38\n0\nNone\n2834\n4\n2834 N 26TH ST\nNEAL KIYONNA\nNone\nNone\nPHILADELPHIA PA\n6007 N FRONT ST\n19120\n92900\nNone\n0.0\n5.0\nNaN\n3.0\n2457.0\nNone\nNEAL KIYONNA\nNone\n381152100\nE\nC+\n2023-08-28T00:00:00Z\n035N230348\n2022-05-11T00:00:00Z\n1\nNone\nNone\nNone\nPA\n88300\nST\nN\n26TH\nNone\n74320.0\n18580.0\nF\n896.0\n1636.0\nH\nNone\nNone\nNone\nI\n1940\nY\n19132\nRSA5\n1001643492\n24\nROW PORCH FRONT\n401092557\n\n\n\n\n\n\n\n\nlen(salesRaw)\n\n24456"
  },
  {
    "objectID": "analysis/predictionmodelling.html#clean-data",
    "href": "analysis/predictionmodelling.html#clean-data",
    "title": "Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Clean data",
    "text": "Clean data\n\n# The feature columns we want to use\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    \"zip_code\",\n    \"geometry\"\n]\n\n# Trim to these columns and remove NaNs\nsales = salesRaw[cols].dropna()\n\n# Trim zip code to only the first five digits\nsales['zip_code'] = sales['zip_code'].astype(str).str.slice(0, 5)\n\n\nlen(sales)\n\n23463\n\n\n\n2.2 Load data for census tracts and neighborhoods\nLoad various Philadelphia-based regions that we will use in our analysis.\n\nCensus tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\nNeighborhoods can be downloaded from: https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n\n\nneigh = gpd.read_file(\"https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\")\ntract = gpd.read_file(\"Census_Tracts_2010.geojson\")\n\n\ntract.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n2.3 Spatially join the sales data and neighborhoods/census tracts.\nPerform a spatial join, such that each sale has an associated neighborhood and census tract.\nNote: After performing the first spatial join, you will need to use the drop() function to remove the index_right column; otherwise an error will be raised on the second spatial join about duplicate columns.\n\njoin = gpd.sjoin(\n        sales, tract.to_crs(tract.crs), predicate=\"within\").drop(columns=[\"index_right\"])\n# print(join.columns)\n\nsales_full = gpd.sjoin(\n    join,\n        neigh.to_crs(neigh.crs), predicate = \"within\").drop(columns=[\"index_right\"])\nsales_full.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\ngeometry\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id\ncreated_at\nupdated_at\n\n\n\n\n0\n21000\n1164.0\n1425.0\n1.0\n0.0\n1.0\n3.0\n2.0\n7\n19120\nPOINT (-75.13389 40.03928)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n586\n1\n1188.0\n1575.0\n0.0\n0.0\n1.0\n3.0\n2.0\n3\n19120\nPOINT (-75.13367 40.04039)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1041\n137000\n1176.0\n1234.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13277 40.04028)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1567\n259000\n1310.0\n1754.0\n1.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13173 40.03868)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n2000\n220000\n1272.0\n1445.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13245 40.03959)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n\n\n\n\n\n\nsales_full.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\ngeometry\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id\ncreated_at\nupdated_at\n\n\n\n\n0\n21000\n1164.0\n1425.0\n1.0\n0.0\n1.0\n3.0\n2.0\n7\n19120\nPOINT (-75.13389 40.03928)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n586\n1\n1188.0\n1575.0\n0.0\n0.0\n1.0\n3.0\n2.0\n3\n19120\nPOINT (-75.13367 40.04039)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1041\n137000\n1176.0\n1234.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13277 40.04028)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1567\n259000\n1310.0\n1754.0\n1.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13173 40.03868)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n2000\n220000\n1272.0\n1445.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13245 40.03959)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n\n\n\n\n\n\n\n2.4 Train a Random Forest on the sales data\nIn this step, you should follow the steps outlined in lecture to preprocess and train your model. We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\nPreprocessing Requirements\nTrim the sales data to those sales with prices between $3,000 and $1 million\nSet up a pipeline that includes both numerical columns and categorical columns Include one-hot encoded variable for the neighborhood of the sale, instead of ZIP code. We don’t want to include multiple location based categories, since they encode much of the same information.\nTraining requirements - Use a 70/30% training/test split and predict the log of the sales price. - Use GridSearchCV to perform a k-fold cross validation that optimize at least 2 hyperparameters of the RandomForestRegressor - After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\nNote: You don’t need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!\n\ntrim_sales = sales_full.loc[sales_full[\"sale_price\"]&gt;3000].loc[sales_full[\"sale_price\"]&lt;1000000]\ntrim_sales.rename(columns={\"mapname\":\"neighborhood\"},inplace=True)\n\n# The feature columns we want to use\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    \"neighborhood\",\n    \"log(sale_price)\"\n]\n\n# Trim to these columns and remove NaNs\ntrim_sales[\"log(sale_price)\"] = np.log(trim_sales[\"sale_price\"])\nsaleDF = trim_sales[cols].dropna()\n\n\nsaleDF.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nneighborhood\nlog(sale_price)\n\n\n\n\n0\n21000\n1164.0\n1425.0\n1.0\n0.0\n1.0\n3.0\n2.0\n7\nOlney\n9.952278\n\n\n1041\n137000\n1176.0\n1234.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\nOlney\n11.827736\n\n\n1567\n259000\n1310.0\n1754.0\n1.0\n0.0\n2.0\n3.0\n2.0\n4\nOlney\n12.464583\n\n\n2000\n220000\n1272.0\n1445.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\nOlney\n12.301383\n\n\n2618\n200000\n1304.0\n1360.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\nOlney\n12.206073\n\n\n\n\n\n\n\n\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nfrom itertools import compress\n\nnumCols = []\ntxtCols = []\nfor col in cols:\n\n    numCols.append(is_numeric_dtype(saleDF[col]))\n    txtCols.append(~numCols[-1])\n\n\nnumCols = list(compress(cols,numCols))[1:-1] \ncatCols = [\"exterior_condition\",\"neighborhood\"]\n\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numCols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catCols),\n    ]\n)\nnumCols\n\n['total_livable_area',\n 'total_area',\n 'garage_spaces',\n 'fireplaces',\n 'number_of_bathrooms',\n 'number_of_bedrooms',\n 'number_stories']\n\n\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(saleDF, test_size=0.3, random_state=42)\n\n# the target labels: log of sale price\ny_train = train_set[\"log(sale_price)\"]\ny_test = test_set[\"log(sale_price)\"]\n\n\n\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=42)\n                                       \n)\n\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [2, 5, 7, 9, 13, 21, 33, 51],\n}\n\n# Create the grid and use 3-fold CV\ngrid = GridSearchCV(pipe, param_grid, cv=3, verbose=1)\n\n# Run the search\ngrid.fit(train_set, y_train)\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'neighborhood'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'neighborhood'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['total_livable_area',\n                                                   'total_area',\n                                                   'garage_spaces',\n                                                   'fireplaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'neighborhood'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['total_livable_area', 'total_area',\n                                  'garage_spaces', 'fireplaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'neighborhood'])])num['total_livable_area', 'total_area', 'garage_spaces', 'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories']StandardScalerStandardScaler()cat['exterior_condition', 'neighborhood']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nprint(\"Getting 3-fold cross validation results.\")\n\nscores = cross_val_score(\n    pipe,\n    train_set,\n    y_train,\n    cv=3,\n)\n\nprint(\"Completed\")\n# # Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Standard deviation = \", scores.std())\n\nGetting 3-fold cross validation results.\nCompleted\nR^2 scores =  [0.56575893 0.51311316 0.53833274]\nScores mean =  0.5390682769418876\nStandard deviation =  0.021498838781376265\n\n\n\nprint(f\"The optimal model of our 64 candiates had a max depth of {grid.best_estimator_.steps[1][1].max_depth} and an n_estimator count of {grid.best_estimator_.steps[1][1].n_estimators}\\nleading to an accuracy score of {grid.best_estimator_.score(test_set,y_test)} or {grid.best_estimator_.score(test_set,y_test)*100:.2f}% on the test set.\")\n\n\n\nprint(\"\\n\\nCV Score Report ...\\n\")\nprint(\"R^2 scores:\\n\\tFold 1 = \", scores[0],\"\\n\\tFold 2 = \", scores[1],\"\\n\\tFold 3 = \", scores[2],)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score standard deviation = \", scores.std())\n\nThe optimal model of our 64 candiates had a max depth of 51 and an n_estimator count of 200\nleading to an accuracy score of 0.5573963547661122 or 55.74% on the test set.\n\n\nCV Score Report ...\n\nR^2 scores:\n    Fold 1 =  0.5657589318132974 \n    Fold 2 =  0.5131131592492879 \n    Fold 3 =  0.5383327397630778\nScores mean =  0.5390682769418876\nScore standard deviation =  0.021498838781376265\n\n\n\n#not sure why I am getting an error for transformer but the cells after still work\nrandom_forest = grid.best_estimator_.named_steps[\"randomforestregressor\"]\nfeatures = numCols + list(transformer.named_transformers_['cat'].get_feature_names_out())\n\n\nimportance = pd.DataFrame(\n    {\"Feature\": features, \"Importance\": random_forest.feature_importances_}\n)\n\nimportance = importance.sort_values(\"Importance\", ascending=False).iloc[:30]\n\nimportance.head(n=20)\n\n\nimportance.hvplot.barh(x=\"Feature\", y=\"Importance\", height=700, flip_yaxis=True)\n\nAttributeError: 'ColumnTransformer' object has no attribute 'transformers_'\n\n\n\n\n2.5 Calculate the percent error of your model predictions for each sale in the test set\nFit your best model and use it to make predictions on the test set.\nNote: This should be the percent error in terms of sale price. You’ll need to convert if your model predicted the log of sales price!\n\nimport math\n\npredictions = grid.best_estimator_.predict(test_set)\n\n\nerrors = predictions - y_test \n\nprecent_errors = math.e**(predictions) - math.e**(y_test) \n\nprint(errors[0])\nprint(precent_errors[0])\nprint()\n\nprint(test_set[0:1][\"log(sale_price)\"])\nprint()\nprint(predictions[0])\n\n1.5150772918914885\n74545.2282497846\n\n16061    11.350407\nName: log(sale_price), dtype: float64\n\n12.527490609299118\n\n\n\n\n2.6 Make a data frame with percent errors and census tract info for each sale in the test set\nCreate a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\nNotes\n\nWhen using the “train_test_split()” function, the index of the test data frame includes the labels from the original sales data frame\nYou can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\nAdd a new column to this data frame holding the percent error data\nMake sure to use the percent error and not the absolute percent error\n\n\ntest_saleDF = saleDF.loc[test_set.index]\n# test_set.index\n\ntest_saleDF[\"log(predictions)\"] = predictions\ntest_saleDF[\"predictions\"] = math.e**predictions\ntest_saleDF[\"error\"] = precent_errors\ntest_saleDF[\"percent_error\"] = (precent_errors)/test_saleDF[\"sale_price\"]*100\n\ntest_saleDF = sales_full.merge(test_saleDF,how=\"right\")\n\n\ntest_saleDF[[\"sale_price\",\"log(sale_price)\",\"log(predictions)\",\"predictions\",\"error\",\"percent_error\"]].tail()\n\n\n\n\n\n\n\n\nsale_price\nlog(sale_price)\nlog(predictions)\npredictions\nerror\npercent_error\n\n\n\n\n5352\n181000\n12.106252\n11.345868\n84615.078520\n-96384.921480\n-53.251338\n\n\n5353\n300000\n12.611538\n12.959598\n424895.325384\n124895.325384\n41.631775\n\n\n5354\n265000\n12.487485\n12.673232\n319091.035201\n54091.035201\n20.411711\n\n\n5355\n380000\n12.847927\n12.646338\n310624.010585\n-69375.989415\n-18.256839\n\n\n5356\n134000\n11.805595\n12.530408\n276622.279170\n142622.279170\n106.434537\n\n\n\n\n\n\n\n\n\n2.8 Plot a map of the median percent error by census tract\n\nYou’ll want to group your data frame of test sales by the GEOID10 column and take the median of you percent error column\nMerge the census tract geometries back in and use geopandas to plot.\n\n\ntractPE = tract.merge(test_saleDF.groupby(\"GEOID10\").agg({\"predictions\": \"median\",\"error\": \"median\",\"percent_error\" : \"median\"}), on=\"GEOID10\")\n# sales_full.geometry\ntractPE.columns\n\nIndex(['OBJECTID', 'STATEFP10', 'COUNTYFP10', 'TRACTCE10', 'GEOID10', 'NAME10',\n       'NAMELSAD10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10',\n       'INTPTLAT10', 'INTPTLON10', 'LOGRECNO', 'geometry', 'predictions',\n       'error', 'percent_error'],\n      dtype='object')\n\n\n\ntractPE.rename(columns={\"NAME10\":\"Census Tract #\",\"predictions\":\"Predicted Sale Price ($)\",\"error\":\"Prediction Error ($)\",\"percent_error\":\"Percent Error\"}).explore(\n    column = \"Percent Error\",\n    cmap = \"viridis\",\n    tiles = \"CartoDB positron\",\n    tooltip = [\"Census Tract #\",\"Predicted Sale Price ($)\",\"Prediction Error ($)\",\"Percent Error\"]\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n2.9 Compare the percent errors in Qualifying Census Tracts and other tracts\nQualifying Census Tracts are a poverty designation that HUD uses to allocate housing tax credits\n\nI’ve included a list of the census tract names that qualify in Philadelphia\nAdd a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\nThen, group by this new column and calculate the median percent error\n\nYou should find that the algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts\n\nqct = ['5',\n '20',\n '22',\n '28.01',\n '30.01',\n '30.02',\n '31',\n '32',\n '33',\n '36',\n '37.01',\n '37.02',\n '39.01',\n '41.01',\n '41.02',\n '56',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '69',\n '70',\n '71.01',\n '71.02',\n '72',\n '73',\n '74',\n '77',\n '78',\n '80',\n '81.01',\n '81.02',\n '82',\n '83.01',\n '83.02',\n '84',\n '85',\n '86.01',\n '86.02',\n '87.01',\n '87.02',\n '88.01',\n '88.02',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '98.01',\n '100',\n '101',\n '102',\n '103',\n '104',\n '105',\n '106',\n '107',\n '108',\n '109',\n '110',\n '111',\n '112',\n '113',\n '119',\n '121',\n '122.01',\n '122.03',\n '131',\n '132',\n '137',\n '138',\n '139',\n '140',\n '141',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '151.01',\n '151.02',\n '152',\n '153',\n '156',\n '157',\n '161',\n '162',\n '163',\n '164',\n '165',\n '167.01',\n '167.02',\n '168',\n '169.01',\n '169.02',\n '170',\n '171',\n '172.01',\n '172.02',\n '173',\n '174',\n '175',\n '176.01',\n '176.02',\n '177.01',\n '177.02',\n '178',\n '179',\n '180.02',\n '188',\n '190',\n '191',\n '192',\n '195.01',\n '195.02',\n '197',\n '198',\n '199',\n '200',\n '201.01',\n '201.02',\n '202',\n '203',\n '204',\n '205',\n '206',\n '208',\n '239',\n '240',\n '241',\n '242',\n '243',\n '244',\n '245',\n '246',\n '247',\n '249',\n '252',\n '253',\n '265',\n '267',\n '268',\n '271',\n '274.01',\n '274.02',\n '275',\n '276',\n '277',\n '278',\n '279.01',\n '279.02',\n '280',\n '281',\n '282',\n '283',\n '284',\n '285',\n '286',\n '287',\n '288',\n '289.01',\n '289.02',\n '290',\n '291',\n '293',\n '294',\n '298',\n '299',\n '300',\n '301',\n '302',\n '305.01',\n '305.02',\n '309',\n '311.01',\n '312',\n '313',\n '314.01',\n '314.02',\n '316',\n '318',\n '319',\n '321',\n '325',\n '329',\n '330',\n '337.01',\n '345.01',\n '357.01',\n '376',\n '377',\n '380',\n '381',\n '382',\n '383',\n '389',\n '390']\n\n\ntractPE[\"is_qct\"] = tractPE[\"NAME10\"].isin(qct)\nqctGroup = tractPE.groupby(\"is_qct\",as_index=False).agg({\"predictions\": \"median\",\"error\": [\"mean\",\"median\"],\"percent_error\" : [\"mean\",\"median\"]})\nqct_t = qctGroup[1:][\"percent_error\"][\"median\"].squeeze()\nqct_f = qctGroup[:1][\"percent_error\"][\"median\"].squeeze()\nqctGroup"
  },
  {
    "objectID": "analysis/evictions.html",
    "href": "analysis/evictions.html",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport hvplot.pandas \nimport matplotlib.pyplot as plt\nimport rasterio as rio\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\nThis assignment will contain two parts:"
  },
  {
    "objectID": "analysis/evictions.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "href": "analysis/evictions.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "Part 1: Exploring Evictions and Code Violations in Philadelphia",
    "text": "Part 1: Exploring Evictions and Code Violations in Philadelphia\nIn this assignment, we’ll explore spatial trends evictions in Philadelphia using data from the Eviction Lab and building code violations using data from OpenDataPhilly.\nWe’ll be exploring the idea that evictions can occur as retaliation against renters for reporting code violations. Spatial correlations between evictions and code violations from the City’s Licenses and Inspections department can offer some insight into this question.\nA couple of interesting background readings: - HuffPost article - PlanPhilly article"
  },
  {
    "objectID": "analysis/evictions.html#explore-eviction-lab-data",
    "href": "analysis/evictions.html#explore-eviction-lab-data",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.1 Explore Eviction Lab Data",
    "text": "1.1 Explore Eviction Lab Data\nThe Eviction Lab built the first national database for evictions. If you aren’t familiar with the project, you can explore their website: https://evictionlab.org/\n\n1.1.1 Read data using geopandas\nThe first step is to read the eviction data by census tract using geopandas. The data for all of Pennsylvania by census tract is available in the data/ folder in a GeoJSON format.\nLoad the data file “PA-tracts.geojson” using geopandas\nNote: If you’d like to see all columns in the data frame, you can increase the max number of columns using pandas display options:\n\nCityLimitsdf = gpd.read_file(\"./Data/City_Limits.geojson\")\nPATractsdf = gpd.read_file(\"./Data/PA-tracts.geojson\")\npprTreePoints = gpd.read_file(\"./Data/ppr_tree_canopy_points_2015.geojson\")\n\n\n#CityLimitsdf.head()\n#PATractsdf.head()\n#pprTreePoints.head(5)\n\n\n\n1.1.2 Explore and trim the data\nWe will need to trim data to Philadelphia only. Take a look at the data dictionary for the descriptions of the various columns in top-level repository folder: eviction_lab_data_dictionary.txt\nNote: the column names are shortened — see the end of the above file for the abbreviations. The numbers at the end of the columns indicate the years. For example, e-16 is the number of evictions in 2016.\nTake a look at the individual columns and trim to census tracts in Philadelphia. (Hint: Philadelphia is both a city and a county).\n\ntracts_filtered = PATractsdf[PATractsdf['pl'].str.startswith('Philadelphia')]\n\n\ntracts_filtered.head()\n\n\n\n\n\n\n\n\nGEOID\nwest\nsouth\neast\nnorth\nn\npl\np-00\npr-00\nroh-00\npro-00\nmgr-00\nmhi-00\nmpv-00\nrb-00\npw-00\npaa-00\nph-00\npai-00\npa-00\npnp-00\npm-00\npo-00\nef-00\ne-00\ner-00\nefr-00\nlf-00\nimputed-00\nsubbed-00\np-01\npr-01\nroh-01\npro-01\nmgr-01\nmhi-01\nmpv-01\nrb-01\npw-01\npaa-01\nph-01\npai-01\npa-01\npnp-01\npm-01\npo-01\nef-01\ne-01\ner-01\nefr-01\nlf-01\nimputed-01\nsubbed-01\np-02\npr-02\nroh-02\npro-02\nmgr-02\nmhi-02\nmpv-02\nrb-02\npw-02\npaa-02\nph-02\npai-02\npa-02\npnp-02\npm-02\npo-02\nef-02\ne-02\ner-02\nefr-02\nlf-02\nimputed-02\nsubbed-02\np-03\npr-03\nroh-03\npro-03\nmgr-03\nmhi-03\nmpv-03\nrb-03\npw-03\npaa-03\nph-03\npai-03\npa-03\npnp-03\npm-03\npo-03\nef-03\ne-03\ner-03\nefr-03\nlf-03\nimputed-03\nsubbed-03\np-04\npr-04\nroh-04\npro-04\nmgr-04\nmhi-04\nmpv-04\nrb-04\npw-04\npaa-04\nph-04\npai-04\npa-04\npnp-04\npm-04\npo-04\nef-04\ne-04\ner-04\nefr-04\nlf-04\nimputed-04\nsubbed-04\np-05\npr-05\nroh-05\npro-05\nmgr-05\nmhi-05\nmpv-05\nrb-05\npw-05\npaa-05\nph-05\npai-05\npa-05\npnp-05\npm-05\npo-05\nef-05\ne-05\ner-05\nefr-05\nlf-05\nimputed-05\nsubbed-05\np-06\npr-06\nroh-06\npro-06\nmgr-06\nmhi-06\nmpv-06\nrb-06\npw-06\npaa-06\nph-06\npai-06\npa-06\npnp-06\npm-06\npo-06\nef-06\ne-06\ner-06\nefr-06\nlf-06\nimputed-06\nsubbed-06\np-07\npr-07\nroh-07\npro-07\nmgr-07\nmhi-07\nmpv-07\nrb-07\npw-07\npaa-07\nph-07\npai-07\npa-07\npnp-07\npm-07\npo-07\nef-07\ne-07\ner-07\nefr-07\nlf-07\nimputed-07\nsubbed-07\np-08\npr-08\nroh-08\npro-08\nmgr-08\nmhi-08\nmpv-08\nrb-08\npw-08\npaa-08\nph-08\npai-08\npa-08\npnp-08\npm-08\npo-08\nef-08\ne-08\ner-08\nefr-08\nlf-08\nimputed-08\nsubbed-08\np-09\npr-09\nroh-09\npro-09\nmgr-09\nmhi-09\nmpv-09\nrb-09\npw-09\npaa-09\nph-09\npai-09\npa-09\npnp-09\npm-09\npo-09\nef-09\ne-09\ner-09\nefr-09\nlf-09\nimputed-09\nsubbed-09\np-10\npr-10\nroh-10\npro-10\nmgr-10\nmhi-10\nmpv-10\nrb-10\npw-10\npaa-10\nph-10\npai-10\npa-10\npnp-10\npm-10\npo-10\nef-10\ne-10\ner-10\nefr-10\nlf-10\nimputed-10\nsubbed-10\np-11\npr-11\nroh-11\npro-11\nmgr-11\nmhi-11\nmpv-11\nrb-11\npw-11\npaa-11\nph-11\npai-11\npa-11\npnp-11\npm-11\npo-11\nef-11\ne-11\ner-11\nefr-11\nlf-11\nimputed-11\nsubbed-11\np-12\npr-12\nroh-12\npro-12\nmgr-12\nmhi-12\nmpv-12\nrb-12\npw-12\npaa-12\nph-12\npai-12\npa-12\npnp-12\npm-12\npo-12\nef-12\ne-12\ner-12\nefr-12\nlf-12\nimputed-12\nsubbed-12\np-13\npr-13\nroh-13\npro-13\nmgr-13\nmhi-13\nmpv-13\nrb-13\npw-13\npaa-13\nph-13\npai-13\npa-13\npnp-13\npm-13\npo-13\nef-13\ne-13\ner-13\nefr-13\nlf-13\nimputed-13\nsubbed-13\np-14\npr-14\nroh-14\npro-14\nmgr-14\nmhi-14\nmpv-14\nrb-14\npw-14\npaa-14\nph-14\npai-14\npa-14\npnp-14\npm-14\npo-14\nef-14\ne-14\ner-14\nefr-14\nlf-14\nimputed-14\nsubbed-14\np-15\npr-15\nroh-15\npro-15\nmgr-15\nmhi-15\nmpv-15\nrb-15\npw-15\npaa-15\nph-15\npai-15\npa-15\npnp-15\npm-15\npo-15\nef-15\ne-15\ner-15\nefr-15\nlf-15\nimputed-15\nsubbed-15\np-16\npr-16\nroh-16\npro-16\nmgr-16\nmhi-16\nmpv-16\nrb-16\npw-16\npaa-16\nph-16\npai-16\npa-16\npnp-16\npm-16\npo-16\nef-16\ne-16\ner-16\nefr-16\nlf-16\nimputed-16\nsubbed-16\ngeometry\n\n\n\n\n435\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n25.0\n21.0\n1.51\n1.80\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.70\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.70\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.00\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.00\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.00\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.00\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.00\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.00\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\n\n\n436\n42101000200\n-75.1631\n39.9529\n-75.1511\n39.9578\n2\nPhiladelphia County, Pennsylvania\n1362.00\n56.42\n374.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1362.00\n56.42\n415.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1362.00\n56.42\n455.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n4.0\n4.0\n0.88\n0.88\n1.0\n0.0\n0.0\n1362.00\n56.42\n496.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n3.0\n3.0\n0.60\n0.60\n1.0\n0.0\n0.0\n1362.00\n56.42\n537.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n6.0\n6.0\n1.12\n1.12\n1.0\n0.0\n0.0\n1633.00\n3.45\n578.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n1.0\n0.0\n0.00\n0.17\n1.0\n0.0\n0.0\n1633.00\n3.45\n618.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n6.0\n6.0\n0.97\n0.97\n1.0\n0.0\n0.0\n1633.00\n3.45\n659.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n9.0\n7.0\n1.06\n1.37\n0.0\n0.0\n1.0\n1633.00\n3.45\n700.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n11.0\n7.0\n1.00\n1.57\n0.0\n0.0\n1.0\n1633.00\n3.45\n740.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n6.0\n5.0\n0.68\n0.81\n0.0\n0.0\n1.0\n2937.0\n5.07\n781.0\n68.21\n905.0\n49928.0\n261100.0\n26.4\n22.64\n9.67\n2.69\n0.10\n63.16\n0.03\n1.40\n0.31\n6.0\n1.0\n0.13\n0.77\n0.0\n0.0\n1.0\n2331.0\n15.78\n792.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n9.0\n6.0\n0.76\n1.14\n0.0\n0.0\n1.0\n2331.0\n15.78\n802.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n8.0\n3.0\n0.37\n1.00\n0.0\n0.0\n1.0\n2331.0\n15.78\n813.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n14.0\n10.0\n1.23\n1.72\n0.0\n0.0\n1.0\n2331.0\n15.78\n824.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n5.0\n3.0\n0.36\n0.61\n0.0\n0.0\n1.0\n2331.0\n15.78\n834.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n10.0\n9.0\n1.08\n1.20\n0.0\n0.0\n1.0\n2331.0\n15.78\n845.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n11.0\n8.0\n0.95\n1.30\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\n\n\n437\n42101000300\n-75.1798\n39.9544\n-75.1623\n39.9599\n3\nPhiladelphia County, Pennsylvania\n2570.00\n12.16\n861.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2570.00\n12.16\n915.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2570.00\n12.16\n969.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n14.0\n12.0\n1.24\n1.44\n1.0\n0.0\n0.0\n2570.00\n12.16\n1023.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n21.0\n17.0\n1.66\n2.05\n1.0\n0.0\n0.0\n2570.00\n12.16\n1077.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n23.0\n23.0\n2.13\n2.13\n1.0\n0.0\n0.0\n4497.00\n1.63\n1132.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n12.0\n10.0\n0.88\n1.06\n1.0\n0.0\n0.0\n4497.00\n1.63\n1186.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n19.0\n16.0\n1.35\n1.60\n1.0\n0.0\n0.0\n4497.00\n1.63\n1240.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n21.0\n7.0\n0.56\n1.69\n0.0\n0.0\n1.0\n4497.00\n1.63\n1294.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n25.0\n11.0\n0.85\n1.93\n0.0\n0.0\n1.0\n4497.00\n1.63\n1348.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n27.0\n12.0\n0.89\n2.00\n0.0\n0.0\n1.0\n3169.0\n7.20\n1402.0\n75.58\n1827.0\n71250.0\n451800.0\n28.0\n72.26\n10.22\n4.26\n0.03\n10.35\n0.03\n2.52\n0.32\n24.0\n13.0\n0.93\n1.71\n0.0\n0.0\n1.0\n3405.0\n4.17\n1489.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n21.0\n8.0\n0.54\n1.41\n0.0\n0.0\n1.0\n3405.0\n4.17\n1575.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n27.0\n12.0\n0.76\n1.71\n0.0\n0.0\n1.0\n3405.0\n4.17\n1662.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n31.0\n10.0\n0.60\n1.87\n0.0\n0.0\n1.0\n3405.0\n4.17\n1749.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n27.0\n14.0\n0.80\n1.54\n0.0\n0.0\n1.0\n3405.0\n4.17\n1835.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n18.0\n5.0\n0.27\n0.98\n0.0\n0.0\n1.0\n3405.0\n4.17\n1922.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n26.0\n14.0\n0.73\n1.35\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\n\n\n438\n42101000801\n-75.1833\n39.9486\n-75.1773\n39.9515\n8.01\nPhiladelphia County, Pennsylvania\n1478.00\n14.40\n810.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1478.00\n14.40\n801.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1478.00\n14.40\n793.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n7.0\n5.0\n0.63\n0.88\n1.0\n0.0\n0.0\n1478.00\n14.40\n784.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n19.0\n13.0\n1.66\n2.42\n1.0\n0.0\n0.0\n1478.00\n14.40\n775.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n17.0\n14.0\n1.81\n2.19\n1.0\n0.0\n0.0\n1344.37\n11.10\n767.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n10.0\n6.0\n0.78\n1.30\n1.0\n0.0\n0.0\n1344.37\n11.10\n758.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n12.0\n7.0\n0.92\n1.58\n1.0\n0.0\n0.0\n1344.37\n11.10\n749.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n12.0\n5.0\n0.67\n1.60\n0.0\n0.0\n1.0\n1344.37\n11.10\n740.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n11.0\n4.0\n0.54\n1.49\n0.0\n0.0\n1.0\n1344.37\n11.10\n732.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n10.0\n2.0\n0.27\n1.37\n0.0\n0.0\n1.0\n1562.0\n2.46\n723.0\n71.09\n2001.0\n83125.0\n459900.0\n25.9\n78.04\n2.94\n5.76\n0.00\n10.82\n0.26\n1.92\n0.26\n14.0\n4.0\n0.55\n1.94\n0.0\n0.0\n1.0\n1692.0\n3.25\n734.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n13.0\n7.0\n0.95\n1.77\n0.0\n0.0\n1.0\n1692.0\n3.25\n746.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n7.0\n0.0\n0.00\n0.94\n0.0\n0.0\n1.0\n1692.0\n3.25\n757.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n15.0\n3.0\n0.40\n1.98\n0.0\n0.0\n1.0\n1692.0\n3.25\n768.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n10.0\n4.0\n0.52\n1.30\n0.0\n0.0\n1.0\n1692.0\n3.25\n780.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n16.0\n8.0\n1.03\n2.05\n0.0\n0.0\n1.0\n1692.0\n3.25\n791.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n13.0\n4.0\n0.51\n1.64\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\n\n\n439\n42101000804\n-75.1712\n39.9470\n-75.1643\n39.9501\n8.04\nPhiladelphia County, Pennsylvania\n3301.00\n14.40\n2058.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n3301.00\n14.40\n2050.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n3301.00\n14.40\n2042.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n22.0\n18.0\n0.88\n1.08\n1.0\n0.0\n0.0\n3301.00\n14.40\n2033.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n31.0\n21.0\n1.03\n1.52\n1.0\n0.0\n0.0\n3301.00\n14.40\n2025.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n18.0\n15.0\n0.74\n0.89\n1.0\n0.0\n0.0\n3002.54\n11.10\n2017.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n28.0\n19.0\n0.94\n1.39\n1.0\n0.0\n0.0\n3002.54\n11.10\n2009.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n14.0\n13.0\n0.65\n0.70\n1.0\n0.0\n0.0\n3002.54\n11.10\n2001.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n33.0\n11.0\n0.55\n1.65\n0.0\n0.0\n1.0\n3002.54\n11.10\n1992.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n17.0\n4.0\n0.20\n0.85\n0.0\n0.0\n1.0\n3002.54\n11.10\n1984.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n27.0\n8.0\n0.40\n1.36\n0.0\n0.0\n1.0\n3609.0\n7.69\n1976.0\n76.32\n1562.0\n75357.0\n330200.0\n26.0\n78.55\n2.72\n4.96\n0.03\n11.75\n0.03\n1.72\n0.25\n43.0\n13.0\n0.66\n2.18\n0.0\n0.0\n1.0\n3746.0\n0.00\n2000.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n38.0\n9.0\n0.45\n1.90\n0.0\n0.0\n1.0\n3746.0\n0.00\n2024.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n31.0\n16.0\n0.79\n1.53\n0.0\n0.0\n1.0\n3746.0\n0.00\n2048.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n27.0\n8.0\n0.39\n1.32\n0.0\n0.0\n1.0\n3746.0\n0.00\n2072.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n28.0\n11.0\n0.53\n1.35\n0.0\n0.0\n1.0\n3746.0\n0.00\n2096.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n18.0\n7.0\n0.33\n0.86\n0.0\n0.0\n1.0\n3746.0\n0.00\n2120.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n22.0\n7.0\n0.33\n1.04\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\n\n\n\n\n\n\n\n\n\n1.1.3 Transform from wide to tidy format\nFor this assignment, we are interested in the number of evictions by census tract for various years. Right now, each year has it’s own column, so it will be easiest to transform to a tidy format.\nUse the pd.melt() function to transform the eviction data into tidy format, using the number of evictions from 2003 to 2016.\nThe tidy data frame should have four columns: GEOID, geometry, a column holding the number of evictions, and a column telling you what the name of the original column was for that value.\nHints: - You’ll want to specify the GEOID and geometry columns as the id_vars. This will keep track of the census tract information. - You should specify the names of the columns holding the number of evictions as the value_vars. - You can generate a list of this column names using Python’s f-string formatting: python     value_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\n\nvalue_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\ntidytracts = tracts_filtered.melt(id_vars=[\"GEOID\", \"geometry\"], value_vars=value_vars)\ntidytracts.rename(columns={\"variable\": \"year\", \"value\": \"evictions\"}, inplace=True)\ntidytracts\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n...\n...\n...\n...\n...\n\n\n5371\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n5372\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n5373\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n5374\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n5375\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n5376 rows × 4 columns\n\n\n\n\ntidytracts.head()\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n\n\n\n\n\n\n\n1.1.4 Plot the total number of evictions per year from 2003 to 2016\nUse hvplot to plot the total number of evictions from 2003 to 2016. You will first need to perform a group by operation and sum up the total number of evictions for all census tracts, and then use hvplot() to make your plot.\nYou can use any type of hvplot chart you’d like to show the trend in number of evictions over time.\n\nEvicYears = tidytracts.groupby('year')['evictions'].sum()\n\n\nEvicYears = pd.DataFrame(EvicYears).reset_index()\nEvicYears\n\n\n\n\n\n\n\n\nyear\nevictions\n\n\n\n\n0\ne-03\n10647.0\n\n\n1\ne-04\n10491.0\n\n\n2\ne-05\n10550.0\n\n\n3\ne-06\n11078.0\n\n\n4\ne-07\n11032.0\n\n\n5\ne-08\n10866.0\n\n\n6\ne-09\n9821.0\n\n\n7\ne-10\n10628.0\n\n\n8\ne-11\n10882.0\n\n\n9\ne-12\n11130.0\n\n\n10\ne-13\n10803.0\n\n\n11\ne-14\n11182.0\n\n\n12\ne-15\n10098.0\n\n\n13\ne-16\n10264.0\n\n\n\n\n\n\n\n\nEvicYears.hvplot.line(\n           x='year', \n           y='evictions', \n           title='Total Number of Evictions in Philadelphia, 2003-2016'\n          ) \n\n\n\n\n\n  \n\n\n\n\n\n\n1.1.5 The number of evictions across Philadelphia\nOur tidy data frame is still a GeoDataFrame with a geometry column, so we can visualize the number of evictions for all census tracts.\nUse hvplot() to generate a choropleth showing the number of evictions for a specified year, with a widget dropdown to select a given year (or variable name, e.g., e-16, e-15, etc).\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select between them. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\ntidytracts\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n...\n...\n...\n...\n...\n\n\n5371\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n5372\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n5373\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n5374\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n5375\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n5376 rows × 4 columns\n\n\n\n\ntidytracts.hvplot(\n    values=\"evictions\",\n    hover_cols=[\"evictions\", \"year\"],\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    hover_fill_color=\"white\",\n    title=\"Evictions in Philadelphia by Year\",\n    colormap='winter'\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []"
  },
  {
    "objectID": "analysis/evictions.html#code-violations-in-philadelphia",
    "href": "analysis/evictions.html#code-violations-in-philadelphia",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.2 Code Violations in Philadelphia",
    "text": "1.2 Code Violations in Philadelphia\nNext, we’ll explore data for code violations from the Licenses and Inspections Department of Philadelphia to look for potential correlations with the number of evictions.\n\n1.2.1 Load data from 2012 to 2016\nL+I violation data for years including 2012 through 2016 (inclusive) is provided in a CSV format in the “data/” folder.\nLoad the data using pandas and convert to a GeoDataFrame.\n\nLIViolations = gpd.read_file(\"./Data/li_violations.csv\")\n\n\nLIViolations\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nNone\n\n\n1\n40.050593\n-75.126578\nLICENSE-CHANGE OF ADDRESS\nNone\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nNone\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nNone\n\n\n4\n40.02326\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nNone\n\n\n...\n...\n...\n...\n...\n\n\n434047\n40.012805\n-75.155963\nSD-REQD EXIST GROUP R\nNone\n\n\n434048\n40.009985\n-75.068968\nRUBBISH/GARBAGE EXTERIOR-OWNER\nNone\n\n\n434049\n40.009829\n-75.068912\nCLIP VIOLATION NOTICE\nNone\n\n\n434050\n40.009776\n-75.068895\nPERSONAL PROPERTY EXT OWNER\nNone\n\n\n434051\n40.009776\n-75.068895\nLICENSE - RENTAL PROPERTY\nNone\n\n\n\n\n434052 rows × 4 columns\n\n\n\n\nLIVioGDF = gpd.GeoDataFrame(LIViolations, geometry=gpd.points_from_xy(LIViolations.lng, LIViolations.lat), crs=\"EPSG:4326\")\n\n\nLIVioGDF\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nPOINT (-75.12608 40.05053)\n\n\n1\n40.050593\n-75.126578\nLICENSE-CHANGE OF ADDRESS\nPOINT (-75.12658 40.05059)\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nPOINT (-75.12889 39.99199)\n\n\n4\n40.02326\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-75.16485 40.02326)\n\n\n...\n...\n...\n...\n...\n\n\n434047\n40.012805\n-75.155963\nSD-REQD EXIST GROUP R\nPOINT (-75.15596 40.01281)\n\n\n434048\n40.009985\n-75.068968\nRUBBISH/GARBAGE EXTERIOR-OWNER\nPOINT (-75.06897 40.00999)\n\n\n434049\n40.009829\n-75.068912\nCLIP VIOLATION NOTICE\nPOINT (-75.06891 40.00983)\n\n\n434050\n40.009776\n-75.068895\nPERSONAL PROPERTY EXT OWNER\nPOINT (-75.06889 40.00978)\n\n\n434051\n40.009776\n-75.068895\nLICENSE - RENTAL PROPERTY\nPOINT (-75.06889 40.00978)\n\n\n\n\n434052 rows × 4 columns\n\n\n\n\n\n1.2.2 Trim to specific violation types\nThere are many different types of code violations (running the nunique() function on the violationdescription column will extract all of the unique ones). More information on different types of violations can be found on the City’s website.\nBelow, I’ve selected 15 types of violations that deal with property maintenance and licensing issues. We’ll focus on these violations. The goal is to see if these kinds of violations are correlated spatially with the number of evictions in a given area.\nUse the list of violations given to trim your data set to only include these types.\n\nviolation_types = [\n    \"INT-PLMBG MAINT FIXTURES-RES\",\n    \"INT S-CEILING REPAIR/MAINT SAN\",\n    \"PLUMBING SYSTEMS-GENERAL\",\n    \"CO DETECTOR NEEDED\",\n    \"INTERIOR SURFACES\",\n    \"EXT S-ROOF REPAIR\",\n    \"ELEC-RECEPTABLE DEFECTIVE-RES\",\n    \"INT S-FLOOR REPAIR\",\n    \"DRAINAGE-MAIN DRAIN REPAIR-RES\",\n    \"DRAINAGE-DOWNSPOUT REPR/REPLC\",\n    \"LIGHT FIXTURE DEFECTIVE-RES\",\n    \"LICENSE-RES SFD/2FD\",\n    \"ELECTRICAL -HAZARD\",\n    \"VACANT PROPERTIES-GENERAL\",\n    \"INT-PLMBG FIXTURES-RES\",\n]\n\n\nviolations = LIVioGDF[LIVioGDF[\"violationdescription\"].isin(violation_types)]\n\n\n\n1.2.3 Make a hex bin map\nThe code violation data is point data. We can get a quick look at the geographic distribution using matplotlib and the hexbin() function. Make a hex bin map of the code violations and overlay the census tract outlines.\nHints: - The eviction data from part 1 was by census tract, so the census tract geometries are available as part of that GeoDataFrame. You can use it to overlay the census tracts on your hex bin map. - Make sure you convert your GeoDataFrame to a CRS that’s better for visualization than plain old 4326.\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.hexbin(\n    violations[\"geometry\"].x,\n    violations[\"geometry\"].y,\n    gridsize=100,\n    cmap=\"plasma\",\n    alpha=1,\n    edgecolors=\"none\",\n    mincnt=1,\n)\n\nax.set_xlabel(\"Latitude\")\nax.set_ylabel(\"Longitude\")\nax.set_title(\"Hex Bin Map of Code Violations in Philadelphia\")\n\n\nplt.show()\n\n\n\n\n\n\n1.2.4 Spatially join data sets\nTo do a census tract comparison to our eviction data, we need to find which census tract each of the code violations falls into. Use the geopandas.sjoin() function to do just that.\nHints - You can re-use your eviction data frame, but you will only need the geometry column (specifying census tract polygons) and the GEOID column (specifying the name of each census tract). - Make sure both data frames have the same CRS before joining them together!\n\ntidytractsNoDup = tidytracts.drop_duplicates(\"GEOID\")\ntidytractsNoDup\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n...\n...\n...\n...\n...\n\n\n379\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-03\n105.0\n\n\n380\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-03\n45.0\n\n\n381\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-03\n21.0\n\n\n382\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-03\n6.0\n\n\n383\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\n\n\n\n\n384 rows × 4 columns\n\n\n\n\nviolations.head()\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n\n\n25\n40.022406\n-75.121872\nEXT S-ROOF REPAIR\nPOINT (-75.12187 40.02241)\n\n\n30\n40.023237\n-75.121726\nCO DETECTOR NEEDED\nPOINT (-75.12173 40.02324)\n\n\n31\n40.023397\n-75.122241\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.12224 40.02340)\n\n\n34\n40.023773\n-75.121603\nINT S-FLOOR REPAIR\nPOINT (-75.12160 40.02377)\n\n\n\n\n\n\n\n\ntidytractsNoDup = tidytractsNoDup.to_crs(\"EPSG:4326\")\nviolations = violations.to_crs(\"EPSG:4326\") \njoined_data = gpd.sjoin(violations, tidytractsNoDup)\njoined_data\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\nindex_right\nGEOID\nyear\nevictions\n\n\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n364\n42101027100\ne-03\n6.0\n\n\n16024\n40.045785\n-75.127928\nCO DETECTOR NEEDED\nPOINT (-75.12793 40.04579)\n364\n42101027100\ne-03\n6.0\n\n\n17225\n40.045634\n-75.126469\nLICENSE-RES SFD/2FD\nPOINT (-75.12647 40.04563)\n364\n42101027100\ne-03\n6.0\n\n\n43436\n40.045738\n-75.125851\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.12585 40.04574)\n364\n42101027100\ne-03\n6.0\n\n\n43452\n40.045779\n-75.125843\nEXT S-ROOF REPAIR\nPOINT (-75.12584 40.04578)\n364\n42101027100\ne-03\n6.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n219512\n40.101835\n-75.045433\nDRAINAGE-DOWNSPOUT REPR/REPLC\nPOINT (-75.04543 40.10184)\n136\n42101035602\ne-03\n7.0\n\n\n224788\n40.004739\n-75.206722\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.20672 40.00474)\n283\n42101012201\ne-03\n28.0\n\n\n224790\n40.004739\n-75.206722\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.20672 40.00474)\n283\n42101012201\ne-03\n28.0\n\n\n240195\n40.013165\n-75.142804\nLICENSE-RES SFD/2FD\nPOINT (-75.14280 40.01317)\n171\n42101980500\ne-03\n0.0\n\n\n310973\n40.034424\n-75.017732\nDRAINAGE-DOWNSPOUT REPR/REPLC\nPOINT (-75.01773 40.03442)\n175\n42101989100\ne-03\n0.0\n\n\n\n\n34108 rows × 8 columns\n\n\n\n\n\n1.2.5 Calculate the number of violations by type per census tract\nNext, we’ll want to find the number of violations (for each kind) per census tract. You should group the data frame by violation type and census tract name.\nThe result of this step should be a data frame with three columns: violationdescription, GEOID, and N, where N is the number of violations of that kind in the specified census tract.\nOptional: to make prettier plots\nSome census tracts won’t have any violations, and they won’t be included when we do the above calculation. However, there is a trick to set the values for those census tracts to be zero. After you calculate the sizes of each violation/census tract group, you can run:\nN = N.unstack(fill_value=0).stack().reset_index(name='N')\nwhere N gives the total size of each of the groups, specified by violation type and census tract name.\nSee this StackOverflow post for more details.\nThis part is optional, but will make the resulting maps a bit prettier.\n\n#N = N.unstack(fill_value=0).stack().reset_index(name='N')\nvioldesc = joined_data.groupby([\"violationdescription\", \"GEOID\"]).size().reset_index(name=\"N\")\nvioldesc\n\n\n\n\n\n\n\n\nviolationdescription\nGEOID\nN\n\n\n\n\n0\nCO DETECTOR NEEDED\n42101000401\n1\n\n\n1\nCO DETECTOR NEEDED\n42101000402\n1\n\n\n2\nCO DETECTOR NEEDED\n42101000700\n1\n\n\n3\nCO DETECTOR NEEDED\n42101000803\n1\n\n\n4\nCO DETECTOR NEEDED\n42101000804\n1\n\n\n...\n...\n...\n...\n\n\n3995\nVACANT PROPERTIES-GENERAL\n42101036501\n1\n\n\n3996\nVACANT PROPERTIES-GENERAL\n42101037900\n1\n\n\n3997\nVACANT PROPERTIES-GENERAL\n42101038000\n1\n\n\n3998\nVACANT PROPERTIES-GENERAL\n42101038200\n2\n\n\n3999\nVACANT PROPERTIES-GENERAL\n42101038300\n3\n\n\n\n\n4000 rows × 3 columns\n\n\n\n\n\n1.2.6 Merge with census tracts geometries\nWe now have the number of violations of different types per census tract specified as a regular DataFrame. You can now merge it with the census tract geometries (from your eviction data GeoDataFrame) to create a GeoDataFrame.\nHints - Use pandas.merge() and specify the on keyword to be the column holding census tract names. - Make sure the result of the merge operation is a GeoDataFrame — you will want the GeoDataFrame holding census tract geometries to be the first argument of the pandas.merge() function.\n\nmergedN_evics = tidytractsNoDup.merge(violdesc,  on=\"GEOID\")\nmergedN_evics\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\nviolationdescription\nN\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n6\n\n\n1\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nELECTRICAL -HAZARD\n1\n\n\n2\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nEXT S-ROOF REPAIR\n3\n\n\n3\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nINT S-CEILING REPAIR/MAINT SAN\n9\n\n\n4\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nINT S-FLOOR REPAIR\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3995\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nINT S-CEILING REPAIR/MAINT SAN\n4\n\n\n3996\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nINT S-FLOOR REPAIR\n2\n\n\n3997\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nINTERIOR SURFACES\n2\n\n\n3998\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nLICENSE-RES SFD/2FD\n11\n\n\n3999\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nPLUMBING SYSTEMS-GENERAL\n2\n\n\n\n\n4000 rows × 6 columns\n\n\n\n\n\n1.2.7 Interactive choropleths for each violation type\nNow, we can use hvplot() to create an interactive choropleth for each violation type and add a widget to specify different violation types.\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select different violation types. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\nmergedN_evics.hvplot(\n    geo = True,\n    groupby = \"violationdescription\",\n    c = \"N\",\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    title=\"Evictions in Philadelphia by Year\",\n    colormap=\"bmw\"\n)"
  },
  {
    "objectID": "analysis/evictions.html#a-side-by-side-comparison",
    "href": "analysis/evictions.html#a-side-by-side-comparison",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.3. A side-by-side comparison",
    "text": "1.3. A side-by-side comparison\nFrom the interactive maps of evictions and violations, you should notice a lot of spatial overlap.\nAs a final step, we’ll make a side-by-side comparison to better show the spatial correlations. This will involve a few steps:\n\nTrim the evictions data frame plotted in section 1.1.5 to only include evictions from 2016.\nTrim the L+I violations data frame plotted in section 1.2.7 to only include a single violation type (pick whichever one you want!).\nUse hvplot() to make two interactive choropleth maps, one for the data from step 1. and one for the data in step 2.\nShow these two plots side by side (one row and 2 columns) using the syntax for combining charts.\n\nNote: since we selected a single year and violation type, you won’t need to use the groupby= keyword here.\n\nevics2016 = tidytracts[tidytracts[\"year\"] == \"e-16\"]\nevics2016\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n4992\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-16\n16.0\n\n\n4993\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-16\n8.0\n\n\n4994\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-16\n14.0\n\n\n4995\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-16\n4.0\n\n\n4996\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-16\n7.0\n\n\n...\n...\n...\n...\n...\n\n\n5371\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n5372\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n5373\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n5374\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n5375\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n384 rows × 4 columns\n\n\n\n\nevics2016drain = mergedN_evics[mergedN_evics[\"violationdescription\"] == \"DRAINAGE-DOWNSPOUT REPR/REPLC\"]\nevics2016drain\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\nviolationdescription\nN\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n6\n\n\n8\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n1\n\n\n14\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n2\n\n\n29\n42101001002\nMULTIPOLYGON (((-75.14919 39.94903, -75.14602 ...\ne-03\n16.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n3\n\n\n38\n42101001400\nMULTIPOLYGON (((-75.16558 39.94366, -75.16567 ...\ne-03\n30.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3946\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-03\n105.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n14\n\n\n3961\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-03\n45.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n21\n\n\n3975\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-03\n21.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n4\n\n\n3985\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-03\n6.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n3\n\n\n3993\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n5\n\n\n\n\n311 rows × 6 columns\n\n\n\n\np1 = evics2016drain.hvplot(\n    values=\"N\",\n    hover_cols=[\"N\"],\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    hover_color=\"yellow\",\n    title=\"Reported Drainage Damage in Households 2016\",\n    colormap='Greens'\n)\np1\n\n\n\n\np2 = evics2016.hvplot(\n    values=\"evictions\",\n    hover_cols=\"evictions\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    title=\"Evictions in Philadelphia 2016\",\n)\n\np2\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []"
  },
  {
    "objectID": "analysis/evictions.html#extra-credit",
    "href": "analysis/evictions.html#extra-credit",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.4. Extra Credit",
    "text": "1.4. Extra Credit\nIdentify the 20 most common types of violations within the time period of 2012 to 2016 and create a set of interactive choropleths similar to what was done in section 1.2.7.\nUse this set of maps to identify 3 types of violations that don’t seem to have much spatial overlap with the number of evictions in the City.\n\n#tracts12to16 = tidytracts[tidytracts[\"year\"].astype(str).isin[f\"e-{x:02d}\" for x in range(12, 17)]]\n\n#tidytracts12to16 = tidytracts[tidytracts[\"year\"] == \"e-16\" or tidytracts[\"year\"] == \"e-15\" or tidytracts[\"year\"] == \"e-14\" or tidytracts[\"year\"] == \"e-13\" or tidytracts[\"year\"] == \"e-13\" or tidytracts[\"year\"] == \"e-12\"]\n\n\nvalue_vars = [f\"e-{x:02d}\" for x in range(12, 17)]\ntidytracts12to16 = tracts_filtered.melt(id_vars=[\"GEOID\", \"geometry\"], value_vars=value_vars)\ntidytracts12to16.rename(columns={\"variable\": \"year\", \"value\": \"evictions\"}, inplace=True)\ntidytracts12to16\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-12\n7.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-12\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-12\n12.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-12\n0.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-12\n16.0\n\n\n...\n...\n...\n...\n...\n\n\n1915\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n1916\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n1917\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n1918\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n1919\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n1920 rows × 4 columns\n\n\n\n\ngrouped_evics = mergedN_evics.groupby(\"violationdescription\").size().reset_index(name=\"N\")\ntidyevics = grouped_evics.sort_values(by=[\"N\"], ascending=False)\ntop20_violations = tidyevics.iloc[:20, 0]              \n\n\ntop20_violations\n\n6     INT S-CEILING REPAIR/MAINT SAN\n11               LICENSE-RES SFD/2FD\n5                  EXT S-ROOF REPAIR\n0                 CO DETECTOR NEEDED\n1      DRAINAGE-DOWNSPOUT REPR/REPLC\n7                 INT S-FLOOR REPAIR\n2     DRAINAGE-MAIN DRAIN REPAIR-RES\n8             INT-PLMBG FIXTURES-RES\n9       INT-PLMBG MAINT FIXTURES-RES\n12       LIGHT FIXTURE DEFECTIVE-RES\n3      ELEC-RECEPTABLE DEFECTIVE-RES\n10                 INTERIOR SURFACES\n13          PLUMBING SYSTEMS-GENERAL\n4                 ELECTRICAL -HAZARD\n14         VACANT PROPERTIES-GENERAL\nName: violationdescription, dtype: object\n\n\n\nCelingVio2016 = mergedN_evics[mergedN_evics[\"violationdescription\"] == \"INT S-CEILING REPAIR/MAINT SAN\"]\n\n\nCOVio2016= mergedN_evics[mergedN_evics[\"violationdescription\"] == \"CO DETECTOR NEEDED\"]\n\n\nElectricVio2016 = mergedN_evics[mergedN_evics[\"violationdescription\"] == \"ELECTRICAL -HAZARD\"]\n\n\nElectricVio2016.hvplot(\n    values=\"N\",\n    hover_cols=\"N\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    colormap=\"bmw\",\n    title=\"Electrical Hazards 2016\",\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []\n\n\n\n\n\n\n  \n\n\n\n\n\nCOVio2016.hvplot(\n    values=\"N\",\n    hover_cols=\"N\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    colormap=\"coolwarm\",\n    title=\"Carbon Monoxide Detectors Needed in 2016\",\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []\n\n\n\n\n\n\n  \n\n\n\n\n\nCelingVio2016.hvplot(\n    values=\"N\",\n    hover_cols=\"N\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    colormap=\"fire\",\n    title=\"Ceiling Repair Violations 2016\",\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []"
  },
  {
    "objectID": "analysis/evictions.html#part-2-exploring-the-ndvi-in-philadelphia",
    "href": "analysis/evictions.html#part-2-exploring-the-ndvi-in-philadelphia",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "Part 2: Exploring the NDVI in Philadelphia",
    "text": "Part 2: Exploring the NDVI in Philadelphia\nIn this part, we’ll explore the NDVI in Philadelphia a bit more. This part will include two parts:\n\nWe’ll compare the median NDVI within the city limits and the immediate suburbs\nWe’ll calculate the NDVI around street trees in the city."
  },
  {
    "objectID": "analysis/evictions.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "href": "analysis/evictions.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "2.1 Comparing the NDVI in the city and the suburbs",
    "text": "2.1 Comparing the NDVI in the city and the suburbs\n\n2.1.1 Load Landsat data for Philadelphia\nUse rasterio to load the landsat data for Philadelphia (available in the “data/” folder)\n\nlandsat = rio.open(\"./Data/landsat8_philly.tif\")\nlandsat\n\n&lt;open DatasetReader name='./Data/landsat8_philly.tif' mode='r'&gt;\n\n\n\n\n2.1.2 Separating the city from the suburbs\nCreate two polygon objects, one for the city limits and one for the suburbs. To calculate the suburbs polygon, we will take everything outside the city limits but still within the bounding box.\n\nThe city limits are available in the “data/” folder.\nTo calculate the suburbs polygon, the “envelope” attribute of the city limits geometry will be useful.\nYou can use geopandas’ geometric manipulation functionality to calculate the suburbs polygon from the city limits polygon and the envelope polygon.\n\n\nenvelope = CityLimitsdf.geometry.envelope\nsuburbs = envelope - CityLimitsdf.geometry\nsuburbs\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_7664\\3105865041.py:2: FutureWarning: '-' operator will be deprecated. Use the 'difference' method instead.\n  suburbs = envelope - CityLimitsdf.geometry\n\n\n0    MULTIPOLYGON (((-75.28031 39.86747, -75.28031 ...\ndtype: geometry\n\n\n\nprint(landsat.crs.to_epsg())\n\n32618\n\n\n\nCityLimitsdf = CityLimitsdf.to_crs(32618)\nsuburbs = suburbs.to_crs(32618)\n\n\nsuburbs.squeeze()\n\n\n\n\n\n\n2.1.3 Mask and calculate the NDVI for the city and the suburbs\nUsing the two polygons from the last section, use rasterio’s mask functionality to create two masked arrays from the landsat data, one for the city and one for the suburbs.\nFor each masked array, calculate the NDVI.\n\nfrom rasterio.mask import mask\n\n\nmaskedCity, mask_transformCity = mask(\n    dataset=landsat,            \n    shapes=CityLimitsdf.geometry,  \n    crop=True,                    \n    all_touched=True,             \n    filled=False,                 \n)\nmaskedSub, mask_transform = mask(\n    dataset=landsat,            \n    shapes=suburbs.geometry,  \n    crop=True,                    \n    all_touched=True,             \n    filled=False,                 \n)\n    \n\n\nredC = maskedCity[3]\nnirC = maskedCity[4]\nredS = maskedSub[3]\nnirS = maskedSub[4]\n\n\ndef calculate_NDVI(nir, red):\n    nir = nir.astype(float)\n    red = red.astype(float)\n\n    check = np.logical_and(red.mask == False, nir.mask == False)\n\n    ndvi = np.where(check, (nir - red) / (nir + red), np.nan)\n\n    return ndvi\n\nNDVI_C = calculate_NDVI(nirC, redC)\nNDVI_S = calculate_NDVI(nirS, redS)\n\n\n\n2.1.4 Calculate the median NDVI within the city and within the suburbs\n\nCalculate the median value from your NDVI arrays for the city and suburbs\nNumpy’s nanmedian function will be useful for ignoring NaN elements\nPrint out the median values. Which has a higher NDVI: the city or suburbs?\n\n\ncity_ndvi_median = np.nanmedian(NDVI_C)\nsuburbs_ndvi_median = np.nanmedian(NDVI_S)\n\nprint(f\"Median NDVI for the city: {city_ndvi_median}\")\nprint(f\"Median NDVI for the suburbs: {suburbs_ndvi_median}\")\n\nMedian NDVI for the city: 0.20268593532493442\nMedian NDVI for the suburbs: 0.37466958688920776"
  },
  {
    "objectID": "analysis/evictions.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "href": "analysis/evictions.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "2.2 Calculating the NDVI for Philadelphia’s street treets",
    "text": "2.2 Calculating the NDVI for Philadelphia’s street treets\n\n2.2.1 Load the street tree data\nThe data is available in the “data/” folder. It has been downloaded from OpenDataPhilly. It contains the locations of abot 2,500 street trees in Philadelphia.\n\npprTreePoints\n\n\n\n\n\n\n\n\nobjectid\nfcode\ngeometry\n\n\n\n\n0\n1\n3000\nPOINT (-75.00538 40.06254)\n\n\n1\n2\n3000\nPOINT (-75.12959 39.96692)\n\n\n2\n3\n3000\nPOINT (-75.12838 39.98397)\n\n\n3\n4\n3000\nPOINT (-75.12892 39.98489)\n\n\n4\n5\n3000\nPOINT (-75.12948 39.97148)\n\n\n...\n...\n...\n...\n\n\n2475\n2476\n3000\nPOINT (-75.15779 39.97728)\n\n\n2476\n2477\n3000\nPOINT (-75.15773 39.97727)\n\n\n2477\n2478\n3000\nPOINT (-75.15768 39.97727)\n\n\n2478\n2479\n3000\nPOINT (-75.15763 39.97725)\n\n\n2479\n2480\n3000\nPOINT (-75.15757 39.97725)\n\n\n\n\n2480 rows × 3 columns\n\n\n\n\npprTreePoints = pprTreePoints.to_crs(32618)\n\n\n\n2.2.2 Calculate the NDVI values at the locations of the street trees\n\nUse the rasterstats package to calculate the NDVI values at the locations of the street trees.\nSince these are point geometries, you can calculate either the median or the mean statistic (only one pixel will contain each point).\n\n\nimport rasterstats \nfrom rasterstats import zonal_stats\n\n\nstats = zonal_stats(\n    pprTreePoints,  # The vector data\n    NDVI_C,  # The array holding the raster data\n    affine=landsat.transform,  # The affine transform for the raster data\n    stats=[\"mean\", \"median\"],  # The stats to compute\n    nodata=np.nan,  # Missing data representation\n)\n#stats\n\n\n\n2.2.3 Plotting the results\nMake two plots of the results:\n\nA histogram of the NDVI values, using matplotlib’s hist function. Include a vertical line that marks the NDVI = 0 threshold\nA plot of the street tree points, colored by the NDVI value, using geopandas’ plot function. Include the city limits boundary on your plot.\n\nThe figures should be clear and well-styled, with for example, labels for axes, legends, and clear color choices.\n\nmedian_stats = [stats_dict[\"median\"] for stats_dict in stats]\nmean_stats = [stats_dict[\"mean\"] for stats_dict in stats]\n\n\na2 = plt.subplots()\nplt.hist(mean_stats, bins=100, density=True)\nplt.axvline(0, linestyle=\"dashed\", color=\"gray\")\nplt.xlabel(\"mean\")\nplt.ylabel(\"median\")\nplt.title(\"Histogram of NDVI Values for Street Trees in Philadelphia\")\nplt.grid(True)\nplt.show()\n\n\na2\n\n\n\n\n(&lt;Figure size 640x480 with 1 Axes&gt;,\n &lt;Axes: title={'center': 'Histogram of NDVI Values for Street Trees in Philadelphia'}, xlabel='mean', ylabel='median'&gt;)\n\n\n\npprTreePoints[\"median_stats\"] = [stats_dict[\"median\"] for stats_dict in stats]\npprTreePoints.head()\n\n\n\n\n\n\n\n\nobjectid\nfcode\ngeometry\nmedian_stats\n\n\n\n\n0\n1\n3000\nPOINT (499541.269 4434698.265)\n0.235337\n\n\n1\n2\n3000\nPOINT (488932.471 4424093.158)\n0.261535\n\n\n2\n3\n3000\nPOINT (489039.214 4425985.827)\n0.096769\n\n\n3\n4\n3000\nPOINT (488993.171 4426088.005)\n0.076630\n\n\n4\n5\n3000\nPOINT (488943.113 4424599.478)\n0.267952\n\n\n\n\n\n\n\n\npprTreePoints.explore(column=\"median_stats\", cmap=\"magma\", tiles=\"Cartodb positron\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/assignment-5.html",
    "href": "analysis/assignment-5.html",
    "title": "Exploring Yelp Reviews in Philadelphia",
    "section": "",
    "text": "In this assignment, we’ll explore restaurant review data available through the Yelp Dataset Challenge. The dataset includes Yelp data for user reviews and business information for many metropolitan areas. I’ve already downloaded this dataset (8 GB total!) and extracted out the data files for reviews and restaurants in Philadelphia. I’ve placed these data files into the data directory in this repository.\nThis assignment is broken into two parts:\nPart 1: Analyzing correlations between restaurant reviews and census data\nWe’ll explore the relationship between restaurant reviews and the income levels of the restaurant’s surrounding area.\nPart 2: Exploring the impact of fast food restaurants\nWe’ll run a sentiment analysis on reviews of fast food restaurants and estimate income levels in neighborhoods with fast food restaurants. We’ll test how well our sentiment analysis works by comparing the number of stars to the sentiment of reviews.\nBackground readings - Does sentiment analysis work? - The Geography of Taste: Using Yelp to Study Urban Culture"
  },
  {
    "objectID": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "href": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "title": "Exploring Yelp Reviews in Philadelphia",
    "section": "1. Correlating restaurant ratings and income levels",
    "text": "1. Correlating restaurant ratings and income levels\nIn this part, we’ll use the census API to download household income data and explore how it correlates with restaurant review data."
  },
  {
    "objectID": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "href": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "title": "Exploring Yelp Reviews in Philadelphia",
    "section": "2. Fast food trends in Philadelphia",
    "text": "2. Fast food trends in Philadelphia\nAt the end of part 1, you should have seen a strong trend where higher income tracts generally had restaurants with better reviews. In this section, we’ll explore the impact of fast food restaurants and how they might be impacting this trend.\nHypothesis\n\nFast food restaurants are predominantly located in areas with lower median income levels.\nFast food restaurants have worse reviews compared to typical restaurants.\n\nIf true, these two hypotheses could help to explain the trend we found in part 1. Let’s dive in and test our hypotheses!\n\n2.1 Identify fast food restaurants\nThe “categories” column in our dataset contains multiple classifications for each restaurant. One such category is “Fast Food”. In this step, add a new column called “is_fast_food” that is True if the “categories” column contains the term “Fast Food” and False otherwise\n\nrestauranttract['is_fast_food']  = restauranttract['categories'].str.contains('Fast Food', case=False, na=False).astype(int)\nrestauranttract[restauranttract[\"is_fast_food\"] == 1][\"name\"].head(5)\n\n11                Wendy's\n22    Crown Fried Chicken\n33            Chick-fil-A\n34               Checkers\n37                    KFC\nName: name, dtype: object\n\n\n\nrestauranttract.head(2)\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\nindex_right\nSTATEFP\n...\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\nMedHHInc\nstate\ncounty\ntract\nis_fast_food\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15557 39.95550)\n113.0\n42\n...\nS\n386232.0\n0.0\n+39.9554162\n-075.1569255\n91067.0\n42\n101\n000200\n0\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n311.0\n42\n...\nS\n444406.0\n0.0\n+39.9547160\n-075.1465255\n91944.0\n42\n101\n000102\n0\n\n\n\n\n2 rows × 26 columns\n\n\n\n\n\n2.2 Calculate the median income for fast food and otherwise\nGroup by the “is_fast_food” column and calculate the median income for restaurants that are and are not fast food. You should find that income levels are lower in tracts with fast food.\nNote: this is just an estimate, since we are calculating a median of median income values.\n\nFFtable = median_income_by_fast_food = restauranttract.groupby(\"is_fast_food\")[\"MedHHInc\"].median().reset_index()\nFFtable\n\n\n\n\n\n\n\n\nis_fast_food\nMedHHInc\n\n\n\n\n0\n0\n75583.0\n\n\n1\n1\n53480.0\n\n\n\n\n\n\n\n\nrestauranttract.rename(columns={\"B19013_001E\": \"medianHHincomelevel\"})\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\nindex_right\nSTATEFP\n...\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\nMedHHInc\nstate\ncounty\ntract\nis_fast_food\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15557 39.95550)\n113.0\n42\n...\nS\n386232.0\n0.0\n+39.9554162\n-075.1569255\n91067.0\n42\n101\n000200\n0\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n311.0\n42\n...\nS\n444406.0\n0.0\n+39.9547160\n-075.1465255\n91944.0\n42\n101\n000102\n0\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\nPOINT (-75.16257 39.94322)\n67.0\n42\n...\nS\n239383.0\n0.0\n+39.9419037\n-075.1591158\n93203.0\n42\n101\n001500\n0\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Restaurants\nPOINT (-75.15745 39.93982)\n29.0\n42\n...\nS\n242440.0\n0.0\n+39.9400001\n-075.1593102\n99125.0\n42\n101\n001800\n0\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\nPOINT (-75.21832 40.02246)\n288.0\n42\n...\nS\n858218.0\n55718.0\n+40.0244730\n-075.2151045\n79264.0\n42\n101\n021000\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7345\nVZbkSeZtFynEascotq7ExA\n39.953391\n-75.196765\nAli Baba Magic Food\n8\n4.0\nRestaurants, Food Stands\nPOINT (-75.19677 39.95339)\n255.0\n42\n...\nS\n184674.0\n0.0\n+39.9539319\n-075.1984477\n36250.0\n42\n101\n008801\n0\n\n\n7346\ngPr1io7ks0Eo3FDsnDTYfg\n40.060414\n-75.191084\nTata Cafe\n21\n4.0\nSandwiches, Restaurants, Italian\nPOINT (-75.19109 40.06041)\n243.0\n42\n...\nS\n865284.0\n1982.0\n+40.0683411\n-075.1883025\n92404.0\n42\n101\n025600\n0\n\n\n7347\nwVxXRFf10zTTAs11nr4xeA\n40.032483\n-75.214430\nPrimoHoagies\n55\n3.0\nRestaurants, Specialty Food, Food, Sandwiches, Italian\nPOINT (-75.21443 40.03248)\n204.0\n42\n...\nS\n538872.0\n0.0\n+40.0322339\n-075.2181174\n79464.0\n42\n101\n021300\n0\n\n\n7348\n8n93L-ilMAsvwUatarykSg\n39.951018\n-75.198240\nKitchen Gia\n22\n3.0\nCoffee & Tea, Food, Sandwiches, American (Traditional), Restaurants\nPOINT (-75.19824 39.95102)\n315.0\n42\n...\nS\n883507.0\n49251.0\n+39.9523358\n-075.1889603\n27821.0\n42\n101\n036902\n0\n\n\n7349\nWnT9NIzQgLlILjPT0kEcsQ\n39.935982\n-75.158665\nAdelita Taqueria & Restaurant\n35\n4.5\nRestaurants, Mexican\nPOINT (-75.15867 39.93598)\n33.0\n42\n...\nS\n535423.0\n0.0\n+39.9367634\n-075.1595100\n91382.0\n42\n101\n002400\n0\n\n\n\n\n7350 rows × 26 columns\n\n\n\n\n\n2.3 Load fast food review data\nIn the rest of part 2, we’re going to run a sentiment analysis on the reviews for fast food restaurants. The review data for all fast food restaurants identified in part 2.1 is already stored in the data/ folder. The data is stored as a JSON file and you can use pandas.read_json to load it.\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\nreviews = pd.read_json('Data/reviews_philly_fast_food.json', orient='records', lines=True)\nrestreviews = reviews.merge(resturantsGpd, on = \"business_id\", how = \"inner\")\nrestreviews = restreviews.rename(columns={\"stars_x\": \"stars_given\", \"stars_y\": \"overall_rating\"})\nrestreviews.head(1)\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars_given\ntext\nlatitude\nlongitude\nname\nreview_count\noverall_rating\ncategories\ngeometry\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n39.93944\n-75.166805\nMcDonald's\n55\n2.0\nFast Food, Food, Restaurants, Coffee & Tea, Burgers\nPOINT (-75.16680 39.93944)\n\n\n\n\n\n\n\n\n\n2.4 Trim to the most popular fast food restaurants\nThere’s too many reviews to run a sentiment analysis on all of them in a reasonable time. Let’s trim our reviews dataset to the most popular fast food restaurants, using the list provided below.\nYou will need to get the “business_id” values for each of these restaurants from the restaurants data loaded in part 1.3. Then, trim the reviews data to include reviews only for those business IDs.\n\npopular_fast_food = [\n    \"McDonald's\",\n    \"Wendy's\",\n    \"Subway\",\n    \"Popeyes Louisiana Kitchen\",\n    \"Taco Bell\",\n    \"KFC\",\n    \"Burger King\",\n]\n\n\nrestfocus = restreviews[restreviews['name'].isin(popular_fast_food)]\nrestfocus.head(1)\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars_given\ntext\nlatitude\nlongitude\nname\nreview_count\noverall_rating\ncategories\ngeometry\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n39.93944\n-75.166805\nMcDonald's\n55\n2.0\nFast Food, Food, Restaurants, Coffee & Tea, Burgers\nPOINT (-75.16680 39.93944)\n\n\n\n\n\n\n\n\n\n2.5 Run the emotions classifier on fast food reviews\nRun a sentiment analysis on the reviews data from the previous step. Use the DistilBERT model that can predict emotion labels (anger, fear, sadness, joy, love, and surprise). Transform the result from the classifier into a DataFrame so that you have a column for each of the emotion labels.\n\ndescriptions = restfocus[\"text\"].str.strip().tolist()\ndescriptions[:3]\n\n['I know I shouldn\\'t expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\".  Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.',\n 'Dirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news!  A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day!  We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.',\n \"That's a shame! \\nThis place is full of junkies customers \\nThe staff and the service is fast \\nIt's just too much like homeless or you can tell the addict people all coming here and soliciting\"]\n\n\n\nexample_string = \"This is an Example\"\ndescriptions_words = [desc.split() for desc in descriptions]\ndescriptions_words_flat = []\n\nfor list_of_words in descriptions_words:\n    for word in list_of_words:\n        descriptions_words_flat.append(word)\ndescriptions_words_lower = [word.lower() for word in descriptions_words_flat]\nimport nltk\n\nnltk.download(\"stopwords\");\nstop_words = list(set(nltk.corpus.stopwords.words(\"english\")))\ndescriptions_no_stop = []\n\nfor word in descriptions_words_lower:\n    if word not in stop_words:\n        descriptions_no_stop.append(word)\ndescriptions_no_stop = [\n    word for word in descriptions_words_lower if word not in stop_words\n]\nimport string\npunctuation = list(string.punctuation)\ndescriptions_final = []\n\n# Loop over all words\nfor word in descriptions_no_stop:\n    # Remove any punctuation from the words\n    for p in punctuation:\n        word = word.replace(p, \"\")\n\n    # Save it if the string is not empty\n    if word != \"\":\n        descriptions_final.append(word)\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nwords = pd.DataFrame({\"words\": descriptions_final})\n\n\nN = (\n    words.groupby(\"words\", as_index=False)\n    .size()\n    .sort_values(\"size\", ascending=False, ignore_index=True)\n)\n\n\ntop20 = N.head(20)\n\ntop20\n\n\n\n\n\n\n\n\nwords\nsize\n\n\n\n\n0\nfood\n1840\n\n\n1\norder\n1430\n\n\n2\nget\n1012\n\n\n3\none\n980\n\n\n4\nlike\n922\n\n\n5\nservice\n901\n\n\n6\nplace\n882\n\n\n7\ntime\n873\n\n\n8\nchicken\n848\n\n\n9\nmcdonalds\n810\n\n\n10\ngo\n748\n\n\n11\nlocation\n677\n\n\n12\neven\n637\n\n\n13\ndrive\n601\n\n\n14\ngot\n585\n\n\n15\nfries\n570\n\n\n16\nnever\n568\n\n\n17\nordered\n563\n\n\n18\nback\n554\n\n\n19\ngood\n550\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot horizontal bar graph\nsns.barplot(\n    y=\"words\",\n    x=\"size\",\n    data=top20,\n    ax=ax,\n    color=\"crimson\",\n    saturation=1.0,\n)\n\nax.set_title(\"Most Common Words Found in Fast Food reviews\")\n\n# Remove spines (the box)\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\n# Remove x and y ticks\nax.tick_params(bottom=False, left=False)\n\n\n\n\n\n\n2.6 Identify the predicted emotion for each text\nUse the pandas idxmax() to identify the predicted emotion for each review, and add this value to a new column called “prediction”\nThe predicted emotion has the highest confidence score across all emotion labels for a particular label.\n\nfrom transformers import pipeline\nmodel = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n\nemotion_classifier = pipeline(\n    task=\"text-classification\", \n    model=model,  \n    top_k=None, \n    tokenizer=model,  \n    truncation=True, \n)\n\n\n\n\nC:\\Users\\Owner\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\huggingface_hub\\file_download.py:138: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Owner\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\nTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n  warnings.warn(message)\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%time \n\nemotion_scores = emotion_classifier(descriptions)\nemotion_scores[0]\n\nCPU times: total: 1min 49s\nWall time: 5min 1s\n\n\n[{'label': 'sadness', 'score': 0.7338687777519226},\n {'label': 'fear', 'score': 0.25067660212516785},\n {'label': 'anger', 'score': 0.011039001867175102},\n {'label': 'joy', 'score': 0.002758048241958022},\n {'label': 'surprise', 'score': 0.0010148986475542188},\n {'label': 'love', 'score': 0.0006427847547456622}]\n\n\n\nemotion = pd.DataFrame(\n    [{d[\"label\"]: d[\"score\"] for d in dd} for dd in emotion_scores]\n).assign(text=descriptions)\nemotion.head(3)\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\n\n\n\n\n0\n0.733869\n0.250677\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n1\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n\n\n2\n0.268274\n0.039276\n0.658548\n0.030782\n0.001894\n0.001226\nThat's a shame! \\nThis place is full of junkies customers \\nThe staff and the service is fast \\nIt's just too much like homeless or you can tell the addict people all coming here and soliciting\n\n\n\n\n\n\n\n\nemotion_labels = [\"anger\", \"fear\", \"sadness\", \"joy\", \"surprise\", \"love\"]\nemotion['prediction'] = emotion[emotion_labels].idxmax(axis=1)\n\n\nemotion_count = emotion.groupby(\"prediction\").size().sort_values()\n\n# Plotting\nax = emotion_count.plot(kind='barh', color='darkolivegreen')\n\n# Add title\nax.set_title('Emotion Predictions for Yelp Fast Food Reviews')\n\n# Remove spines (the box)\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\n# Remove x and y ticks\nax.tick_params(bottom=False, left=False)\n\n\n\n\n\n\n2.7 Combine the ratings and sentiment data\nCombine the data from part 2.4 (reviews data) and part 2.6 (emotion data). Use the pd.concat() function and combine along the column axis.\nNote: You’ll need to reset the index of your reviews data frame so it matches the emotion data index (it should run from 0 to the length of the data - 1).\n\nreview_sentiment = emotion.merge(restfocus, on = \"text\", how = \"left\")\n\n\n\n2.8 Plot sentiment vs. stars\nWe now have a dataframe with the predicted primary emotion for each review and the associated number of stars for each review. Let’s explore two questions:\n\nDoes sentiment analysis work? Do reviews with fewer stars have negative emotions?\nFor our fast food restaurants, are reviews generally positive or negative?\n\nUse seaborn’s histplot() to make a stacked bar chart showing the breakdown of each emotion for each stars category (1 star, 2 stars, etc.). A few notes:\n\nTo stack multiple emotion labels in one bar, use the multiple=\"stack\" keyword\nThe discrete=True can be helpful to tell seaborn our stars values are discrete categories\n\n\nreview_sentiment_grouped = review_sentiment.groupby(\"stars_given\")[[\"sadness\", \"fear\", \"joy\", \"anger\", \"surprise\"]].mean()\nreview_sentiment_grouped = review_sentiment_grouped.reset_index()\nreview_sentiment_long = pd.melt(review_sentiment_grouped, id_vars=['stars_given'], var_name='emotion', value_name='score')\n\n\n\npalette = sns.color_palette(\"cubehelix\")\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=review_sentiment_long, x=\"stars_given\", weights=\"score\", hue=\"emotion\", multiple=\"stack\", discrete=True, palette=palette)\n\n\nplt.title('Stacked Histogram of Review Sentiment Scores by Star Rating')\nplt.xlabel(\"User's Rating\")\nplt.ylabel('Score')\n\n\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.tick_params(bottom=False, left=False)\n\nplt.show()\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_17508\\205832856.py:5: UserWarning: The palette list has more values (6) than needed (5), which may not be intended.\n  sns.histplot(data=review_sentiment_long, x=\"stars_given\", weights=\"score\", hue=\"emotion\", multiple=\"stack\", discrete=True, palette=palette)\n\n\n\n\n\nQuestion: What does your chart indicate for the effectiveness of our sentiment analysis? Does our original hypothesis about fast food restaurants seem plausible?\nThe chart indicates there are certain emotions more prominent in certain ratings. Places recieving a 4 or 5 have mostly joyful reviews, with scatters of other emotions that may have been misinterpreted by the model. Lower rated places see anger or sadness which makes sense given the user’s attitude towards the restaurant."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Dedicated public service professional with 4+ years of experience in GIS and urban planning. Skilled in machine learning, data & policy analysis, visualization, cloud computing, database management, and financial modelling. Passions include real estate, sustainability, and transportation.\nI’m passionate about geospatial science, machine learning, and mapping. My favorite tools are ArcGIS, R, and Python. I’m pursuing a Masters of Urban Spatial Analytics at the University of Pennsylvania’s Weitzman School of Design. In previous years, I worked as an urban planner-aide and GIS analyst for local governments and developed data visualizations."
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html",
    "href": "analysis/Assignment2Kapuvari.html",
    "title": "Visualization Basics in Seaborn, Matploblib, Altair",
    "section": "",
    "text": "Trevor Kapuvari Voter Turnout 2018"
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#data-wrangling-and-database-framing",
    "href": "analysis/Assignment2Kapuvari.html#data-wrangling-and-database-framing",
    "title": "Visualization Basics in Seaborn, Matploblib, Altair",
    "section": "Data Wrangling and Database Framing",
    "text": "Data Wrangling and Database Framing\nvoterturnout2018df\n\nMainParties18 = voterturnout2018df[voterturnout2018df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\nMainParties17 = voterturnout2017df[voterturnout2017df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\nMainParties16 = voterturnout2016df[voterturnout2016df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\nMainParties15 = voterturnout2015df[voterturnout2015df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\n\nmerged_elections = pd.concat([MainParties18, MainParties17, MainParties16, MainParties15], axis=0)\nmerged_elections\n\n\n\n\n\n\n\n\nelection\nelection_date\nprecinct_description\nprecinct_code\npolitical_party\nvoter_count\n\n\n\n\n0\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 01\n101\nDEMOCRATIC\n158\n\n\n3\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 01\n101\nREPUBLICAN\n9\n\n\n4\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 02\n102\nDEMOCRATIC\n174\n\n\n8\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 02\n102\nREPUBLICAN\n8\n\n\n9\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 03\n103\nDEMOCRATIC\n243\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n6473\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 44\n6644\nREPUBLICAN\n61\n\n\n6477\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n6645\nREPUBLICAN\n48\n\n\n6478\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n6645\nDEMOCRATIC\n67\n\n\n6482\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n6646\nREPUBLICAN\n94\n\n\n6483\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n6646\nDEMOCRATIC\n113\n\n\n\n\n12959 rows × 6 columns\n\n\n\n\nmerged_elections.set_index(['precinct_code', 'election', 'political_party'], append=True)\n#setting an index better organized the table to indicate ways to tidy/pivot it to our liking\n\n\n\n\n\n\n\n\n\n\n\nelection_date\nprecinct_description\nvoter_count\n\n\n\nprecinct_code\nelection\npolitical_party\n\n\n\n\n\n\n\n0\n101\n2018 GENERAL PRIMARY\nDEMOCRATIC\n5/15/2018\nPHILA WD 01 DIV 01\n158\n\n\n3\n101\n2018 GENERAL PRIMARY\nREPUBLICAN\n5/15/2018\nPHILA WD 01 DIV 01\n9\n\n\n4\n102\n2018 GENERAL PRIMARY\nDEMOCRATIC\n5/15/2018\nPHILA WD 01 DIV 02\n174\n\n\n8\n102\n2018 GENERAL PRIMARY\nREPUBLICAN\n5/15/2018\nPHILA WD 01 DIV 02\n8\n\n\n9\n103\n2018 GENERAL PRIMARY\nDEMOCRATIC\n5/15/2018\nPHILA WD 01 DIV 03\n243\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n6473\n6644\n2015 MUNICIPAL PRIMARY\nREPUBLICAN\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 44\n61\n\n\n6477\n6645\n2015 MUNICIPAL PRIMARY\nREPUBLICAN\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n48\n\n\n6478\n6645\n2015 MUNICIPAL PRIMARY\nDEMOCRATIC\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n67\n\n\n6482\n6646\n2015 MUNICIPAL PRIMARY\nREPUBLICAN\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n94\n\n\n6483\n6646\n2015 MUNICIPAL PRIMARY\nDEMOCRATIC\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n113\n\n\n\n\n12959 rows × 3 columns\n\n\n\n\ntidyElections = pd.pivot(merged_elections, columns = (\"election\", \"political_party\"), index = \"precinct_code\", values = \"voter_count\")\ntidyElections\n\n\n\n\n\n\n\nelection\n2018 GENERAL PRIMARY\n2017 MUNICIPAL PRIMARY\n2016 GENERAL PRIMARY\n2015 MUNICIPAL PRIMARY\n\n\npolitical_party\nDEMOCRATIC\nREPUBLICAN\nDEMOCRATIC\nREPUBLICAN\nREPUBLICAN\nDEMOCRATIC\nDEMOCRATIC\nREPUBLICAN\n\n\nprecinct_code\n\n\n\n\n\n\n\n\n\n\n\n\n101\n158.0\n9.0\n135.0\n4.0\n13.0\n207.0\n157.0\n3.0\n\n\n102\n174.0\n8.0\n178.0\n6.0\n38.0\n288.0\n187.0\n11.0\n\n\n103\n243.0\n16.0\n205.0\n10.0\n37.0\n318.0\n245.0\n17.0\n\n\n104\n183.0\n28.0\n147.0\n21.0\n65.0\n242.0\n170.0\n24.0\n\n\n105\n84.0\n7.0\n62.0\n8.0\n11.0\n124.0\n98.0\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6642\n49.0\n40.0\n47.0\n32.0\n95.0\n122.0\n101.0\n63.0\n\n\n6643\n84.0\n63.0\n79.0\n44.0\n176.0\n168.0\n147.0\n100.0\n\n\n6644\n98.0\n44.0\n71.0\n33.0\n122.0\n175.0\n159.0\n61.0\n\n\n6645\n31.0\n26.0\n29.0\n19.0\n100.0\n80.0\n67.0\n48.0\n\n\n6646\n58.0\n69.0\n73.0\n78.0\n164.0\n119.0\n113.0\n94.0\n\n\n\n\n1688 rows × 8 columns\n\n\n\n\nMatplotlib Chart, Data & Plotting\nHere we are comparing voter participation for the Democratic Party between the years of 2016 and 2018, specifically among their primary elections.\nMatplotlib was used for the scatter chart because of the simplisity and direct approach to plotting large sets of data. The basic approach of matplotlib creates visualizations that are easy to comprehend for viewers.\nHere, we can tell that the precincts that had more votes in 2016 also had more votes in 2018, with some notable outliers.\n\nx= tidyElections['2018 GENERAL PRIMARY']['DEMOCRATIC']\ny = tidyElections['2016 GENERAL PRIMARY']['DEMOCRATIC']\n\n\n#matplotlib\n\nfig, ax = plt.subplots()\nx = x #2018 votes\ny = y #2016 votes \nax.scatter(x, y, c='blue')\nax.set_title('Democratic Votes in General Primary, 2018  vs 2016, by Precinct')\nax.set_xlabel('Votes in 2018')\nax.set_ylabel('Votes in 2016')\nplt.ylim(0,400)\nplt.show()\n\n\n\n\nThe scatter plot displays the difference in Democratic votes in the 2018 and 2016 general primaries. Each dot represents a precinct that compares the two years. You may also notice the cluster goes off the chart when trending upward. This cut-off was done on purpose, the x and y axis are fixed to be the same to demonstrate the difference in voter participation. In 2016, a presidential election year, showed significantly more participation than in 2018, a mid-term election year. When lookinng at individual points, you will notice there is, generally, more votes counted in 2016 than in 2018."
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#altair-chart-1",
    "href": "analysis/Assignment2Kapuvari.html#altair-chart-1",
    "title": "Visualization Basics in Seaborn, Matploblib, Altair",
    "section": "Altair Chart 1",
    "text": "Altair Chart 1\n\nPie Chart of Election Participation Overall\n\nalt.Chart(Electiondf, title=\"Voting Comparisons by Election\").mark_arc(innerRadius=50).encode(\n    theta=\"Total\",\n    color=\"Election:N\",\n)\n\n\n\n\n\n\n\nThe pie chart helps us conclude the amount of voter participation in each election year.\nWe notice two major changes over time from the chart. The first is that voter turnout decreased from 2015 to 2018 generally, and peaked at 2016.\nThe second is that the largest declines in turnout were from municipal elections, specifically the years 2015 and 2017."
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#altair-2",
    "href": "analysis/Assignment2Kapuvari.html#altair-2",
    "title": "Visualization Basics in Seaborn, Matploblib, Altair",
    "section": "Altair 2",
    "text": "Altair 2\n\nVoter Participation in Elections, Visualized Through Bar Chart\n\n#altair 2\nalt.Chart(Electiondf, title=\"Voting Comparisons by Election (Bar Form)\").mark_bar().encode(\n    x=\"Election\",\n    y=\"Total\",\n).properties(\n    width=alt.Step(40),\n)\n\n\n\n\n\n\n\nThe bar graph shows the same thing as the pie chart but better compares individual years to one another. We notice here that voter turnout has been declining overall in both types of elections ever since 2016."
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#altair-3",
    "href": "analysis/Assignment2Kapuvari.html#altair-3",
    "title": "Visualization Basics in Seaborn, Matploblib, Altair",
    "section": "Altair 3",
    "text": "Altair 3\n\nScatter Plot, Democrats vs Republicans in 9 Sampled Precincts\nHere we look at 9 sampled precincts, specifically where Republicans accumulated their largest number of total votes.\n\nElection2018 = pd.read_csv(\"./Data/voters2018top9.csv\")\nElection2018\n\n\n\n\n\n\n\n\nPrecinct\nSum of REPUBLICAN\nSum of DEMOCRATIC\n\n\n\n\n0\n3505\n69\n118\n\n\n1\n4503\n81\n118\n\n\n2\n4520\n71\n99\n\n\n3\n4524\n69\n89\n\n\n4\n5824\n77\n148\n\n\n5\n5841\n90\n142\n\n\n6\n6311\n86\n180\n\n\n7\n6520\n87\n255\n\n\n8\n6617\n81\n24\n\n\n9\n6646\n69\n58\n\n\n\n\n\n\n\n\n#altair  3\nbrush = alt.selection_interval()\nalt.Chart(Election2018).mark_point().encode(\n    x='Sum of DEMOCRATIC',\n    y='Sum of REPUBLICAN',\n    color=alt.condition(brush, 'Precinct', alt.value('grey')),\n).add_params(brush)\n\n\n\n\n\n\n\nThis chart shows precincts where Republicans accumulated the most votes in their primaries. We notice in one precinct, 6617, that there was larger participation by Republicans than by Democrats. This comparison can help us predict future election results in this parrticular precinct. Meanwhile, other precincts still have a heavy Democratic lean in terms of votes."
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations."
  },
  {
    "objectID": "analysis/RFvsSKL.html",
    "href": "analysis/RFvsSKL.html",
    "title": "Comparing RandomForest to Scikit-Learn",
    "section": "",
    "text": "S1: Filtering the data\n\nLoad MNIST FASHION dataset (hint: use the practice notebook)\nSelect a training set with the first n=100 samples from each category\nSelect a testing set with the first n=100 samples from each category\nCreate a numpy matrix named mat_avg defined as: A 10 x 2 matrix with the average intensity of all images in each category for the training and testing sets (rows: 10 categories, columns: average intensity in train set, average intensity in test set)\n\nQ1.1: Which category has the largest average intensity value in training data: 2\nQ1.2: Which category has the largest average intensity value in testing data: 4\n\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.datasets import mnist\n\n\n\nmnist = tf.keras.datasets.fashion_mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train_subset = []\ny_train_subset = []\nfor i in range(10):\n    x_train_subset.append(x_train[y_train == i][:100])\n    y_train_subset.append(y_train[y_train == i][:100])\n\nx_test_subset = []\ny_test_subset = []\nfor i in range(10):\n    x_test_subset.append(x_test[y_test == i][:100])\n    y_test_subset.append(y_test[y_test == i][:100])\n\n\nmat_avg = np.zeros((10, 2))\nfor i in range(10):\n    mat_avg[i, 0] = np.mean(x_train_subset[i])\n    mat_avg[i, 1] = np.mean(x_test_subset[i])\n\nmax_train_avg = np.argmax(mat_avg[:, 0])\nprint(\"Category with largest average intensity in training data:\", max_train_avg)\n\n\nmax_test_avg = np.argmax(mat_avg[:, 1])\nprint(\"Category with largest average intensity in testing data:\", max_test_avg)\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\nCategory with largest average intensity in training data: 2\nCategory with largest average intensity in testing data: 4\n\n\n\n\nS2: Finding the average image\n\nFind and display a single average image of all shoes (categories ‘Sandal’, ‘Sneaker’, ‘Ankle boot’) in training and testing data (use the smaller sample you created)\n\n\n\nx_train_shoes = np.concatenate((x_train_subset[5], x_train_subset[7], x_train_subset[9]), axis=0)\navg_shoe_train = np.mean(x_train_shoes, axis=0)\n\nx_test_shoes = np.concatenate((x_test_subset[5], x_test_subset[7], x_test_subset[9]), axis=0)\navg_shoe_test = np.mean(x_test_shoes, axis=0)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(avg_shoe_train, cmap='viridis')\nplt.title('Average shoe in training data')\n\nplt.subplot(1, 2, 2)\nplt.imshow(avg_shoe_test, cmap='viridis')\nplt.title('Average shoe in testing data')\nplt.show()\n\n\n\n\n\n\nS3: Image distances\nIn the training set, find the shoe image that is most dissimilar from the mean shoe image. Show it as a 2D image In the training set, find the shoe image that is most similar from the mean shoe image. Show it as a 2D image Do the same for the testing set Hint: You can use the “euclidean distance” as your similarity metric. Given that an image i is represented with a flattened feature vector v_i , and the second image j with v_m, the distance between these two images can be calculated using the vector norm of their differences ( | v_i - v_j | )\nQ3.1: What is the index of most similar shoe image in the training set: 189\nQ3.2: What is the index of most dissimilar shoe image in the training set: 236\nQ3.1: What is the index of most dissimilar shoe image in the testing set: 257\n\n\ndistances_train = []\nfor i in range(len(x_train_shoes)):\n    distance = np.linalg.norm(x_train_shoes[i].flatten() - avg_shoe_train.flatten())\n    distances_train.append(distance)\n\nmost_dissimilar_train_index = np.argmax(distances_train)\nmost_dissimilar_train_image = x_train_shoes[most_dissimilar_train_index]\n\ndistances_train = []\nfor i in range(len(x_train_shoes)):\n    distance = np.linalg.norm(x_train_shoes[i].flatten() - avg_shoe_train.flatten())\n    distances_train.append(distance)\n\nmost_similar_train_index = np.argmin(distances_train)\nmost_similar_train_image = x_train_shoes[most_similar_train_index]\n\ndistances_test = []\nfor i in range(len(x_test_shoes)):\n    distance = np.linalg.norm(x_test_shoes[i].flatten() - avg_shoe_test.flatten())\n    distances_test.append(distance)\n\nmost_dissimilar_test_index = np.argmax(distances_test)\nmost_dissimilar_test_image = x_test_shoes[most_dissimilar_test_index]\n\ndistances_test = []\nfor i in range(len(x_test_shoes)):\n    distance = np.linalg.norm(x_test_shoes[i].flatten() - avg_shoe_test.flatten())\n    distances_test.append(distance)\n\nmost_similar_test_index = np.argmin(distances_test)\nmost_similar_test_image = x_test_shoes[most_similar_test_index]\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(2, 2, 1)\nplt.imshow(most_dissimilar_train_image, cmap='gray')\nplt.title('Most dissimilar shoe in training data')\n\nplt.subplot(2, 2, 2)\nplt.imshow(most_similar_train_image, cmap='gray')\nplt.title('Most similar shoe in training data')\n\nplt.subplot(2, 2, 3)\nplt.imshow(most_dissimilar_test_image, cmap='gray')\nplt.title('Most dissimilar shoe in testing data')\n\nplt.subplot(2, 2, 4)\nplt.imshow(most_similar_test_image, cmap='gray')\nplt.title('Most similar shoe in testing data')\n\nplt.show()\n\n# Q3.1: What is the index of most similar shoe image in the training set:\nprint(\"Most similar shoe index in training set:\", most_similar_train_index)\n\n# Q3.2: What is the index of most dissimilar shoe image in the training set:\nprint(\"Most dissimilar shoe index in training set:\", most_dissimilar_train_index)\n\n# Q3.1: What is the index of most dissimilar shoe image in the testing set:\nprint(\"Most dissimilar shoe index in testing set:\", most_dissimilar_test_index)\n\n\n\n\n\nMost similar shoe index in training set: 189\nMost dissimilar shoe index in training set: 236\nMost dissimilar shoe index in testing set: 257\n\n\n\n\nS4: Train a classifier to differentiate shoes from no-shoes\n\nCreate new labels for train and test images as shoes (1) or no-shoes (0)\nTrain 2 different classifiers on the training set (SVM and Random Forest). Apply the classifiers on the testing data\nDisplay the confusion matrix of each classifier\nDisplay 4 images that are mis-classified as shoes by each classifier\n\nQ1.1: What is the testing accuracy of each classifier:\nSVM, 0.9968\nRandomForest: 0.999\nQ1.2: What is the category (original label) that is most frequently mis-classified as a shoe:\nMost frequently mis-classified as shoe (SVM): 8\nMost frequently mis-classified as shoe (RF): 8\nQ1.3: What is the category (original label) that is most frequently mis-classified as a non-shoe:\nMost frequently mis-classified as non-shoe (SVM): 5\nMost frequently mis-classified as non-shoe (RF): 9\n\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n\nx_train_shoes = np.zeros(len(x_train_subset[2]) + len(x_train_subset[7]) + len(x_train_subset[9]))\nx_test_shoes = np.zeros(len(x_test_subset[2]) + len(x_test_subset[7]) + len(x_test_subset[9]))\n\nx_train_noshoes = np.ones(len(x_train) - len(x_train_shoes))\nx_test_noshoes = np.ones(len(x_test) - len(x_test_shoes))\n\nx_train_shoes_rf = np.concatenate((x_train_shoes, x_train_noshoes), axis=0)\nx_test_shoes_rf = np.concatenate((x_test_shoes, x_test_noshoes), axis=0)\n\n\ny_train_shoes = np.zeros(len(y_train_subset[2]) + len(y_train_subset[7]) + len(y_train_subset[9]))\ny_test_shoes = np.zeros(len(y_test_subset[2]) + len(y_test_subset[7]) + len(y_test_subset[9]))\n\ny_train_noshoes = np.ones(len(y_train) - len(y_train_shoes))\ny_test_noshoes = np.ones(len(y_test) - len(y_test_shoes))\n\ny_train_shoes_rf = np.concatenate((y_train_shoes, y_train_noshoes), axis=0)\ny_test_shoes_rf = np.concatenate((y_test_shoes, y_test_noshoes), axis=0)\n\n\n\nnew_y_train = np.zeros(len(y_train))\nnew_y_test = np.zeros(len(y_test))\nfor i in range(len(y_train)):\n    if y_train[i] in [5, 7, 9]:\n        new_y_train[i] = 1\n    else:\n        new_y_train[i] = 0\nfor i in range(len(y_test)):\n    if y_test[i] in [5, 7, 9]:\n        new_y_test[i] = 1\n    else:\n        new_y_test[i] = 0\n\nsvm_model = SVC(kernel='linear')\nsvm_model.fit(x_train.reshape(-1, 784), new_y_train)\n\nrf_model = RandomForestClassifier(n_estimators=100)\nrf_model.fit(x_train.reshape(-1, 784), new_y_train)\n\n\nsvm_predictions = svm_model.predict(x_test.reshape(-1, 784))\nrf_predictions = rf_model.predict(x_test.reshape(-1, 784))\n\n\nsvm_cm = confusion_matrix(new_y_test, svm_predictions)\nrf_cm = confusion_matrix(new_y_test, rf_predictions)\n\nprint(\"SVM Confusion Matrix:\")\nprint(svm_cm)\nprint(\"Random Forest Confusion Matrix:\")\nprint(rf_cm)\n\nsvm_misclassified_shoes = []\nrf_misclassified_shoes = []\nfor i in range(len(svm_predictions)):\n    if svm_predictions[i] == 1 and new_y_test[i] == 0:\n        svm_misclassified_shoes.append(i)\nfor i in range(len(rf_predictions)):\n    if rf_predictions[i] == 1 and new_y_test[i] == 0:\n        rf_misclassified_shoes.append(i)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(2, 2, 1)\nplt.imshow(x_test[svm_misclassified_shoes[0]], cmap='gray')\nplt.title('SVM misclassified shoe 1')\nplt.subplot(2, 2, 2)\nplt.imshow(x_test[svm_misclassified_shoes[1]], cmap='gray')\nplt.title('SVM misclassified shoe 2')\nplt.subplot(2, 2, 3)\nplt.imshow(x_test[rf_misclassified_shoes[0]], cmap='gray')\nplt.title('RF misclassified shoe 1')\nplt.subplot(2, 2, 4)\nplt.imshow(x_test[rf_misclassified_shoes[1]], cmap='gray')\nplt.title('RF misclassified shoe 2')\nplt.show()\n\n\nsvm_accuracy = svm_model.score(x_test.reshape(-1, 784), new_y_test)\nrf_accuracy = rf_model.score(x_test.reshape(-1, 784), new_y_test)\nprint(\"SVM testing accuracy:\", svm_accuracy)\nprint(\"Random Forest testing accuracy:\", rf_accuracy)\n\n\nsvm_misclassified_shoes_labels = []\nfor i in svm_misclassified_shoes:\n    svm_misclassified_shoes_labels.append(y_test[i])\nmost_frequent_misclassified_shoe_label = max(set(svm_misclassified_shoes_labels), key=svm_misclassified_shoes_labels.count)\nprint(\"Most frequently mis-classified as shoe (SVM):\", most_frequent_misclassified_shoe_label)\n\nrf_misclassified_shoes_labels = []\nfor i in rf_misclassified_shoes:\n    rf_misclassified_shoes_labels.append(y_test[i])\nmost_frequent_misclassified_shoe_label = max(set(rf_misclassified_shoes_labels), key=rf_misclassified_shoes_labels.count)\nprint(\"Most frequently mis-classified as shoe (RF):\", most_frequent_misclassified_shoe_label)\n\n\nsvm_misclassified_noshoes_labels = []\nfor i in range(len(svm_predictions)):\n    if svm_predictions[i] == 0 and new_y_test[i] == 1:\n        svm_misclassified_noshoes_labels.append(y_test[i])\nmost_frequent_misclassified_noshoe_label = max(set(svm_misclassified_noshoes_labels), key=svm_misclassified_noshoes_labels.count)\nprint(\"Most frequently mis-classified as non-shoe (SVM):\", most_frequent_misclassified_noshoe_label)\n\nrf_misclassified_noshoes_labels = []\nfor i in range(len(rf_predictions)):\n    if rf_predictions[i] == 0 and new_y_test[i] == 1:\n        rf_misclassified_noshoes_labels.append(y_test[i])\nmost_frequent_misclassified_noshoe_label = max(set(rf_misclassified_noshoes_labels), key=rf_misclassified_noshoes_labels.count)\nprint(\"Most frequently mis-classified as non-shoe (RF):\", most_frequent_misclassified_noshoe_label)\n\nSVM Confusion Matrix:\n[[6979   21]\n [  11 2989]]\nRandom Forest Confusion Matrix:\n[[6993    7]\n [   5 2995]]\nSVM testing accuracy: 0.9968\nRandom Forest testing accuracy: 0.9988\nMost frequently mis-classified as shoe (SVM): 8\nMost frequently mis-classified as shoe (RF): 8\nMost frequently mis-classified as non-shoe (SVM): 5\nMost frequently mis-classified as non-shoe (RF): 5\n\n\n\n\n\n\n\nBonus:\n\nIn question S4 you have a chance to discard one of image categories that are part of the non-shoe set. Which category would you prefer to discard? Explain and justify with data\n\nThe bottom two that are misclassified from the RandomForest model. These are the most “shoe-like” non-shoes that can misinform the model in terms of identifying shoes. The other two from SVM are obviously not shoes and can show themselves more as outliers, having a less hidden impact on the model.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nresult = confusion_matrix(y_test, svm_predictions)\nrf_result = confusion_matrix(y_test, rf_predictions)\n\n# other commparions, new_y_test (only 2 indexes),\n\nplt.figure(figsize=(12, 6))\n\n# Plot SVM Confusion Matrix\nplt.subplot(1, 2, 1) # 1 row, 2 columns, 1st subplot\nsns.heatmap(result, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.title(\"Confusion Matrix for SVM\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\n\n#plt.xticks(np.arange(len(xlabels)), labels, rotation=45)\n#plt.yticks(np.arange(len(ylabels)), labels)\n\n# Plot Random Forest Confusion Matrix\nplt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd subplot\nsns.heatmap(rf_result, annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\nplt.title(\"Confusion Matrix for Random Forest\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\n\n#plt.xticks(np.arange(len(labels)), labels, rotation=45)\n#plt.yticks(np.arange(len(labels)), labels)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Trevor Kapuvari’s Python Page",
    "section": "",
    "text": "This website is dedicated to my previous works in Python.\nPython is one of the many tools I’ve used for spatial and data visualizations, programming, and machine learning. Yet this site is only a fraction of the work I’ve done in the geospatial sciences atmosphere. One notable project using pythion was a cluster analysis on Philadelphia’s amenity landscape. The project was highlighted at Wharton-Weitzman Future of Cities Conference 2024\nMy other projects & experience can be found on my Portfolio or on my Resume."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Trevor Kapuvari’s Python Page",
    "section": "",
    "text": "This website is dedicated to my previous works in Python.\nPython is one of the many tools I’ve used for spatial and data visualizations, programming, and machine learning. Yet this site is only a fraction of the work I’ve done in the geospatial sciences atmosphere. One notable project using pythion was a cluster analysis on Philadelphia’s amenity landscape. The project was highlighted at Wharton-Weitzman Future of Cities Conference 2024\nMy other projects & experience can be found on my Portfolio or on my Resume."
  },
  {
    "objectID": "index.html#overview-of-python-package-experience",
    "href": "index.html#overview-of-python-package-experience",
    "title": "Trevor Kapuvari’s Python Page",
    "section": "Overview of Python Package Experience",
    "text": "Overview of Python Package Experience\n\nGeneral: Pandas, Geopandas\nVisualization: Matploblib, Altair, Seaborn, Datashader\nWebscrapping: Selenium, BeautifulSoup, Requests\nGeospatial Analysis: Cenpy, Pygris, OSMnx, Google Earth Engine\nMachine Learning: scikit-learn, RandomForest\nArtificial Intelligence: Huggingface, Tensorflow"
  },
  {
    "objectID": "RemoteSensing/index.html",
    "href": "RemoteSensing/index.html",
    "title": "Remote Sensing",
    "section": "",
    "text": "Remote Sensing\nThis section includes examples of Remote Sensing for various uses."
  },
  {
    "objectID": "RemoteSensing/national_landcover_supervised_classification.html",
    "href": "RemoteSensing/national_landcover_supervised_classification.html",
    "title": "Machine Learning with Earth Engine - Supervised Classification",
    "section": "",
    "text": "Note: the following notebook is taken verbatim from Dr. Qiusheng Wu’s geemap website with only extremely minor modifications. I have removed the image download at the end and added a model evaluation step, but otherwise it remains his work."
  },
  {
    "objectID": "RemoteSensing/national_landcover_supervised_classification.html#supervised-classification-algorithms-available-in-earth-engine",
    "href": "RemoteSensing/national_landcover_supervised_classification.html#supervised-classification-algorithms-available-in-earth-engine",
    "title": "Machine Learning with Earth Engine - Supervised Classification",
    "section": "Supervised classification algorithms available in Earth Engine",
    "text": "Supervised classification algorithms available in Earth Engine\nSource: https://developers.google.com/earth-engine/classification\nThe Classifier package handles supervised classification by traditional ML algorithms running in Earth Engine. These classifiers include CART, RandomForest, NaiveBayes and SVM. The general workflow for classification is:\n\nCollect training data. Assemble features which have a property that stores the known class label and properties storing numeric values for the predictors.\nInstantiate a classifier. Set its parameters if necessary.\nTrain the classifier using the training data.\nClassify an image or feature collection.\nEstimate classification error with independent validation data.\n\nThe training data is a FeatureCollection with a property storing the class label and properties storing predictor variables. Class labels should be consecutive, integers starting from 0. If necessary, use remap() to convert class values to consecutive integers. The predictors should be numeric."
  },
  {
    "objectID": "RemoteSensing/national_landcover_supervised_classification.html#step-by-step-tutorial",
    "href": "RemoteSensing/national_landcover_supervised_classification.html#step-by-step-tutorial",
    "title": "Machine Learning with Earth Engine - Supervised Classification",
    "section": "Step-by-step tutorial",
    "text": "Step-by-step tutorial\n\nImport libraries\n\nimport ee\nimport geemap\n\n\n            \n            \n\n\n\n\nAuthenticate and Initialize Earth Engine\n\nee.Authenticate()\n\n\n            \n            \n\n\nTrue\n\n\n\nee.Initialize(project='remotesensing-musa6500')\n\n\n            \n            \n\n\n\n\nCreate an interactive map\n\nMap = geemap.Map()\nMap\n\n\n            \n            \n\n\n\n\n\n\n\nAdd data to the map\n\npoint = ee.Geometry.Point([-122.4439, 37.7538])\n# point = ee.Geometry.Point([-87.7719, 41.8799])\n\nimage = (\n    ee.ImageCollection(\"LANDSAT/LC08/C01/T1_SR\")\n    .filterBounds(point)\n    .filterDate(\"2016-01-01\", \"2016-12-31\")\n    .sort(\"CLOUD_COVER\")\n    .first()\n    .select(\"B[1-7]\")\n)\n\nvis_params = {\"min\": 0, \"max\": 3000, \"bands\": [\"B5\", \"B4\", \"B3\"]}\n\nMap.centerObject(point, 8)\nMap.addLayer(image, vis_params, \"Landsat-8\")\n\n\n            \n            \n\n\n\n\nCheck image properties\n\nee.Date(image.get(\"system:time_start\")).format(\"YYYY-MM-dd\").getInfo()\n\n\n            \n            \n\n\n'2016-11-18'\n\n\n\nimage.get(\"CLOUD_COVER\").getInfo()\n\n\n            \n            \n\n\n0.08\n\n\n\n\nMake training dataset\nThere are several ways you can create a region for generating the training dataset.\n\nDraw a shape (e.g., rectangle) on the map and the use region = Map.user_roi\nDefine a geometry, such as region = ee.Geometry.Rectangle([-122.6003, 37.4831, -121.8036, 37.8288])\nCreate a buffer zone around a point, such as region = ee.Geometry.Point([-122.4439, 37.7538]).buffer(10000)\nIf you don’t define a region, it will use the image footprint by default\n\n\n# region = Map.user_roi\n# region = ee.Geometry.Rectangle([-122.6003, 37.4831, -121.8036, 37.8288])\n# region = ee.Geometry.Point([-122.4439, 37.7538]).buffer(10000)\n\n\n            \n            \n\n\nIn this example, we are going to use the USGS National Land Cover Database (NLCD) to create label dataset for training\n\n\nnlcd = ee.Image(\"USGS/NLCD/NLCD2016\").select(\"landcover\").clip(image.geometry())\nMap.addLayer(nlcd, {}, \"NLCD\")\nMap\n\n\n            \n            \n\n\n\n\n\n\n# Make the training dataset.\npoints = nlcd.sample(\n    **{\n        \"region\": image.geometry(),\n        \"scale\": 30,\n        \"numPixels\": 5000,\n        \"seed\": 0,\n        \"geometries\": True,  # Set this to False to ignore geometries\n    }\n)\n\nMap.addLayer(points, {}, \"training\", False)\n\n\n            \n            \n\n\n\nprint(points.size().getInfo())\n\n\n            \n            \n\n\n3583\n\n\n\nprint(points.first().getInfo())\n\n\n            \n            \n\n\n{'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [-122.25798986874739, 38.2706212827936]}, 'id': '0', 'properties': {'landcover': 31}}\n\n\n\n\nTrain the classifier\n\n# Randomly split the sample into 70% training and 30% validation\nsample = points.randomColumn()  # Adds a random column to each feature\ntrainingSample = sample.filter(ee.Filter.lt('random', 0.7))\nvalidationSample = sample.filter(ee.Filter.gte('random', 0.7))\n\n# Use these bands for prediction.\nbands = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"]\n\n\n# This property of the table stores the land cover labels.\nlabel = \"landcover\"\n\n# Overlay the points on the imagery to get training.\ntraining = image.select(bands).sampleRegions(\n    collection=trainingSample,\n    properties=[label],\n    scale=30\n)\ntrained = ee.Classifier.smileCart().train(training, label, bands)\n\n\n            \n            \n\n\n\n\nEvaluate accuracy and kappa coefficient\n\n# Classify the validation sample\nvalidation = image.select(bands).sampleRegions(\n    collection=validationSample,\n    properties=[label],\n    scale=30\n)\nvalidated = validation.classify(trained)\n\n# Compare the classified values against the actual labels\nerrorMatrix = validated.errorMatrix(label, 'classification')\n\n# compute overall accuracy\nprint('Overall Accuracy:', errorMatrix.accuracy().getInfo())\n\n# Compute Kappa statistic\nprint('Kappa Coefficient:', errorMatrix.kappa().getInfo())\n\n\n            \n            \n\n\nOverall Accuracy: 0.5014084507042254\nKappa Coefficient: 0.4326718191339521\n\n\n\nprint(training.first().getInfo())\n\n\n            \n            \n\n\n{'type': 'Feature', 'geometry': None, 'id': '0_0', 'properties': {'B1': 575, 'B2': 814, 'B3': 1312, 'B4': 1638, 'B5': 1980, 'B6': 2091, 'B7': 1967, 'landcover': 31}}\n\n\n\n\nClassify the image\n\n# Classify the image with the same bands used for training.\nresult = image.select(bands).classify(trained)\n\n# # Display the clusters with random colors.\nMap.addLayer(result.randomVisualizer(), {}, \"classified\")\nMap\n\n\n            \n            \n\n\n\n\n\n\n\nRender categorical map\nTo render a categorical map, we can set two image properties: landcover_class_values and landcover_class_palette. We can use the same style as the NLCD so that it is easy to compare the two maps.\n\nclass_values = nlcd.get(\"landcover_class_values\").getInfo()\n\n\n            \n            \n\n\n\nclass_palette = nlcd.get(\"landcover_class_palette\").getInfo()\n\n\n            \n            \n\n\n\nlandcover = result.set(\"classification_class_values\", class_values)\nlandcover = landcover.set(\"classification_class_palette\", class_palette)\n\n\n            \n            \n\n\n\nMap.addLayer(landcover, {}, \"Land cover\")\nMap\n\n\n            \n            \n\n\n\n\n\n\n\nVisualize the result\n\nprint(\"Change layer opacity:\")\ncluster_layer = Map.layers[-1]\ncluster_layer.interact(opacity=(0, 1, 0.1))\n\n\n            \n            \n\n\nChange layer opacity:\n\n\n\n\n\n\n\nAdd a legend to the map\n\nMap.add_legend(builtin_legend=\"NLCD\")\nMap"
  },
  {
    "objectID": "SpatialOptimization/CoverageMatrix.html",
    "href": "SpatialOptimization/CoverageMatrix.html",
    "title": "Trevor Kapuvari Python Demonstrations",
    "section": "",
    "text": "Maximize 27X1+694X2+32X3+X4+12X5+44X6+3X7+19X8 Subject to: X1-(Y1+Y2+Y3)&lt;=0 X2-(Y1+Y3+Y4+Y7)&lt;=0 X3-(Y1+Y3+Y5)&lt;=0 X4-Y8&lt;=0 X5-(Y1+Y4)&lt;=0 X6-(Y1+Y2+Y4+Y6)&lt;=0 X7-(Y3+Y5+Y6+Y7)&lt;=0 X8-(Y3+Y4+Y7+Y8)&lt;=0 Y1+Y2+Y3+Y4+Y5+Y6+Y7+Y8&lt;=5 Binary X1 X2 X3 X4 X5 X6 X7 X8 Y1 Y2 Y3 Y4 Y5 Y6 Y7 Y8 End"
  },
  {
    "objectID": "SpatialOptimization/Knapsack_Example.html",
    "href": "SpatialOptimization/Knapsack_Example.html",
    "title": "Trevor Kapuvari Python Demonstrations",
    "section": "",
    "text": "Example of use of the Pulp libray for a simple knapsack problem\n\n#Install pulp\n!pip install pulp\n\n# Import libraries\nfrom pulp import *\nimport time\n\nCollecting pulp\n  Downloading PuLP-2.8.0-py3-none-any.whl (17.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.7/17.7 MB 19.9 MB/s eta 0:00:00\nInstalling collected packages: pulp\nSuccessfully installed pulp-2.8.0\n\n\nLet set up our problem with the decision variables (items), with their value “v” and weight “w”. We then set up the capacity of our bagpack.\n\n# A list of tuples of items (value, weight)\n# (value, volume)\nitems = [(3,4), (8,5), (10,1.5), (2,0.5), (7,1),\n (10,2), (5,1.5), (9,1), (9,0.5), (6,0.5)]\n\n# number of items\nitemCount = len(items)\n\n# Knapsack max weight capacity\nbinCapacity = 7\n\nHere we set up our decision variables (the X), the lower and upper bounds, couple with the integer constrains (cat=Integer) tells us the decision variable will be binary.\n\n# Decision variables (array), x[i] gets 1 when item i is included in the solution\nx = pulp.LpVariable.dicts('item', range(itemCount),\n                            lowBound = 0,\n                            upBound = 1,\n                            cat = 'Integer')\n\nHere we set up our problem (it is of maximization type, and it is a linear programming problem).\nWe add the objective function (sum of the values), and the constraint (sum of the weights).\n\n# Initialize the problem and specify the type\nproblem = LpProblem(\"Knapsack Problem\", LpMaximize)\n\n# Add the objective function\nproblem += lpSum([ x[i] * (items[i])[0] for i in range(itemCount) ]), \"Objective: Maximize profit\"\n\n# Capacity constraint: the sum of the weights must be less than the capacity\nproblem += lpSum([ x[i] * (items[i])[1] for i in range(itemCount) ]) &lt;= binCapacity, \"Constraint: Max capacity\"\n\nHere we write the problem as an lp file and we then solve it, using the “solve” function. We also keep track of the time it takes.\n\n#print problem.constraints\n\n# Write the model to disk, not necessary\nproblem.writeLP(\"Knapsack.lp\")\n\n# Solve the optimization problem\nstart_time = time.time()\nproblem.solve()\nprint(\"Solved in %s seconds.\" % (time.time() - start_time))\n\nSolved in 0.011299371719360352 seconds.\n\n\nFinally, we report on the optimal solution, and whether each variable was chosen or not\n\n# Was the problem solved to optimality?\nprint(\"Status:\", LpStatus[problem.status])\n\n# Each of the variables is printed with it's resolved optimum value\nfor v in problem.variables():\n    print(v.name, \"=\", v.varValue)\n\n# The optimised objective function value is printed to the screen\nprint(\"Total profit = \", value(problem.objective))\n\nStatus: Optimal\nitem_0 = 0.0\nitem_1 = 0.0\nitem_2 = 1.0\nitem_3 = 1.0\nitem_4 = 1.0\nitem_5 = 1.0\nitem_6 = 0.0\nitem_7 = 1.0\nitem_8 = 1.0\nitem_9 = 1.0\nTotal profit =  53.0\n\n\nHere, somore more info about the solution\n\nused_cap = 0.0\nprint(\"Used items:\")\nfor i in range(itemCount):\n    if x[i].value() == 1:\n        print(i, items[i])\n        used_cap += items[i][1]\nprint(\"Profit: %d - Used capacity: %d (/ %d)\" % (value(problem.objective), used_cap, binCapacity))\n\n\nUsed items:\n2 (10, 1.5)\n3 (2, 0.5)\n4 (7, 1)\n5 (10, 2)\n7 (9, 1)\n8 (9, 0.5)\n9 (6, 0.5)\nProfit: 53 - Used capacity: 7 (/ 7)"
  }
]