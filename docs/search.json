[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "MUSA 550 Final Project Template",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on our course’s GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template.\nWe covered the basics of getting started with Quarto and GitHub Pages in week 9. Take a look at the slides for lecture 9A to find out more."
  },
  {
    "objectID": "analysis/predictionmodelling.html",
    "href": "analysis/predictionmodelling.html",
    "title": "Predictive Modeling of Housing Prices in Philadelphia",
    "section": "",
    "text": "Due date: Wednesday, 12/6 by the end of the day\nLectures 12B and 13A will cover predictive modeling of housing prices in Philadelphia. We’ll extend that analysis in this section by:"
  },
  {
    "objectID": "analysis/predictionmodelling.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "href": "analysis/predictionmodelling.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "title": "Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness",
    "text": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness\n\n2.1 Load data from the Office of Property Assessment\nUse the requests package to query the CARTO API for single-family property assessment data in Philadelphia for properties that had their last sale during 2022.\nSources: - OpenDataPhilly - Metadata\n\nimport geopandas as gpd\nimport holoviews as hv\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport requests\nimport missingno as msno\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Show all columns\npd.options.display.max_columns = 999\n\n\n\n\n\n\n\n\n\n\n\n\n# The API endpoint\ncarto_url = \"https://phl.carto.com/api/v2/sql\"\n\n# Only pull 2022 sales for single family residential properties\nwhere = \"sale_date &gt;= '2022-01-01' and sale_date &lt;= '2022-12-31'\"\nwhere = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n\n\n# Create the query\nquery = f\"SELECT * FROM opa_properties_public WHERE {where}\"\n\n# Make the request\nparams = {\"q\": query, \"format\": \"geojson\", \"where\": where}\nresponse = requests.get(carto_url, params=params)\n\n# Make the GeoDataFrame\nsalesRaw = gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\n\n\nsalesRaw.head()\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\ncensus_tract\ncentral_air\ncross_reference\ndate_exterior_condition\ndepth\nexempt_building\nexempt_land\nexterior_condition\nfireplaces\nfrontage\nfuel\ngarage_spaces\ngarage_type\ngeneral_construction\ngeographic_ward\nhomestead_exemption\nhouse_extension\nhouse_number\ninterior_condition\nlocation\nmailing_address_1\nmailing_address_2\nmailing_care_of\nmailing_city_state\nmailing_street\nmailing_zip\nmarket_value\nmarket_value_date\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_of_rooms\nnumber_stories\noff_street_open\nother_building\nowner_1\nowner_2\nparcel_number\nparcel_shape\nquality_grade\nrecording_date\nregistry_number\nsale_date\nsale_price\nseparate_utilities\nsewer\nsite_type\nstate_code\nstreet_code\nstreet_designation\nstreet_direction\nstreet_name\nsuffix\ntaxable_building\ntaxable_land\ntopography\ntotal_area\ntotal_livable_area\ntype_heater\nunfinished\nunit\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\n\n\n\n\n0\nPOINT (-75.13389 40.03928)\n1056\n2022-05-24T00:00:00Z\nH\n241' N OF CHEW ST\n54230133\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n275\nN\nNone\nNone\n95.0\n0.0\n0.0\n7\n0.0\n15.0\nNone\n1.0\nNone\nB\n61\n0\nNone\n5732\n4\n5732 N 7TH ST\nWALKER MICHAEL\nNone\nNone\nSICKLERVILLE NJ\n44 FARMHOUSE RD\n08081\n133400\nNone\n1.0\n3.0\nNaN\n2.0\n1920.0\nNone\nWALKER MICHAEL\nNone\n612234600\nE\nC\n2023-10-04T00:00:00Z\n135N7 61\n2022-08-21T00:00:00Z\n21000\nNone\nNone\nNone\nNJ\n87930\nST\nN\n7TH\nNone\n106720.0\n26680.0\nF\n1425.0\n1164.0\nH\nNone\nNone\nNone\nI\n1925\nY\n19120\nRSA5\n1001602509\n24\nROW PORCH FRONT\n401090517\n\n\n1\nPOINT (-75.14337 40.00957)\n1145\n2022-05-24T00:00:00Z\nD\n415' N OF ERIE AVE\n54230032\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n198\nN\nNone\nNone\n45.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n0.0\nNone\nA\n43\n0\nNone\n3753\n4\n3753 N DELHI ST\nNone\nNone\nNone\nDELRAY BEACH FL\n4899 NW 6TH STREET\n33445\n73800\nNone\n1.0\n3.0\nNaN\n2.0\n1683.0\nNone\nRJ SIMPLE SOLUTION LLC\nNone\n432345900\nE\nC\n2023-10-04T00:00:00Z\n100N040379\n2022-06-13T00:00:00Z\n35000\nNone\nNone\nNone\nFL\n28040\nST\nN\nDELHI\nNone\n59040.0\n14760.0\nF\n720.0\n960.0\nH\nNone\nNone\nNone\nI\n1942\nY\n19140\nRM1\n1001175031\n24\nROW PORCH FRONT\n401090494\n\n\n2\nPOINT (-75.07249 40.01381)\n1357\n2022-05-24T00:00:00Z\nD\n119'11 1/2\" NE\n54228837\nH30\nSEMI/DET 2 STY MASONRY\n1\nSINGLE FAMILY\n299\nN\nNone\nNone\n76.0\n0.0\n0.0\n4\n0.0\n20.0\nNone\n0.0\nNone\nB\n62\n0\nNone\n5033\n4\n5033 DITMAN ST\nCSC INGEO\nNone\nNone\nPHILADELPHIA PA\n5033 DITMAN ST\n19124-2230\n119100\nNone\n1.0\n3.0\nNaN\n2.0\n698.0\nNone\nLISHANSKY MARINA\nNone\n622444400\nE\nC\n2023-10-02T00:00:00Z\n89N17 208\n2022-12-28T00:00:00Z\n1\nNone\nNone\nNone\nPA\n28660\nST\nNone\nDITMAN\nNone\n95280.0\n23820.0\nF\n1523.0\n1190.0\nB\nNone\nNone\nNone\nI\n1945\nNone\n19124\nRSA5\n1001181518\n32\nTWIN CONVENTIONAL\n401090706\n\n\n3\nPOINT (-75.12854 40.03916)\n1766\n2022-05-24T00:00:00Z\nNone\n71'8\" E LAWRENCE ST\n54226519\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n274\nNone\nNone\nNone\n88.0\n0.0\n0.0\n4\n0.0\n14.0\nNone\n0.0\nNone\nA\n61\n0\nNone\n416\n4\n416 W GRANGE AVE\nNone\nNone\nNone\nPHILADELPHIA PA\n416 W GRANGE AVE\n19120-1854\n124100\nNone\n1.0\n3.0\nNaN\n2.0\n957.0\nNone\nWALLACE DANE\nNone\n612061100\nE\nC\n2023-09-25T00:00:00Z\n122N2 150\n2022-10-26T00:00:00Z\n1\nNone\nNone\nNone\nPA\n38040\nAVE\nW\nGRANGE\nNone\n99280.0\n24820.0\nF\n1241.0\n1104.0\nNone\nNone\nNone\nNone\nI\n1953\nY\n19120\nRSA5\n1001249126\n24\nROW PORCH FRONT\n401091115\n\n\n4\nPOINT (-75.17362 39.99887)\n2005\n2022-05-24T00:00:00Z\nD\n261'4\" N OF SOMERSET\n54217081\nO50\nROW 3 STY MASONRY\n1\nSINGLE FAMILY\n172\nN\nNone\nNone\n56.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n0.0\nNone\nB\n38\n0\nNone\n2834\n4\n2834 N 26TH ST\nNEAL KIYONNA\nNone\nNone\nPHILADELPHIA PA\n6007 N FRONT ST\n19120\n92900\nNone\n0.0\n5.0\nNaN\n3.0\n2457.0\nNone\nNEAL KIYONNA\nNone\n381152100\nE\nC+\n2023-08-28T00:00:00Z\n035N230348\n2022-05-11T00:00:00Z\n1\nNone\nNone\nNone\nPA\n88300\nST\nN\n26TH\nNone\n74320.0\n18580.0\nF\n896.0\n1636.0\nH\nNone\nNone\nNone\nI\n1940\nY\n19132\nRSA5\n1001643492\n24\nROW PORCH FRONT\n401092557\n\n\n\n\n\n\n\n\nlen(salesRaw)\n\n24456"
  },
  {
    "objectID": "analysis/predictionmodelling.html#clean-data",
    "href": "analysis/predictionmodelling.html#clean-data",
    "title": "Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Clean data",
    "text": "Clean data\n\n# The feature columns we want to use\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    \"zip_code\",\n    \"geometry\"\n]\n\n# Trim to these columns and remove NaNs\nsales = salesRaw[cols].dropna()\n\n# Trim zip code to only the first five digits\nsales['zip_code'] = sales['zip_code'].astype(str).str.slice(0, 5)\n\n\nlen(sales)\n\n23463\n\n\n\n2.2 Load data for census tracts and neighborhoods\nLoad various Philadelphia-based regions that we will use in our analysis.\n\nCensus tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\nNeighborhoods can be downloaded from: https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n\n\nneigh = gpd.read_file(\"https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\")\ntract = gpd.read_file(\"Census_Tracts_2010.geojson\")\n\n\ntract.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n2.3 Spatially join the sales data and neighborhoods/census tracts.\nPerform a spatial join, such that each sale has an associated neighborhood and census tract.\nNote: After performing the first spatial join, you will need to use the drop() function to remove the index_right column; otherwise an error will be raised on the second spatial join about duplicate columns.\n\njoin = gpd.sjoin(\n        sales, tract.to_crs(tract.crs), predicate=\"within\").drop(columns=[\"index_right\"])\n# print(join.columns)\n\nsales_full = gpd.sjoin(\n    join,\n        neigh.to_crs(neigh.crs), predicate = \"within\").drop(columns=[\"index_right\"])\nsales_full.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\ngeometry\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id\ncreated_at\nupdated_at\n\n\n\n\n0\n21000\n1164.0\n1425.0\n1.0\n0.0\n1.0\n3.0\n2.0\n7\n19120\nPOINT (-75.13389 40.03928)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n586\n1\n1188.0\n1575.0\n0.0\n0.0\n1.0\n3.0\n2.0\n3\n19120\nPOINT (-75.13367 40.04039)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1041\n137000\n1176.0\n1234.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13277 40.04028)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1567\n259000\n1310.0\n1754.0\n1.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13173 40.03868)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n2000\n220000\n1272.0\n1445.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13245 40.03959)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n\n\n\n\n\n\nsales_full.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\ngeometry\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id\ncreated_at\nupdated_at\n\n\n\n\n0\n21000\n1164.0\n1425.0\n1.0\n0.0\n1.0\n3.0\n2.0\n7\n19120\nPOINT (-75.13389 40.03928)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n586\n1\n1188.0\n1575.0\n0.0\n0.0\n1.0\n3.0\n2.0\n3\n19120\nPOINT (-75.13367 40.04039)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1041\n137000\n1176.0\n1234.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13277 40.04028)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1567\n259000\n1310.0\n1754.0\n1.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13173 40.03868)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n2000\n220000\n1272.0\n1445.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\n19120\nPOINT (-75.13245 40.03959)\n350\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825\n0\n+40.0400497\n-075.1322707\n10588\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n\n\n\n\n\n\n\n2.4 Train a Random Forest on the sales data\nIn this step, you should follow the steps outlined in lecture to preprocess and train your model. We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\nPreprocessing Requirements\nTrim the sales data to those sales with prices between $3,000 and $1 million\nSet up a pipeline that includes both numerical columns and categorical columns Include one-hot encoded variable for the neighborhood of the sale, instead of ZIP code. We don’t want to include multiple location based categories, since they encode much of the same information.\nTraining requirements - Use a 70/30% training/test split and predict the log of the sales price. - Use GridSearchCV to perform a k-fold cross validation that optimize at least 2 hyperparameters of the RandomForestRegressor - After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\nNote: You don’t need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!\n\ntrim_sales = sales_full.loc[sales_full[\"sale_price\"]&gt;3000].loc[sales_full[\"sale_price\"]&lt;1000000]\ntrim_sales.rename(columns={\"mapname\":\"neighborhood\"},inplace=True)\n\n# The feature columns we want to use\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    \"neighborhood\",\n    \"log(sale_price)\"\n]\n\n# Trim to these columns and remove NaNs\ntrim_sales[\"log(sale_price)\"] = np.log(trim_sales[\"sale_price\"])\nsaleDF = trim_sales[cols].dropna()\n\n\nsaleDF.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nneighborhood\nlog(sale_price)\n\n\n\n\n0\n21000\n1164.0\n1425.0\n1.0\n0.0\n1.0\n3.0\n2.0\n7\nOlney\n9.952278\n\n\n1041\n137000\n1176.0\n1234.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\nOlney\n11.827736\n\n\n1567\n259000\n1310.0\n1754.0\n1.0\n0.0\n2.0\n3.0\n2.0\n4\nOlney\n12.464583\n\n\n2000\n220000\n1272.0\n1445.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\nOlney\n12.301383\n\n\n2618\n200000\n1304.0\n1360.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\nOlney\n12.206073\n\n\n\n\n\n\n\n\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nfrom itertools import compress\n\nnumCols = []\ntxtCols = []\nfor col in cols:\n\n    numCols.append(is_numeric_dtype(saleDF[col]))\n    txtCols.append(~numCols[-1])\n\n\nnumCols = list(compress(cols,numCols))[1:-1] \ncatCols = [\"exterior_condition\",\"neighborhood\"]\n\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numCols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catCols),\n    ]\n)\nnumCols\n\n['total_livable_area',\n 'total_area',\n 'garage_spaces',\n 'fireplaces',\n 'number_of_bathrooms',\n 'number_of_bedrooms',\n 'number_stories']\n\n\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(saleDF, test_size=0.3, random_state=42)\n\n# the target labels: log of sale price\ny_train = train_set[\"log(sale_price)\"]\ny_test = test_set[\"log(sale_price)\"]\n\n\n\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=42)\n                                       \n)\n\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [2, 5, 7, 9, 13, 21, 33, 51],\n}\n\n# Create the grid and use 3-fold CV\ngrid = GridSearchCV(pipe, param_grid, cv=3, verbose=1)\n\n# Run the search\ngrid.fit(train_set, y_train)\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'neighborhood'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'neighborhood'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['total_livable_area',\n                                                   'total_area',\n                                                   'garage_spaces',\n                                                   'fireplaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'neighborhood'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['total_livable_area', 'total_area',\n                                  'garage_spaces', 'fireplaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'neighborhood'])])num['total_livable_area', 'total_area', 'garage_spaces', 'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories']StandardScalerStandardScaler()cat['exterior_condition', 'neighborhood']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nprint(\"Getting 3-fold cross validation results.\")\n\nscores = cross_val_score(\n    pipe,\n    train_set,\n    y_train,\n    cv=3,\n)\n\nprint(\"Completed\")\n# # Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Standard deviation = \", scores.std())\n\nGetting 3-fold cross validation results.\nCompleted\nR^2 scores =  [0.56575893 0.51311316 0.53833274]\nScores mean =  0.5390682769418876\nStandard deviation =  0.021498838781376265\n\n\n\nprint(f\"The optimal model of our 64 candiates had a max depth of {grid.best_estimator_.steps[1][1].max_depth} and an n_estimator count of {grid.best_estimator_.steps[1][1].n_estimators}\\nleading to an accuracy score of {grid.best_estimator_.score(test_set,y_test)} or {grid.best_estimator_.score(test_set,y_test)*100:.2f}% on the test set.\")\n\n\n\nprint(\"\\n\\nCV Score Report ...\\n\")\nprint(\"R^2 scores:\\n\\tFold 1 = \", scores[0],\"\\n\\tFold 2 = \", scores[1],\"\\n\\tFold 3 = \", scores[2],)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score standard deviation = \", scores.std())\n\nThe optimal model of our 64 candiates had a max depth of 51 and an n_estimator count of 200\nleading to an accuracy score of 0.5573963547661122 or 55.74% on the test set.\n\n\nCV Score Report ...\n\nR^2 scores:\n    Fold 1 =  0.5657589318132974 \n    Fold 2 =  0.5131131592492879 \n    Fold 3 =  0.5383327397630778\nScores mean =  0.5390682769418876\nScore standard deviation =  0.021498838781376265\n\n\n\n#not sure why I am getting an error for transformer but the cells after still work\nrandom_forest = grid.best_estimator_.named_steps[\"randomforestregressor\"]\nfeatures = numCols + list(transformer.named_transformers_['cat'].get_feature_names_out())\n\n\nimportance = pd.DataFrame(\n    {\"Feature\": features, \"Importance\": random_forest.feature_importances_}\n)\n\nimportance = importance.sort_values(\"Importance\", ascending=False).iloc[:30]\n\nimportance.head(n=20)\n\n\nimportance.hvplot.barh(x=\"Feature\", y=\"Importance\", height=700, flip_yaxis=True)\n\nAttributeError: 'ColumnTransformer' object has no attribute 'transformers_'\n\n\n\n\n2.5 Calculate the percent error of your model predictions for each sale in the test set\nFit your best model and use it to make predictions on the test set.\nNote: This should be the percent error in terms of sale price. You’ll need to convert if your model predicted the log of sales price!\n\nimport math\n\npredictions = grid.best_estimator_.predict(test_set)\n\n\nerrors = predictions - y_test \n\nprecent_errors = math.e**(predictions) - math.e**(y_test) \n\nprint(errors[0])\nprint(precent_errors[0])\nprint()\n\nprint(test_set[0:1][\"log(sale_price)\"])\nprint()\nprint(predictions[0])\n\n1.5150772918914885\n74545.2282497846\n\n16061    11.350407\nName: log(sale_price), dtype: float64\n\n12.527490609299118\n\n\n\n\n2.6 Make a data frame with percent errors and census tract info for each sale in the test set\nCreate a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\nNotes\n\nWhen using the “train_test_split()” function, the index of the test data frame includes the labels from the original sales data frame\nYou can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\nAdd a new column to this data frame holding the percent error data\nMake sure to use the percent error and not the absolute percent error\n\n\ntest_saleDF = saleDF.loc[test_set.index]\n# test_set.index\n\ntest_saleDF[\"log(predictions)\"] = predictions\ntest_saleDF[\"predictions\"] = math.e**predictions\ntest_saleDF[\"error\"] = precent_errors\ntest_saleDF[\"percent_error\"] = (precent_errors)/test_saleDF[\"sale_price\"]*100\n\ntest_saleDF = sales_full.merge(test_saleDF,how=\"right\")\n\n\ntest_saleDF[[\"sale_price\",\"log(sale_price)\",\"log(predictions)\",\"predictions\",\"error\",\"percent_error\"]].tail()\n\n\n\n\n\n\n\n\nsale_price\nlog(sale_price)\nlog(predictions)\npredictions\nerror\npercent_error\n\n\n\n\n5352\n181000\n12.106252\n11.345868\n84615.078520\n-96384.921480\n-53.251338\n\n\n5353\n300000\n12.611538\n12.959598\n424895.325384\n124895.325384\n41.631775\n\n\n5354\n265000\n12.487485\n12.673232\n319091.035201\n54091.035201\n20.411711\n\n\n5355\n380000\n12.847927\n12.646338\n310624.010585\n-69375.989415\n-18.256839\n\n\n5356\n134000\n11.805595\n12.530408\n276622.279170\n142622.279170\n106.434537\n\n\n\n\n\n\n\n\n\n2.8 Plot a map of the median percent error by census tract\n\nYou’ll want to group your data frame of test sales by the GEOID10 column and take the median of you percent error column\nMerge the census tract geometries back in and use geopandas to plot.\n\n\ntractPE = tract.merge(test_saleDF.groupby(\"GEOID10\").agg({\"predictions\": \"median\",\"error\": \"median\",\"percent_error\" : \"median\"}), on=\"GEOID10\")\n# sales_full.geometry\ntractPE.columns\n\nIndex(['OBJECTID', 'STATEFP10', 'COUNTYFP10', 'TRACTCE10', 'GEOID10', 'NAME10',\n       'NAMELSAD10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10',\n       'INTPTLAT10', 'INTPTLON10', 'LOGRECNO', 'geometry', 'predictions',\n       'error', 'percent_error'],\n      dtype='object')\n\n\n\ntractPE.rename(columns={\"NAME10\":\"Census Tract #\",\"predictions\":\"Predicted Sale Price ($)\",\"error\":\"Prediction Error ($)\",\"percent_error\":\"Percent Error\"}).explore(\n    column = \"Percent Error\",\n    cmap = \"viridis\",\n    tiles = \"CartoDB positron\",\n    tooltip = [\"Census Tract #\",\"Predicted Sale Price ($)\",\"Prediction Error ($)\",\"Percent Error\"]\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n2.9 Compare the percent errors in Qualifying Census Tracts and other tracts\nQualifying Census Tracts are a poverty designation that HUD uses to allocate housing tax credits\n\nI’ve included a list of the census tract names that qualify in Philadelphia\nAdd a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\nThen, group by this new column and calculate the median percent error\n\nYou should find that the algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts\n\nqct = ['5',\n '20',\n '22',\n '28.01',\n '30.01',\n '30.02',\n '31',\n '32',\n '33',\n '36',\n '37.01',\n '37.02',\n '39.01',\n '41.01',\n '41.02',\n '56',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '69',\n '70',\n '71.01',\n '71.02',\n '72',\n '73',\n '74',\n '77',\n '78',\n '80',\n '81.01',\n '81.02',\n '82',\n '83.01',\n '83.02',\n '84',\n '85',\n '86.01',\n '86.02',\n '87.01',\n '87.02',\n '88.01',\n '88.02',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '98.01',\n '100',\n '101',\n '102',\n '103',\n '104',\n '105',\n '106',\n '107',\n '108',\n '109',\n '110',\n '111',\n '112',\n '113',\n '119',\n '121',\n '122.01',\n '122.03',\n '131',\n '132',\n '137',\n '138',\n '139',\n '140',\n '141',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '151.01',\n '151.02',\n '152',\n '153',\n '156',\n '157',\n '161',\n '162',\n '163',\n '164',\n '165',\n '167.01',\n '167.02',\n '168',\n '169.01',\n '169.02',\n '170',\n '171',\n '172.01',\n '172.02',\n '173',\n '174',\n '175',\n '176.01',\n '176.02',\n '177.01',\n '177.02',\n '178',\n '179',\n '180.02',\n '188',\n '190',\n '191',\n '192',\n '195.01',\n '195.02',\n '197',\n '198',\n '199',\n '200',\n '201.01',\n '201.02',\n '202',\n '203',\n '204',\n '205',\n '206',\n '208',\n '239',\n '240',\n '241',\n '242',\n '243',\n '244',\n '245',\n '246',\n '247',\n '249',\n '252',\n '253',\n '265',\n '267',\n '268',\n '271',\n '274.01',\n '274.02',\n '275',\n '276',\n '277',\n '278',\n '279.01',\n '279.02',\n '280',\n '281',\n '282',\n '283',\n '284',\n '285',\n '286',\n '287',\n '288',\n '289.01',\n '289.02',\n '290',\n '291',\n '293',\n '294',\n '298',\n '299',\n '300',\n '301',\n '302',\n '305.01',\n '305.02',\n '309',\n '311.01',\n '312',\n '313',\n '314.01',\n '314.02',\n '316',\n '318',\n '319',\n '321',\n '325',\n '329',\n '330',\n '337.01',\n '345.01',\n '357.01',\n '376',\n '377',\n '380',\n '381',\n '382',\n '383',\n '389',\n '390']\n\n\ntractPE[\"is_qct\"] = tractPE[\"NAME10\"].isin(qct)\nqctGroup = tractPE.groupby(\"is_qct\",as_index=False).agg({\"predictions\": \"median\",\"error\": [\"mean\",\"median\"],\"percent_error\" : [\"mean\",\"median\"]})\nqct_t = qctGroup[1:][\"percent_error\"][\"median\"].squeeze()\nqct_f = qctGroup[:1][\"percent_error\"][\"median\"].squeeze()\nqctGroup"
  },
  {
    "objectID": "analysis/evictions.html",
    "href": "analysis/evictions.html",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport hvplot.pandas \nimport matplotlib.pyplot as plt\nimport rasterio as rio\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\nThis assignment will contain two parts:"
  },
  {
    "objectID": "analysis/evictions.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "href": "analysis/evictions.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "Part 1: Exploring Evictions and Code Violations in Philadelphia",
    "text": "Part 1: Exploring Evictions and Code Violations in Philadelphia\nIn this assignment, we’ll explore spatial trends evictions in Philadelphia using data from the Eviction Lab and building code violations using data from OpenDataPhilly.\nWe’ll be exploring the idea that evictions can occur as retaliation against renters for reporting code violations. Spatial correlations between evictions and code violations from the City’s Licenses and Inspections department can offer some insight into this question.\nA couple of interesting background readings: - HuffPost article - PlanPhilly article"
  },
  {
    "objectID": "analysis/evictions.html#explore-eviction-lab-data",
    "href": "analysis/evictions.html#explore-eviction-lab-data",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.1 Explore Eviction Lab Data",
    "text": "1.1 Explore Eviction Lab Data\nThe Eviction Lab built the first national database for evictions. If you aren’t familiar with the project, you can explore their website: https://evictionlab.org/\n\n1.1.1 Read data using geopandas\nThe first step is to read the eviction data by census tract using geopandas. The data for all of Pennsylvania by census tract is available in the data/ folder in a GeoJSON format.\nLoad the data file “PA-tracts.geojson” using geopandas\nNote: If you’d like to see all columns in the data frame, you can increase the max number of columns using pandas display options:\n\nCityLimitsdf = gpd.read_file(\"./Data/City_Limits.geojson\")\nPATractsdf = gpd.read_file(\"./Data/PA-tracts.geojson\")\npprTreePoints = gpd.read_file(\"./Data/ppr_tree_canopy_points_2015.geojson\")\n\n\n#CityLimitsdf.head()\n#PATractsdf.head()\n#pprTreePoints.head(5)\n\n\n\n1.1.2 Explore and trim the data\nWe will need to trim data to Philadelphia only. Take a look at the data dictionary for the descriptions of the various columns in top-level repository folder: eviction_lab_data_dictionary.txt\nNote: the column names are shortened — see the end of the above file for the abbreviations. The numbers at the end of the columns indicate the years. For example, e-16 is the number of evictions in 2016.\nTake a look at the individual columns and trim to census tracts in Philadelphia. (Hint: Philadelphia is both a city and a county).\n\ntracts_filtered = PATractsdf[PATractsdf['pl'].str.startswith('Philadelphia')]\n\n\ntracts_filtered.head()\n\n\n\n\n\n\n\n\nGEOID\nwest\nsouth\neast\nnorth\nn\npl\np-00\npr-00\nroh-00\npro-00\nmgr-00\nmhi-00\nmpv-00\nrb-00\npw-00\npaa-00\nph-00\npai-00\npa-00\npnp-00\npm-00\npo-00\nef-00\ne-00\ner-00\nefr-00\nlf-00\nimputed-00\nsubbed-00\np-01\npr-01\nroh-01\npro-01\nmgr-01\nmhi-01\nmpv-01\nrb-01\npw-01\npaa-01\nph-01\npai-01\npa-01\npnp-01\npm-01\npo-01\nef-01\ne-01\ner-01\nefr-01\nlf-01\nimputed-01\nsubbed-01\np-02\npr-02\nroh-02\npro-02\nmgr-02\nmhi-02\nmpv-02\nrb-02\npw-02\npaa-02\nph-02\npai-02\npa-02\npnp-02\npm-02\npo-02\nef-02\ne-02\ner-02\nefr-02\nlf-02\nimputed-02\nsubbed-02\np-03\npr-03\nroh-03\npro-03\nmgr-03\nmhi-03\nmpv-03\nrb-03\npw-03\npaa-03\nph-03\npai-03\npa-03\npnp-03\npm-03\npo-03\nef-03\ne-03\ner-03\nefr-03\nlf-03\nimputed-03\nsubbed-03\np-04\npr-04\nroh-04\npro-04\nmgr-04\nmhi-04\nmpv-04\nrb-04\npw-04\npaa-04\nph-04\npai-04\npa-04\npnp-04\npm-04\npo-04\nef-04\ne-04\ner-04\nefr-04\nlf-04\nimputed-04\nsubbed-04\np-05\npr-05\nroh-05\npro-05\nmgr-05\nmhi-05\nmpv-05\nrb-05\npw-05\npaa-05\nph-05\npai-05\npa-05\npnp-05\npm-05\npo-05\nef-05\ne-05\ner-05\nefr-05\nlf-05\nimputed-05\nsubbed-05\np-06\npr-06\nroh-06\npro-06\nmgr-06\nmhi-06\nmpv-06\nrb-06\npw-06\npaa-06\nph-06\npai-06\npa-06\npnp-06\npm-06\npo-06\nef-06\ne-06\ner-06\nefr-06\nlf-06\nimputed-06\nsubbed-06\np-07\npr-07\nroh-07\npro-07\nmgr-07\nmhi-07\nmpv-07\nrb-07\npw-07\npaa-07\nph-07\npai-07\npa-07\npnp-07\npm-07\npo-07\nef-07\ne-07\ner-07\nefr-07\nlf-07\nimputed-07\nsubbed-07\np-08\npr-08\nroh-08\npro-08\nmgr-08\nmhi-08\nmpv-08\nrb-08\npw-08\npaa-08\nph-08\npai-08\npa-08\npnp-08\npm-08\npo-08\nef-08\ne-08\ner-08\nefr-08\nlf-08\nimputed-08\nsubbed-08\np-09\npr-09\nroh-09\npro-09\nmgr-09\nmhi-09\nmpv-09\nrb-09\npw-09\npaa-09\nph-09\npai-09\npa-09\npnp-09\npm-09\npo-09\nef-09\ne-09\ner-09\nefr-09\nlf-09\nimputed-09\nsubbed-09\np-10\npr-10\nroh-10\npro-10\nmgr-10\nmhi-10\nmpv-10\nrb-10\npw-10\npaa-10\nph-10\npai-10\npa-10\npnp-10\npm-10\npo-10\nef-10\ne-10\ner-10\nefr-10\nlf-10\nimputed-10\nsubbed-10\np-11\npr-11\nroh-11\npro-11\nmgr-11\nmhi-11\nmpv-11\nrb-11\npw-11\npaa-11\nph-11\npai-11\npa-11\npnp-11\npm-11\npo-11\nef-11\ne-11\ner-11\nefr-11\nlf-11\nimputed-11\nsubbed-11\np-12\npr-12\nroh-12\npro-12\nmgr-12\nmhi-12\nmpv-12\nrb-12\npw-12\npaa-12\nph-12\npai-12\npa-12\npnp-12\npm-12\npo-12\nef-12\ne-12\ner-12\nefr-12\nlf-12\nimputed-12\nsubbed-12\np-13\npr-13\nroh-13\npro-13\nmgr-13\nmhi-13\nmpv-13\nrb-13\npw-13\npaa-13\nph-13\npai-13\npa-13\npnp-13\npm-13\npo-13\nef-13\ne-13\ner-13\nefr-13\nlf-13\nimputed-13\nsubbed-13\np-14\npr-14\nroh-14\npro-14\nmgr-14\nmhi-14\nmpv-14\nrb-14\npw-14\npaa-14\nph-14\npai-14\npa-14\npnp-14\npm-14\npo-14\nef-14\ne-14\ner-14\nefr-14\nlf-14\nimputed-14\nsubbed-14\np-15\npr-15\nroh-15\npro-15\nmgr-15\nmhi-15\nmpv-15\nrb-15\npw-15\npaa-15\nph-15\npai-15\npa-15\npnp-15\npm-15\npo-15\nef-15\ne-15\ner-15\nefr-15\nlf-15\nimputed-15\nsubbed-15\np-16\npr-16\nroh-16\npro-16\nmgr-16\nmhi-16\nmpv-16\nrb-16\npw-16\npaa-16\nph-16\npai-16\npa-16\npnp-16\npm-16\npo-16\nef-16\ne-16\ner-16\nefr-16\nlf-16\nimputed-16\nsubbed-16\ngeometry\n\n\n\n\n435\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n25.0\n21.0\n1.51\n1.80\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.70\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.70\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.00\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.00\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.00\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.00\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.00\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.00\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\n\n\n436\n42101000200\n-75.1631\n39.9529\n-75.1511\n39.9578\n2\nPhiladelphia County, Pennsylvania\n1362.00\n56.42\n374.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1362.00\n56.42\n415.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1362.00\n56.42\n455.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n4.0\n4.0\n0.88\n0.88\n1.0\n0.0\n0.0\n1362.00\n56.42\n496.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n3.0\n3.0\n0.60\n0.60\n1.0\n0.0\n0.0\n1362.00\n56.42\n537.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n6.0\n6.0\n1.12\n1.12\n1.0\n0.0\n0.0\n1633.00\n3.45\n578.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n1.0\n0.0\n0.00\n0.17\n1.0\n0.0\n0.0\n1633.00\n3.45\n618.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n6.0\n6.0\n0.97\n0.97\n1.0\n0.0\n0.0\n1633.00\n3.45\n659.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n9.0\n7.0\n1.06\n1.37\n0.0\n0.0\n1.0\n1633.00\n3.45\n700.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n11.0\n7.0\n1.00\n1.57\n0.0\n0.0\n1.0\n1633.00\n3.45\n740.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n6.0\n5.0\n0.68\n0.81\n0.0\n0.0\n1.0\n2937.0\n5.07\n781.0\n68.21\n905.0\n49928.0\n261100.0\n26.4\n22.64\n9.67\n2.69\n0.10\n63.16\n0.03\n1.40\n0.31\n6.0\n1.0\n0.13\n0.77\n0.0\n0.0\n1.0\n2331.0\n15.78\n792.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n9.0\n6.0\n0.76\n1.14\n0.0\n0.0\n1.0\n2331.0\n15.78\n802.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n8.0\n3.0\n0.37\n1.00\n0.0\n0.0\n1.0\n2331.0\n15.78\n813.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n14.0\n10.0\n1.23\n1.72\n0.0\n0.0\n1.0\n2331.0\n15.78\n824.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n5.0\n3.0\n0.36\n0.61\n0.0\n0.0\n1.0\n2331.0\n15.78\n834.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n10.0\n9.0\n1.08\n1.20\n0.0\n0.0\n1.0\n2331.0\n15.78\n845.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n11.0\n8.0\n0.95\n1.30\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\n\n\n437\n42101000300\n-75.1798\n39.9544\n-75.1623\n39.9599\n3\nPhiladelphia County, Pennsylvania\n2570.00\n12.16\n861.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2570.00\n12.16\n915.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2570.00\n12.16\n969.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n14.0\n12.0\n1.24\n1.44\n1.0\n0.0\n0.0\n2570.00\n12.16\n1023.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n21.0\n17.0\n1.66\n2.05\n1.0\n0.0\n0.0\n2570.00\n12.16\n1077.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n23.0\n23.0\n2.13\n2.13\n1.0\n0.0\n0.0\n4497.00\n1.63\n1132.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n12.0\n10.0\n0.88\n1.06\n1.0\n0.0\n0.0\n4497.00\n1.63\n1186.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n19.0\n16.0\n1.35\n1.60\n1.0\n0.0\n0.0\n4497.00\n1.63\n1240.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n21.0\n7.0\n0.56\n1.69\n0.0\n0.0\n1.0\n4497.00\n1.63\n1294.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n25.0\n11.0\n0.85\n1.93\n0.0\n0.0\n1.0\n4497.00\n1.63\n1348.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n27.0\n12.0\n0.89\n2.00\n0.0\n0.0\n1.0\n3169.0\n7.20\n1402.0\n75.58\n1827.0\n71250.0\n451800.0\n28.0\n72.26\n10.22\n4.26\n0.03\n10.35\n0.03\n2.52\n0.32\n24.0\n13.0\n0.93\n1.71\n0.0\n0.0\n1.0\n3405.0\n4.17\n1489.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n21.0\n8.0\n0.54\n1.41\n0.0\n0.0\n1.0\n3405.0\n4.17\n1575.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n27.0\n12.0\n0.76\n1.71\n0.0\n0.0\n1.0\n3405.0\n4.17\n1662.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n31.0\n10.0\n0.60\n1.87\n0.0\n0.0\n1.0\n3405.0\n4.17\n1749.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n27.0\n14.0\n0.80\n1.54\n0.0\n0.0\n1.0\n3405.0\n4.17\n1835.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n18.0\n5.0\n0.27\n0.98\n0.0\n0.0\n1.0\n3405.0\n4.17\n1922.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n26.0\n14.0\n0.73\n1.35\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\n\n\n438\n42101000801\n-75.1833\n39.9486\n-75.1773\n39.9515\n8.01\nPhiladelphia County, Pennsylvania\n1478.00\n14.40\n810.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1478.00\n14.40\n801.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1478.00\n14.40\n793.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n7.0\n5.0\n0.63\n0.88\n1.0\n0.0\n0.0\n1478.00\n14.40\n784.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n19.0\n13.0\n1.66\n2.42\n1.0\n0.0\n0.0\n1478.00\n14.40\n775.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n17.0\n14.0\n1.81\n2.19\n1.0\n0.0\n0.0\n1344.37\n11.10\n767.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n10.0\n6.0\n0.78\n1.30\n1.0\n0.0\n0.0\n1344.37\n11.10\n758.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n12.0\n7.0\n0.92\n1.58\n1.0\n0.0\n0.0\n1344.37\n11.10\n749.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n12.0\n5.0\n0.67\n1.60\n0.0\n0.0\n1.0\n1344.37\n11.10\n740.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n11.0\n4.0\n0.54\n1.49\n0.0\n0.0\n1.0\n1344.37\n11.10\n732.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n10.0\n2.0\n0.27\n1.37\n0.0\n0.0\n1.0\n1562.0\n2.46\n723.0\n71.09\n2001.0\n83125.0\n459900.0\n25.9\n78.04\n2.94\n5.76\n0.00\n10.82\n0.26\n1.92\n0.26\n14.0\n4.0\n0.55\n1.94\n0.0\n0.0\n1.0\n1692.0\n3.25\n734.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n13.0\n7.0\n0.95\n1.77\n0.0\n0.0\n1.0\n1692.0\n3.25\n746.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n7.0\n0.0\n0.00\n0.94\n0.0\n0.0\n1.0\n1692.0\n3.25\n757.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n15.0\n3.0\n0.40\n1.98\n0.0\n0.0\n1.0\n1692.0\n3.25\n768.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n10.0\n4.0\n0.52\n1.30\n0.0\n0.0\n1.0\n1692.0\n3.25\n780.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n16.0\n8.0\n1.03\n2.05\n0.0\n0.0\n1.0\n1692.0\n3.25\n791.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n13.0\n4.0\n0.51\n1.64\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\n\n\n439\n42101000804\n-75.1712\n39.9470\n-75.1643\n39.9501\n8.04\nPhiladelphia County, Pennsylvania\n3301.00\n14.40\n2058.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n3301.00\n14.40\n2050.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n3301.00\n14.40\n2042.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n22.0\n18.0\n0.88\n1.08\n1.0\n0.0\n0.0\n3301.00\n14.40\n2033.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n31.0\n21.0\n1.03\n1.52\n1.0\n0.0\n0.0\n3301.00\n14.40\n2025.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n18.0\n15.0\n0.74\n0.89\n1.0\n0.0\n0.0\n3002.54\n11.10\n2017.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n28.0\n19.0\n0.94\n1.39\n1.0\n0.0\n0.0\n3002.54\n11.10\n2009.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n14.0\n13.0\n0.65\n0.70\n1.0\n0.0\n0.0\n3002.54\n11.10\n2001.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n33.0\n11.0\n0.55\n1.65\n0.0\n0.0\n1.0\n3002.54\n11.10\n1992.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n17.0\n4.0\n0.20\n0.85\n0.0\n0.0\n1.0\n3002.54\n11.10\n1984.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n27.0\n8.0\n0.40\n1.36\n0.0\n0.0\n1.0\n3609.0\n7.69\n1976.0\n76.32\n1562.0\n75357.0\n330200.0\n26.0\n78.55\n2.72\n4.96\n0.03\n11.75\n0.03\n1.72\n0.25\n43.0\n13.0\n0.66\n2.18\n0.0\n0.0\n1.0\n3746.0\n0.00\n2000.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n38.0\n9.0\n0.45\n1.90\n0.0\n0.0\n1.0\n3746.0\n0.00\n2024.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n31.0\n16.0\n0.79\n1.53\n0.0\n0.0\n1.0\n3746.0\n0.00\n2048.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n27.0\n8.0\n0.39\n1.32\n0.0\n0.0\n1.0\n3746.0\n0.00\n2072.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n28.0\n11.0\n0.53\n1.35\n0.0\n0.0\n1.0\n3746.0\n0.00\n2096.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n18.0\n7.0\n0.33\n0.86\n0.0\n0.0\n1.0\n3746.0\n0.00\n2120.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n22.0\n7.0\n0.33\n1.04\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\n\n\n\n\n\n\n\n\n\n1.1.3 Transform from wide to tidy format\nFor this assignment, we are interested in the number of evictions by census tract for various years. Right now, each year has it’s own column, so it will be easiest to transform to a tidy format.\nUse the pd.melt() function to transform the eviction data into tidy format, using the number of evictions from 2003 to 2016.\nThe tidy data frame should have four columns: GEOID, geometry, a column holding the number of evictions, and a column telling you what the name of the original column was for that value.\nHints: - You’ll want to specify the GEOID and geometry columns as the id_vars. This will keep track of the census tract information. - You should specify the names of the columns holding the number of evictions as the value_vars. - You can generate a list of this column names using Python’s f-string formatting: python     value_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\n\nvalue_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\ntidytracts = tracts_filtered.melt(id_vars=[\"GEOID\", \"geometry\"], value_vars=value_vars)\ntidytracts.rename(columns={\"variable\": \"year\", \"value\": \"evictions\"}, inplace=True)\ntidytracts\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n...\n...\n...\n...\n...\n\n\n5371\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n5372\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n5373\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n5374\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n5375\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n5376 rows × 4 columns\n\n\n\n\ntidytracts.head()\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n\n\n\n\n\n\n\n1.1.4 Plot the total number of evictions per year from 2003 to 2016\nUse hvplot to plot the total number of evictions from 2003 to 2016. You will first need to perform a group by operation and sum up the total number of evictions for all census tracts, and then use hvplot() to make your plot.\nYou can use any type of hvplot chart you’d like to show the trend in number of evictions over time.\n\nEvicYears = tidytracts.groupby('year')['evictions'].sum()\n\n\nEvicYears = pd.DataFrame(EvicYears).reset_index()\nEvicYears\n\n\n\n\n\n\n\n\nyear\nevictions\n\n\n\n\n0\ne-03\n10647.0\n\n\n1\ne-04\n10491.0\n\n\n2\ne-05\n10550.0\n\n\n3\ne-06\n11078.0\n\n\n4\ne-07\n11032.0\n\n\n5\ne-08\n10866.0\n\n\n6\ne-09\n9821.0\n\n\n7\ne-10\n10628.0\n\n\n8\ne-11\n10882.0\n\n\n9\ne-12\n11130.0\n\n\n10\ne-13\n10803.0\n\n\n11\ne-14\n11182.0\n\n\n12\ne-15\n10098.0\n\n\n13\ne-16\n10264.0\n\n\n\n\n\n\n\n\nEvicYears.hvplot.line(\n           x='year', \n           y='evictions', \n           title='Total Number of Evictions in Philadelphia, 2003-2016'\n          ) \n\n\n\n\n\n  \n\n\n\n\n\n\n1.1.5 The number of evictions across Philadelphia\nOur tidy data frame is still a GeoDataFrame with a geometry column, so we can visualize the number of evictions for all census tracts.\nUse hvplot() to generate a choropleth showing the number of evictions for a specified year, with a widget dropdown to select a given year (or variable name, e.g., e-16, e-15, etc).\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select between them. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\ntidytracts\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n...\n...\n...\n...\n...\n\n\n5371\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n5372\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n5373\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n5374\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n5375\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n5376 rows × 4 columns\n\n\n\n\ntidytracts.hvplot(\n    values=\"evictions\",\n    hover_cols=[\"evictions\", \"year\"],\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    hover_fill_color=\"white\",\n    title=\"Evictions in Philadelphia by Year\",\n    colormap='winter'\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []"
  },
  {
    "objectID": "analysis/evictions.html#code-violations-in-philadelphia",
    "href": "analysis/evictions.html#code-violations-in-philadelphia",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.2 Code Violations in Philadelphia",
    "text": "1.2 Code Violations in Philadelphia\nNext, we’ll explore data for code violations from the Licenses and Inspections Department of Philadelphia to look for potential correlations with the number of evictions.\n\n1.2.1 Load data from 2012 to 2016\nL+I violation data for years including 2012 through 2016 (inclusive) is provided in a CSV format in the “data/” folder.\nLoad the data using pandas and convert to a GeoDataFrame.\n\nLIViolations = gpd.read_file(\"./Data/li_violations.csv\")\n\n\nLIViolations\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nNone\n\n\n1\n40.050593\n-75.126578\nLICENSE-CHANGE OF ADDRESS\nNone\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nNone\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nNone\n\n\n4\n40.02326\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nNone\n\n\n...\n...\n...\n...\n...\n\n\n434047\n40.012805\n-75.155963\nSD-REQD EXIST GROUP R\nNone\n\n\n434048\n40.009985\n-75.068968\nRUBBISH/GARBAGE EXTERIOR-OWNER\nNone\n\n\n434049\n40.009829\n-75.068912\nCLIP VIOLATION NOTICE\nNone\n\n\n434050\n40.009776\n-75.068895\nPERSONAL PROPERTY EXT OWNER\nNone\n\n\n434051\n40.009776\n-75.068895\nLICENSE - RENTAL PROPERTY\nNone\n\n\n\n\n434052 rows × 4 columns\n\n\n\n\nLIVioGDF = gpd.GeoDataFrame(LIViolations, geometry=gpd.points_from_xy(LIViolations.lng, LIViolations.lat), crs=\"EPSG:4326\")\n\n\nLIVioGDF\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nPOINT (-75.12608 40.05053)\n\n\n1\n40.050593\n-75.126578\nLICENSE-CHANGE OF ADDRESS\nPOINT (-75.12658 40.05059)\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nPOINT (-75.12889 39.99199)\n\n\n4\n40.02326\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-75.16485 40.02326)\n\n\n...\n...\n...\n...\n...\n\n\n434047\n40.012805\n-75.155963\nSD-REQD EXIST GROUP R\nPOINT (-75.15596 40.01281)\n\n\n434048\n40.009985\n-75.068968\nRUBBISH/GARBAGE EXTERIOR-OWNER\nPOINT (-75.06897 40.00999)\n\n\n434049\n40.009829\n-75.068912\nCLIP VIOLATION NOTICE\nPOINT (-75.06891 40.00983)\n\n\n434050\n40.009776\n-75.068895\nPERSONAL PROPERTY EXT OWNER\nPOINT (-75.06889 40.00978)\n\n\n434051\n40.009776\n-75.068895\nLICENSE - RENTAL PROPERTY\nPOINT (-75.06889 40.00978)\n\n\n\n\n434052 rows × 4 columns\n\n\n\n\n\n1.2.2 Trim to specific violation types\nThere are many different types of code violations (running the nunique() function on the violationdescription column will extract all of the unique ones). More information on different types of violations can be found on the City’s website.\nBelow, I’ve selected 15 types of violations that deal with property maintenance and licensing issues. We’ll focus on these violations. The goal is to see if these kinds of violations are correlated spatially with the number of evictions in a given area.\nUse the list of violations given to trim your data set to only include these types.\n\nviolation_types = [\n    \"INT-PLMBG MAINT FIXTURES-RES\",\n    \"INT S-CEILING REPAIR/MAINT SAN\",\n    \"PLUMBING SYSTEMS-GENERAL\",\n    \"CO DETECTOR NEEDED\",\n    \"INTERIOR SURFACES\",\n    \"EXT S-ROOF REPAIR\",\n    \"ELEC-RECEPTABLE DEFECTIVE-RES\",\n    \"INT S-FLOOR REPAIR\",\n    \"DRAINAGE-MAIN DRAIN REPAIR-RES\",\n    \"DRAINAGE-DOWNSPOUT REPR/REPLC\",\n    \"LIGHT FIXTURE DEFECTIVE-RES\",\n    \"LICENSE-RES SFD/2FD\",\n    \"ELECTRICAL -HAZARD\",\n    \"VACANT PROPERTIES-GENERAL\",\n    \"INT-PLMBG FIXTURES-RES\",\n]\n\n\nviolations = LIVioGDF[LIVioGDF[\"violationdescription\"].isin(violation_types)]\n\n\n\n1.2.3 Make a hex bin map\nThe code violation data is point data. We can get a quick look at the geographic distribution using matplotlib and the hexbin() function. Make a hex bin map of the code violations and overlay the census tract outlines.\nHints: - The eviction data from part 1 was by census tract, so the census tract geometries are available as part of that GeoDataFrame. You can use it to overlay the census tracts on your hex bin map. - Make sure you convert your GeoDataFrame to a CRS that’s better for visualization than plain old 4326.\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.hexbin(\n    violations[\"geometry\"].x,\n    violations[\"geometry\"].y,\n    gridsize=100,\n    cmap=\"plasma\",\n    alpha=1,\n    edgecolors=\"none\",\n    mincnt=1,\n)\n\nax.set_xlabel(\"Latitude\")\nax.set_ylabel(\"Longitude\")\nax.set_title(\"Hex Bin Map of Code Violations in Philadelphia\")\n\n\nplt.show()\n\n\n\n\n\n\n1.2.4 Spatially join data sets\nTo do a census tract comparison to our eviction data, we need to find which census tract each of the code violations falls into. Use the geopandas.sjoin() function to do just that.\nHints - You can re-use your eviction data frame, but you will only need the geometry column (specifying census tract polygons) and the GEOID column (specifying the name of each census tract). - Make sure both data frames have the same CRS before joining them together!\n\ntidytractsNoDup = tidytracts.drop_duplicates(\"GEOID\")\ntidytractsNoDup\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n...\n...\n...\n...\n...\n\n\n379\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-03\n105.0\n\n\n380\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-03\n45.0\n\n\n381\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-03\n21.0\n\n\n382\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-03\n6.0\n\n\n383\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\n\n\n\n\n384 rows × 4 columns\n\n\n\n\nviolations.head()\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n\n\n25\n40.022406\n-75.121872\nEXT S-ROOF REPAIR\nPOINT (-75.12187 40.02241)\n\n\n30\n40.023237\n-75.121726\nCO DETECTOR NEEDED\nPOINT (-75.12173 40.02324)\n\n\n31\n40.023397\n-75.122241\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.12224 40.02340)\n\n\n34\n40.023773\n-75.121603\nINT S-FLOOR REPAIR\nPOINT (-75.12160 40.02377)\n\n\n\n\n\n\n\n\ntidytractsNoDup = tidytractsNoDup.to_crs(\"EPSG:4326\")\nviolations = violations.to_crs(\"EPSG:4326\") \njoined_data = gpd.sjoin(violations, tidytractsNoDup)\njoined_data\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\nindex_right\nGEOID\nyear\nevictions\n\n\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n364\n42101027100\ne-03\n6.0\n\n\n16024\n40.045785\n-75.127928\nCO DETECTOR NEEDED\nPOINT (-75.12793 40.04579)\n364\n42101027100\ne-03\n6.0\n\n\n17225\n40.045634\n-75.126469\nLICENSE-RES SFD/2FD\nPOINT (-75.12647 40.04563)\n364\n42101027100\ne-03\n6.0\n\n\n43436\n40.045738\n-75.125851\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.12585 40.04574)\n364\n42101027100\ne-03\n6.0\n\n\n43452\n40.045779\n-75.125843\nEXT S-ROOF REPAIR\nPOINT (-75.12584 40.04578)\n364\n42101027100\ne-03\n6.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n219512\n40.101835\n-75.045433\nDRAINAGE-DOWNSPOUT REPR/REPLC\nPOINT (-75.04543 40.10184)\n136\n42101035602\ne-03\n7.0\n\n\n224788\n40.004739\n-75.206722\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.20672 40.00474)\n283\n42101012201\ne-03\n28.0\n\n\n224790\n40.004739\n-75.206722\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.20672 40.00474)\n283\n42101012201\ne-03\n28.0\n\n\n240195\n40.013165\n-75.142804\nLICENSE-RES SFD/2FD\nPOINT (-75.14280 40.01317)\n171\n42101980500\ne-03\n0.0\n\n\n310973\n40.034424\n-75.017732\nDRAINAGE-DOWNSPOUT REPR/REPLC\nPOINT (-75.01773 40.03442)\n175\n42101989100\ne-03\n0.0\n\n\n\n\n34108 rows × 8 columns\n\n\n\n\n\n1.2.5 Calculate the number of violations by type per census tract\nNext, we’ll want to find the number of violations (for each kind) per census tract. You should group the data frame by violation type and census tract name.\nThe result of this step should be a data frame with three columns: violationdescription, GEOID, and N, where N is the number of violations of that kind in the specified census tract.\nOptional: to make prettier plots\nSome census tracts won’t have any violations, and they won’t be included when we do the above calculation. However, there is a trick to set the values for those census tracts to be zero. After you calculate the sizes of each violation/census tract group, you can run:\nN = N.unstack(fill_value=0).stack().reset_index(name='N')\nwhere N gives the total size of each of the groups, specified by violation type and census tract name.\nSee this StackOverflow post for more details.\nThis part is optional, but will make the resulting maps a bit prettier.\n\n#N = N.unstack(fill_value=0).stack().reset_index(name='N')\nvioldesc = joined_data.groupby([\"violationdescription\", \"GEOID\"]).size().reset_index(name=\"N\")\nvioldesc\n\n\n\n\n\n\n\n\nviolationdescription\nGEOID\nN\n\n\n\n\n0\nCO DETECTOR NEEDED\n42101000401\n1\n\n\n1\nCO DETECTOR NEEDED\n42101000402\n1\n\n\n2\nCO DETECTOR NEEDED\n42101000700\n1\n\n\n3\nCO DETECTOR NEEDED\n42101000803\n1\n\n\n4\nCO DETECTOR NEEDED\n42101000804\n1\n\n\n...\n...\n...\n...\n\n\n3995\nVACANT PROPERTIES-GENERAL\n42101036501\n1\n\n\n3996\nVACANT PROPERTIES-GENERAL\n42101037900\n1\n\n\n3997\nVACANT PROPERTIES-GENERAL\n42101038000\n1\n\n\n3998\nVACANT PROPERTIES-GENERAL\n42101038200\n2\n\n\n3999\nVACANT PROPERTIES-GENERAL\n42101038300\n3\n\n\n\n\n4000 rows × 3 columns\n\n\n\n\n\n1.2.6 Merge with census tracts geometries\nWe now have the number of violations of different types per census tract specified as a regular DataFrame. You can now merge it with the census tract geometries (from your eviction data GeoDataFrame) to create a GeoDataFrame.\nHints - Use pandas.merge() and specify the on keyword to be the column holding census tract names. - Make sure the result of the merge operation is a GeoDataFrame — you will want the GeoDataFrame holding census tract geometries to be the first argument of the pandas.merge() function.\n\nmergedN_evics = tidytractsNoDup.merge(violdesc,  on=\"GEOID\")\nmergedN_evics\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\nviolationdescription\nN\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n6\n\n\n1\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nELECTRICAL -HAZARD\n1\n\n\n2\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nEXT S-ROOF REPAIR\n3\n\n\n3\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nINT S-CEILING REPAIR/MAINT SAN\n9\n\n\n4\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nINT S-FLOOR REPAIR\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3995\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nINT S-CEILING REPAIR/MAINT SAN\n4\n\n\n3996\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nINT S-FLOOR REPAIR\n2\n\n\n3997\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nINTERIOR SURFACES\n2\n\n\n3998\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nLICENSE-RES SFD/2FD\n11\n\n\n3999\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nPLUMBING SYSTEMS-GENERAL\n2\n\n\n\n\n4000 rows × 6 columns\n\n\n\n\n\n1.2.7 Interactive choropleths for each violation type\nNow, we can use hvplot() to create an interactive choropleth for each violation type and add a widget to specify different violation types.\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select different violation types. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\nmergedN_evics.hvplot(\n    geo = True,\n    groupby = \"violationdescription\",\n    c = \"N\",\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    title=\"Evictions in Philadelphia by Year\",\n    colormap=\"bmw\"\n)"
  },
  {
    "objectID": "analysis/evictions.html#a-side-by-side-comparison",
    "href": "analysis/evictions.html#a-side-by-side-comparison",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.3. A side-by-side comparison",
    "text": "1.3. A side-by-side comparison\nFrom the interactive maps of evictions and violations, you should notice a lot of spatial overlap.\nAs a final step, we’ll make a side-by-side comparison to better show the spatial correlations. This will involve a few steps:\n\nTrim the evictions data frame plotted in section 1.1.5 to only include evictions from 2016.\nTrim the L+I violations data frame plotted in section 1.2.7 to only include a single violation type (pick whichever one you want!).\nUse hvplot() to make two interactive choropleth maps, one for the data from step 1. and one for the data in step 2.\nShow these two plots side by side (one row and 2 columns) using the syntax for combining charts.\n\nNote: since we selected a single year and violation type, you won’t need to use the groupby= keyword here.\n\nevics2016 = tidytracts[tidytracts[\"year\"] == \"e-16\"]\nevics2016\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n4992\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-16\n16.0\n\n\n4993\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-16\n8.0\n\n\n4994\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-16\n14.0\n\n\n4995\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-16\n4.0\n\n\n4996\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-16\n7.0\n\n\n...\n...\n...\n...\n...\n\n\n5371\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n5372\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n5373\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n5374\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n5375\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n384 rows × 4 columns\n\n\n\n\nevics2016drain = mergedN_evics[mergedN_evics[\"violationdescription\"] == \"DRAINAGE-DOWNSPOUT REPR/REPLC\"]\nevics2016drain\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\nviolationdescription\nN\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n6\n\n\n8\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n1\n\n\n14\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n2\n\n\n29\n42101001002\nMULTIPOLYGON (((-75.14919 39.94903, -75.14602 ...\ne-03\n16.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n3\n\n\n38\n42101001400\nMULTIPOLYGON (((-75.16558 39.94366, -75.16567 ...\ne-03\n30.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3946\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-03\n105.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n14\n\n\n3961\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-03\n45.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n21\n\n\n3975\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-03\n21.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n4\n\n\n3985\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-03\n6.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n3\n\n\n3993\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-03\n1.0\nDRAINAGE-DOWNSPOUT REPR/REPLC\n5\n\n\n\n\n311 rows × 6 columns\n\n\n\n\np1 = evics2016drain.hvplot(\n    values=\"N\",\n    hover_cols=[\"N\"],\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    hover_color=\"yellow\",\n    title=\"Reported Drainage Damage in Households 2016\",\n    colormap='Greens'\n)\n\np2 = evics2016.hvplot(\n    values=\"evictions\",\n    hover_cols=\"evictions\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    title=\"Evictions in Philadelphia 2016\",\n)\n\np1 + p2\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []"
  },
  {
    "objectID": "analysis/evictions.html#extra-credit",
    "href": "analysis/evictions.html#extra-credit",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "1.4. Extra Credit",
    "text": "1.4. Extra Credit\nIdentify the 20 most common types of violations within the time period of 2012 to 2016 and create a set of interactive choropleths similar to what was done in section 1.2.7.\nUse this set of maps to identify 3 types of violations that don’t seem to have much spatial overlap with the number of evictions in the City.\n\n#tracts12to16 = tidytracts[tidytracts[\"year\"].astype(str).isin[f\"e-{x:02d}\" for x in range(12, 17)]]\n\n#tidytracts12to16 = tidytracts[tidytracts[\"year\"] == \"e-16\" or tidytracts[\"year\"] == \"e-15\" or tidytracts[\"year\"] == \"e-14\" or tidytracts[\"year\"] == \"e-13\" or tidytracts[\"year\"] == \"e-13\" or tidytracts[\"year\"] == \"e-12\"]\n\n\nvalue_vars = [f\"e-{x:02d}\" for x in range(12, 17)]\ntidytracts12to16 = tracts_filtered.melt(id_vars=[\"GEOID\", \"geometry\"], value_vars=value_vars)\ntidytracts12to16.rename(columns={\"variable\": \"year\", \"value\": \"evictions\"}, inplace=True)\ntidytracts12to16\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-12\n7.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-12\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-12\n12.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-12\n0.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-12\n16.0\n\n\n...\n...\n...\n...\n...\n\n\n1915\n42101017800\nMULTIPOLYGON (((-75.11339 39.99649, -75.11137 ...\ne-16\n104.0\n\n\n1916\n42101017900\nMULTIPOLYGON (((-75.10591 39.98804, -75.10836 ...\ne-16\n80.0\n\n\n1917\n42101018002\nMULTIPOLYGON (((-75.10506 39.98707, -75.10437 ...\ne-16\n32.0\n\n\n1918\n42101018300\nMULTIPOLYGON (((-75.06581 39.98629, -75.06859 ...\ne-16\n7.0\n\n\n1919\n42101018400\nMULTIPOLYGON (((-75.05902 39.99251, -75.05954 ...\ne-16\n2.0\n\n\n\n\n1920 rows × 4 columns\n\n\n\n\ngrouped_evics = mergedN_evics.groupby(\"violationdescription\").size().reset_index(name=\"N\")\ntidyevics = grouped_evics.sort_values(by=[\"N\"], ascending=False)\ntop20_violations = tidyevics.iloc[:20, 0]              \n\n\ntop20_violations\n\n6     INT S-CEILING REPAIR/MAINT SAN\n11               LICENSE-RES SFD/2FD\n5                  EXT S-ROOF REPAIR\n0                 CO DETECTOR NEEDED\n1      DRAINAGE-DOWNSPOUT REPR/REPLC\n7                 INT S-FLOOR REPAIR\n2     DRAINAGE-MAIN DRAIN REPAIR-RES\n8             INT-PLMBG FIXTURES-RES\n9       INT-PLMBG MAINT FIXTURES-RES\n12       LIGHT FIXTURE DEFECTIVE-RES\n3      ELEC-RECEPTABLE DEFECTIVE-RES\n10                 INTERIOR SURFACES\n13          PLUMBING SYSTEMS-GENERAL\n4                 ELECTRICAL -HAZARD\n14         VACANT PROPERTIES-GENERAL\nName: violationdescription, dtype: object\n\n\n\nCelingVio2016 = mergedN_evics[mergedN_evics[\"violationdescription\"] == \"INT S-CEILING REPAIR/MAINT SAN\"]\n\n\nCOVio2016= mergedN_evics[mergedN_evics[\"violationdescription\"] == \"CO DETECTOR NEEDED\"]\n\n\nElectricVio2016 = mergedN_evics[mergedN_evics[\"violationdescription\"] == \"ELECTRICAL -HAZARD\"]\n\n\nElectricVio2016.hvplot(\n    values=\"N\",\n    hover_cols=\"N\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    colormap=\"bmw\",\n    title=\"Electrical Hazards 2016\",\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []\n\n\n\n\n\n\n  \n\n\n\n\n\nCOVio2016.hvplot(\n    values=\"N\",\n    hover_cols=\"N\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    colormap=\"coolwarm\",\n    title=\"Carbon Monoxide Detectors Needed in 2016\",\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []\n\n\n\n\n\n\n  \n\n\n\n\n\nCelingVio2016.hvplot(\n    values=\"N\",\n    hover_cols=\"N\",\n    dynamic=False,\n    width=700,\n    height=700,\n    hover_color=\"yellow\",\n    colormap=\"fire\",\n    title=\"Ceiling Repair Violations 2016\",\n)\n\nWARNING:param.main: values option not found for polygons plot with bokeh; similar options include: []"
  },
  {
    "objectID": "analysis/evictions.html#part-2-exploring-the-ndvi-in-philadelphia",
    "href": "analysis/evictions.html#part-2-exploring-the-ndvi-in-philadelphia",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "Part 2: Exploring the NDVI in Philadelphia",
    "text": "Part 2: Exploring the NDVI in Philadelphia\nIn this part, we’ll explore the NDVI in Philadelphia a bit more. This part will include two parts:\n\nWe’ll compare the median NDVI within the city limits and the immediate suburbs\nWe’ll calculate the NDVI around street trees in the city."
  },
  {
    "objectID": "analysis/evictions.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "href": "analysis/evictions.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "2.1 Comparing the NDVI in the city and the suburbs",
    "text": "2.1 Comparing the NDVI in the city and the suburbs\n\n2.1.1 Load Landsat data for Philadelphia\nUse rasterio to load the landsat data for Philadelphia (available in the “data/” folder)\n\nlandsat = rio.open(\"./Data/landsat8_philly.tif\")\nlandsat\n\n&lt;open DatasetReader name='./Data/landsat8_philly.tif' mode='r'&gt;\n\n\n\n\n2.1.2 Separating the city from the suburbs\nCreate two polygon objects, one for the city limits and one for the suburbs. To calculate the suburbs polygon, we will take everything outside the city limits but still within the bounding box.\n\nThe city limits are available in the “data/” folder.\nTo calculate the suburbs polygon, the “envelope” attribute of the city limits geometry will be useful.\nYou can use geopandas’ geometric manipulation functionality to calculate the suburbs polygon from the city limits polygon and the envelope polygon.\n\n\nenvelope = CityLimitsdf.geometry.envelope\nsuburbs = envelope - CityLimitsdf.geometry\nsuburbs\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_7664\\3105865041.py:2: FutureWarning: '-' operator will be deprecated. Use the 'difference' method instead.\n  suburbs = envelope - CityLimitsdf.geometry\n\n\n0    MULTIPOLYGON (((-75.28031 39.86747, -75.28031 ...\ndtype: geometry\n\n\n\nprint(landsat.crs.to_epsg())\n\n32618\n\n\n\nCityLimitsdf = CityLimitsdf.to_crs(32618)\nsuburbs = suburbs.to_crs(32618)\n\n\nsuburbs.squeeze()\n\n\n\n\n\n\n2.1.3 Mask and calculate the NDVI for the city and the suburbs\nUsing the two polygons from the last section, use rasterio’s mask functionality to create two masked arrays from the landsat data, one for the city and one for the suburbs.\nFor each masked array, calculate the NDVI.\n\nfrom rasterio.mask import mask\n\n\nmaskedCity, mask_transformCity = mask(\n    dataset=landsat,            \n    shapes=CityLimitsdf.geometry,  \n    crop=True,                    \n    all_touched=True,             \n    filled=False,                 \n)\nmaskedSub, mask_transform = mask(\n    dataset=landsat,            \n    shapes=suburbs.geometry,  \n    crop=True,                    \n    all_touched=True,             \n    filled=False,                 \n)\n    \n\n\nredC = maskedCity[3]\nnirC = maskedCity[4]\nredS = maskedSub[3]\nnirS = maskedSub[4]\n\n\ndef calculate_NDVI(nir, red):\n    nir = nir.astype(float)\n    red = red.astype(float)\n\n    check = np.logical_and(red.mask == False, nir.mask == False)\n\n    ndvi = np.where(check, (nir - red) / (nir + red), np.nan)\n\n    return ndvi\n\nNDVI_C = calculate_NDVI(nirC, redC)\nNDVI_S = calculate_NDVI(nirS, redS)\n\n\n\n2.1.4 Calculate the median NDVI within the city and within the suburbs\n\nCalculate the median value from your NDVI arrays for the city and suburbs\nNumpy’s nanmedian function will be useful for ignoring NaN elements\nPrint out the median values. Which has a higher NDVI: the city or suburbs?\n\n\ncity_ndvi_median = np.nanmedian(NDVI_C)\nsuburbs_ndvi_median = np.nanmedian(NDVI_S)\n\nprint(f\"Median NDVI for the city: {city_ndvi_median}\")\nprint(f\"Median NDVI for the suburbs: {suburbs_ndvi_median}\")\n\nMedian NDVI for the city: 0.20268593532493442\nMedian NDVI for the suburbs: 0.37466958688920776"
  },
  {
    "objectID": "analysis/evictions.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "href": "analysis/evictions.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "title": "Exploring Evictions and Code Violations in Philadelphia",
    "section": "2.2 Calculating the NDVI for Philadelphia’s street treets",
    "text": "2.2 Calculating the NDVI for Philadelphia’s street treets\n\n2.2.1 Load the street tree data\nThe data is available in the “data/” folder. It has been downloaded from OpenDataPhilly. It contains the locations of abot 2,500 street trees in Philadelphia.\n\npprTreePoints\n\n\n\n\n\n\n\n\nobjectid\nfcode\ngeometry\n\n\n\n\n0\n1\n3000\nPOINT (-75.00538 40.06254)\n\n\n1\n2\n3000\nPOINT (-75.12959 39.96692)\n\n\n2\n3\n3000\nPOINT (-75.12838 39.98397)\n\n\n3\n4\n3000\nPOINT (-75.12892 39.98489)\n\n\n4\n5\n3000\nPOINT (-75.12948 39.97148)\n\n\n...\n...\n...\n...\n\n\n2475\n2476\n3000\nPOINT (-75.15779 39.97728)\n\n\n2476\n2477\n3000\nPOINT (-75.15773 39.97727)\n\n\n2477\n2478\n3000\nPOINT (-75.15768 39.97727)\n\n\n2478\n2479\n3000\nPOINT (-75.15763 39.97725)\n\n\n2479\n2480\n3000\nPOINT (-75.15757 39.97725)\n\n\n\n\n2480 rows × 3 columns\n\n\n\n\npprTreePoints = pprTreePoints.to_crs(32618)\n\n\n\n2.2.2 Calculate the NDVI values at the locations of the street trees\n\nUse the rasterstats package to calculate the NDVI values at the locations of the street trees.\nSince these are point geometries, you can calculate either the median or the mean statistic (only one pixel will contain each point).\n\n\nimport rasterstats \nfrom rasterstats import zonal_stats\n\n\nstats = zonal_stats(\n    pprTreePoints,  # The vector data\n    NDVI_C,  # The array holding the raster data\n    affine=landsat.transform,  # The affine transform for the raster data\n    stats=[\"mean\", \"median\"],  # The stats to compute\n    nodata=np.nan,  # Missing data representation\n)\n#stats\n\n\n\n2.2.3 Plotting the results\nMake two plots of the results:\n\nA histogram of the NDVI values, using matplotlib’s hist function. Include a vertical line that marks the NDVI = 0 threshold\nA plot of the street tree points, colored by the NDVI value, using geopandas’ plot function. Include the city limits boundary on your plot.\n\nThe figures should be clear and well-styled, with for example, labels for axes, legends, and clear color choices.\n\nmedian_stats = [stats_dict[\"median\"] for stats_dict in stats]\nmean_stats = [stats_dict[\"mean\"] for stats_dict in stats]\n\n\na2 = plt.subplots()\nplt.hist(mean_stats, bins=100, density=True)\nplt.axvline(0, linestyle=\"dashed\", color=\"gray\")\nplt.xlabel(\"mean\")\nplt.ylabel(\"median\")\nplt.title(\"Histogram of NDVI Values for Street Trees in Philadelphia\")\nplt.grid(True)\nplt.show()\n\n\na2\n\n\n\n\n(&lt;Figure size 640x480 with 1 Axes&gt;,\n &lt;Axes: title={'center': 'Histogram of NDVI Values for Street Trees in Philadelphia'}, xlabel='mean', ylabel='median'&gt;)\n\n\n\npprTreePoints[\"median_stats\"] = [stats_dict[\"median\"] for stats_dict in stats]\npprTreePoints.head()\n\n\n\n\n\n\n\n\nobjectid\nfcode\ngeometry\nmedian_stats\n\n\n\n\n0\n1\n3000\nPOINT (499541.269 4434698.265)\n0.235337\n\n\n1\n2\n3000\nPOINT (488932.471 4424093.158)\n0.261535\n\n\n2\n3\n3000\nPOINT (489039.214 4425985.827)\n0.096769\n\n\n3\n4\n3000\nPOINT (488993.171 4426088.005)\n0.076630\n\n\n4\n5\n3000\nPOINT (488943.113 4424599.478)\n0.267952\n\n\n\n\n\n\n\n\npprTreePoints.explore(column=\"median_stats\", cmap=\"magma\", tiles=\"Cartodb positron\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/assignment-5.html",
    "href": "analysis/assignment-5.html",
    "title": "Exploring Yelp Reviews in Philadelphia",
    "section": "",
    "text": "In this assignment, we’ll explore restaurant review data available through the Yelp Dataset Challenge. The dataset includes Yelp data for user reviews and business information for many metropolitan areas. I’ve already downloaded this dataset (8 GB total!) and extracted out the data files for reviews and restaurants in Philadelphia. I’ve placed these data files into the data directory in this repository.\nThis assignment is broken into two parts:\nPart 1: Analyzing correlations between restaurant reviews and census data\nWe’ll explore the relationship between restaurant reviews and the income levels of the restaurant’s surrounding area.\nPart 2: Exploring the impact of fast food restaurants\nWe’ll run a sentiment analysis on reviews of fast food restaurants and estimate income levels in neighborhoods with fast food restaurants. We’ll test how well our sentiment analysis works by comparing the number of stars to the sentiment of reviews.\nBackground readings - Does sentiment analysis work? - The Geography of Taste: Using Yelp to Study Urban Culture"
  },
  {
    "objectID": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "href": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "title": "Exploring Yelp Reviews in Philadelphia",
    "section": "1. Correlating restaurant ratings and income levels",
    "text": "1. Correlating restaurant ratings and income levels\nIn this part, we’ll use the census API to download household income data and explore how it correlates with restaurant review data."
  },
  {
    "objectID": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "href": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "title": "Exploring Yelp Reviews in Philadelphia",
    "section": "2. Fast food trends in Philadelphia",
    "text": "2. Fast food trends in Philadelphia\nAt the end of part 1, you should have seen a strong trend where higher income tracts generally had restaurants with better reviews. In this section, we’ll explore the impact of fast food restaurants and how they might be impacting this trend.\nHypothesis\n\nFast food restaurants are predominantly located in areas with lower median income levels.\nFast food restaurants have worse reviews compared to typical restaurants.\n\nIf true, these two hypotheses could help to explain the trend we found in part 1. Let’s dive in and test our hypotheses!\n\n2.1 Identify fast food restaurants\nThe “categories” column in our dataset contains multiple classifications for each restaurant. One such category is “Fast Food”. In this step, add a new column called “is_fast_food” that is True if the “categories” column contains the term “Fast Food” and False otherwise\n\nrestauranttract['is_fast_food']  = restauranttract['categories'].str.contains('Fast Food', case=False, na=False).astype(int)\nrestauranttract[restauranttract[\"is_fast_food\"] == 1][\"name\"].head(5)\n\n11                Wendy's\n22    Crown Fried Chicken\n33            Chick-fil-A\n34               Checkers\n37                    KFC\nName: name, dtype: object\n\n\n\nrestauranttract.head(2)\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\nindex_right\nSTATEFP\n...\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\nMedHHInc\nstate\ncounty\ntract\nis_fast_food\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15557 39.95550)\n113.0\n42\n...\nS\n386232.0\n0.0\n+39.9554162\n-075.1569255\n91067.0\n42\n101\n000200\n0\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n311.0\n42\n...\nS\n444406.0\n0.0\n+39.9547160\n-075.1465255\n91944.0\n42\n101\n000102\n0\n\n\n\n\n2 rows × 26 columns\n\n\n\n\n\n2.2 Calculate the median income for fast food and otherwise\nGroup by the “is_fast_food” column and calculate the median income for restaurants that are and are not fast food. You should find that income levels are lower in tracts with fast food.\nNote: this is just an estimate, since we are calculating a median of median income values.\n\nFFtable = median_income_by_fast_food = restauranttract.groupby(\"is_fast_food\")[\"MedHHInc\"].median().reset_index()\nFFtable\n\n\n\n\n\n\n\n\nis_fast_food\nMedHHInc\n\n\n\n\n0\n0\n75583.0\n\n\n1\n1\n53480.0\n\n\n\n\n\n\n\n\nrestauranttract.rename(columns={\"B19013_001E\": \"medianHHincomelevel\"})\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\nindex_right\nSTATEFP\n...\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\nMedHHInc\nstate\ncounty\ntract\nis_fast_food\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15557 39.95550)\n113.0\n42\n...\nS\n386232.0\n0.0\n+39.9554162\n-075.1569255\n91067.0\n42\n101\n000200\n0\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n311.0\n42\n...\nS\n444406.0\n0.0\n+39.9547160\n-075.1465255\n91944.0\n42\n101\n000102\n0\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\nPOINT (-75.16257 39.94322)\n67.0\n42\n...\nS\n239383.0\n0.0\n+39.9419037\n-075.1591158\n93203.0\n42\n101\n001500\n0\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Restaurants\nPOINT (-75.15745 39.93982)\n29.0\n42\n...\nS\n242440.0\n0.0\n+39.9400001\n-075.1593102\n99125.0\n42\n101\n001800\n0\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\nPOINT (-75.21832 40.02246)\n288.0\n42\n...\nS\n858218.0\n55718.0\n+40.0244730\n-075.2151045\n79264.0\n42\n101\n021000\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7345\nVZbkSeZtFynEascotq7ExA\n39.953391\n-75.196765\nAli Baba Magic Food\n8\n4.0\nRestaurants, Food Stands\nPOINT (-75.19677 39.95339)\n255.0\n42\n...\nS\n184674.0\n0.0\n+39.9539319\n-075.1984477\n36250.0\n42\n101\n008801\n0\n\n\n7346\ngPr1io7ks0Eo3FDsnDTYfg\n40.060414\n-75.191084\nTata Cafe\n21\n4.0\nSandwiches, Restaurants, Italian\nPOINT (-75.19109 40.06041)\n243.0\n42\n...\nS\n865284.0\n1982.0\n+40.0683411\n-075.1883025\n92404.0\n42\n101\n025600\n0\n\n\n7347\nwVxXRFf10zTTAs11nr4xeA\n40.032483\n-75.214430\nPrimoHoagies\n55\n3.0\nRestaurants, Specialty Food, Food, Sandwiches, Italian\nPOINT (-75.21443 40.03248)\n204.0\n42\n...\nS\n538872.0\n0.0\n+40.0322339\n-075.2181174\n79464.0\n42\n101\n021300\n0\n\n\n7348\n8n93L-ilMAsvwUatarykSg\n39.951018\n-75.198240\nKitchen Gia\n22\n3.0\nCoffee & Tea, Food, Sandwiches, American (Traditional), Restaurants\nPOINT (-75.19824 39.95102)\n315.0\n42\n...\nS\n883507.0\n49251.0\n+39.9523358\n-075.1889603\n27821.0\n42\n101\n036902\n0\n\n\n7349\nWnT9NIzQgLlILjPT0kEcsQ\n39.935982\n-75.158665\nAdelita Taqueria & Restaurant\n35\n4.5\nRestaurants, Mexican\nPOINT (-75.15867 39.93598)\n33.0\n42\n...\nS\n535423.0\n0.0\n+39.9367634\n-075.1595100\n91382.0\n42\n101\n002400\n0\n\n\n\n\n7350 rows × 26 columns\n\n\n\n\n\n2.3 Load fast food review data\nIn the rest of part 2, we’re going to run a sentiment analysis on the reviews for fast food restaurants. The review data for all fast food restaurants identified in part 2.1 is already stored in the data/ folder. The data is stored as a JSON file and you can use pandas.read_json to load it.\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\nreviews = pd.read_json('Data/reviews_philly_fast_food.json', orient='records', lines=True)\nrestreviews = reviews.merge(resturantsGpd, on = \"business_id\", how = \"inner\")\nrestreviews = restreviews.rename(columns={\"stars_x\": \"stars_given\", \"stars_y\": \"overall_rating\"})\nrestreviews.head(1)\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars_given\ntext\nlatitude\nlongitude\nname\nreview_count\noverall_rating\ncategories\ngeometry\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n39.93944\n-75.166805\nMcDonald's\n55\n2.0\nFast Food, Food, Restaurants, Coffee & Tea, Burgers\nPOINT (-75.16680 39.93944)\n\n\n\n\n\n\n\n\n\n2.4 Trim to the most popular fast food restaurants\nThere’s too many reviews to run a sentiment analysis on all of them in a reasonable time. Let’s trim our reviews dataset to the most popular fast food restaurants, using the list provided below.\nYou will need to get the “business_id” values for each of these restaurants from the restaurants data loaded in part 1.3. Then, trim the reviews data to include reviews only for those business IDs.\n\npopular_fast_food = [\n    \"McDonald's\",\n    \"Wendy's\",\n    \"Subway\",\n    \"Popeyes Louisiana Kitchen\",\n    \"Taco Bell\",\n    \"KFC\",\n    \"Burger King\",\n]\n\n\nrestfocus = restreviews[restreviews['name'].isin(popular_fast_food)]\nrestfocus.head(1)\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars_given\ntext\nlatitude\nlongitude\nname\nreview_count\noverall_rating\ncategories\ngeometry\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n39.93944\n-75.166805\nMcDonald's\n55\n2.0\nFast Food, Food, Restaurants, Coffee & Tea, Burgers\nPOINT (-75.16680 39.93944)\n\n\n\n\n\n\n\n\n\n2.5 Run the emotions classifier on fast food reviews\nRun a sentiment analysis on the reviews data from the previous step. Use the DistilBERT model that can predict emotion labels (anger, fear, sadness, joy, love, and surprise). Transform the result from the classifier into a DataFrame so that you have a column for each of the emotion labels.\n\ndescriptions = restfocus[\"text\"].str.strip().tolist()\ndescriptions[:3]\n\n['I know I shouldn\\'t expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\".  Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.',\n 'Dirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news!  A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day!  We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.',\n \"That's a shame! \\nThis place is full of junkies customers \\nThe staff and the service is fast \\nIt's just too much like homeless or you can tell the addict people all coming here and soliciting\"]\n\n\n\nexample_string = \"This is an Example\"\ndescriptions_words = [desc.split() for desc in descriptions]\ndescriptions_words_flat = []\n\nfor list_of_words in descriptions_words:\n    for word in list_of_words:\n        descriptions_words_flat.append(word)\ndescriptions_words_lower = [word.lower() for word in descriptions_words_flat]\nimport nltk\n\nnltk.download(\"stopwords\");\nstop_words = list(set(nltk.corpus.stopwords.words(\"english\")))\ndescriptions_no_stop = []\n\nfor word in descriptions_words_lower:\n    if word not in stop_words:\n        descriptions_no_stop.append(word)\ndescriptions_no_stop = [\n    word for word in descriptions_words_lower if word not in stop_words\n]\nimport string\npunctuation = list(string.punctuation)\ndescriptions_final = []\n\n# Loop over all words\nfor word in descriptions_no_stop:\n    # Remove any punctuation from the words\n    for p in punctuation:\n        word = word.replace(p, \"\")\n\n    # Save it if the string is not empty\n    if word != \"\":\n        descriptions_final.append(word)\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nwords = pd.DataFrame({\"words\": descriptions_final})\n\n\nN = (\n    words.groupby(\"words\", as_index=False)\n    .size()\n    .sort_values(\"size\", ascending=False, ignore_index=True)\n)\n\n\ntop20 = N.head(20)\n\ntop20\n\n\n\n\n\n\n\n\nwords\nsize\n\n\n\n\n0\nfood\n1840\n\n\n1\norder\n1430\n\n\n2\nget\n1012\n\n\n3\none\n980\n\n\n4\nlike\n922\n\n\n5\nservice\n901\n\n\n6\nplace\n882\n\n\n7\ntime\n873\n\n\n8\nchicken\n848\n\n\n9\nmcdonalds\n810\n\n\n10\ngo\n748\n\n\n11\nlocation\n677\n\n\n12\neven\n637\n\n\n13\ndrive\n601\n\n\n14\ngot\n585\n\n\n15\nfries\n570\n\n\n16\nnever\n568\n\n\n17\nordered\n563\n\n\n18\nback\n554\n\n\n19\ngood\n550\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot horizontal bar graph\nsns.barplot(\n    y=\"words\",\n    x=\"size\",\n    data=top20,\n    ax=ax,\n    color=\"crimson\",\n    saturation=1.0,\n)\n\nax.set_title(\"Most Common Words Found in Fast Food reviews\")\n\n# Remove spines (the box)\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\n# Remove x and y ticks\nax.tick_params(bottom=False, left=False)\n\n\n\n\n\n\n2.6 Identify the predicted emotion for each text\nUse the pandas idxmax() to identify the predicted emotion for each review, and add this value to a new column called “prediction”\nThe predicted emotion has the highest confidence score across all emotion labels for a particular label.\n\nfrom transformers import pipeline\nmodel = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n\nemotion_classifier = pipeline(\n    task=\"text-classification\", \n    model=model,  \n    top_k=None, \n    tokenizer=model,  \n    truncation=True, \n)\n\n\n\n\nC:\\Users\\Owner\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\huggingface_hub\\file_download.py:138: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Owner\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\nTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n  warnings.warn(message)\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%time \n\nemotion_scores = emotion_classifier(descriptions)\nemotion_scores[0]\n\nCPU times: total: 1min 49s\nWall time: 5min 1s\n\n\n[{'label': 'sadness', 'score': 0.7338687777519226},\n {'label': 'fear', 'score': 0.25067660212516785},\n {'label': 'anger', 'score': 0.011039001867175102},\n {'label': 'joy', 'score': 0.002758048241958022},\n {'label': 'surprise', 'score': 0.0010148986475542188},\n {'label': 'love', 'score': 0.0006427847547456622}]\n\n\n\nemotion = pd.DataFrame(\n    [{d[\"label\"]: d[\"score\"] for d in dd} for dd in emotion_scores]\n).assign(text=descriptions)\nemotion.head(3)\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\n\n\n\n\n0\n0.733869\n0.250677\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n1\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n\n\n2\n0.268274\n0.039276\n0.658548\n0.030782\n0.001894\n0.001226\nThat's a shame! \\nThis place is full of junkies customers \\nThe staff and the service is fast \\nIt's just too much like homeless or you can tell the addict people all coming here and soliciting\n\n\n\n\n\n\n\n\nemotion_labels = [\"anger\", \"fear\", \"sadness\", \"joy\", \"surprise\", \"love\"]\nemotion['prediction'] = emotion[emotion_labels].idxmax(axis=1)\n\n\nemotion_count = emotion.groupby(\"prediction\").size().sort_values()\n\n# Plotting\nax = emotion_count.plot(kind='barh', color='darkolivegreen')\n\n# Add title\nax.set_title('Emotion Predictions for Yelp Fast Food Reviews')\n\n# Remove spines (the box)\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\n# Remove x and y ticks\nax.tick_params(bottom=False, left=False)\n\n\n\n\n\n\n2.7 Combine the ratings and sentiment data\nCombine the data from part 2.4 (reviews data) and part 2.6 (emotion data). Use the pd.concat() function and combine along the column axis.\nNote: You’ll need to reset the index of your reviews data frame so it matches the emotion data index (it should run from 0 to the length of the data - 1).\n\nreview_sentiment = emotion.merge(restfocus, on = \"text\", how = \"left\")\n\n\n\n2.8 Plot sentiment vs. stars\nWe now have a dataframe with the predicted primary emotion for each review and the associated number of stars for each review. Let’s explore two questions:\n\nDoes sentiment analysis work? Do reviews with fewer stars have negative emotions?\nFor our fast food restaurants, are reviews generally positive or negative?\n\nUse seaborn’s histplot() to make a stacked bar chart showing the breakdown of each emotion for each stars category (1 star, 2 stars, etc.). A few notes:\n\nTo stack multiple emotion labels in one bar, use the multiple=\"stack\" keyword\nThe discrete=True can be helpful to tell seaborn our stars values are discrete categories\n\n\nreview_sentiment_grouped = review_sentiment.groupby(\"stars_given\")[[\"sadness\", \"fear\", \"joy\", \"anger\", \"surprise\"]].mean()\nreview_sentiment_grouped = review_sentiment_grouped.reset_index()\nreview_sentiment_long = pd.melt(review_sentiment_grouped, id_vars=['stars_given'], var_name='emotion', value_name='score')\n\n\n\npalette = sns.color_palette(\"cubehelix\")\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=review_sentiment_long, x=\"stars_given\", weights=\"score\", hue=\"emotion\", multiple=\"stack\", discrete=True, palette=palette)\n\n\nplt.title('Stacked Histogram of Review Sentiment Scores by Star Rating')\nplt.xlabel(\"User's Rating\")\nplt.ylabel('Score')\n\n\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.tick_params(bottom=False, left=False)\n\nplt.show()\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_17508\\205832856.py:5: UserWarning: The palette list has more values (6) than needed (5), which may not be intended.\n  sns.histplot(data=review_sentiment_long, x=\"stars_given\", weights=\"score\", hue=\"emotion\", multiple=\"stack\", discrete=True, palette=palette)\n\n\n\n\n\nQuestion: What does your chart indicate for the effectiveness of our sentiment analysis? Does our original hypothesis about fast food restaurants seem plausible?\nThe chart indicates there are certain emotions more prominent in certain ratings. Places recieving a 4 or 5 have mostly joyful reviews, with scatters of other emotions that may have been misinterpreted by the model. Lower rated places see anger or sadness which makes sense given the user’s attitude towards the restaurant."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "On this about page, you might want to add more information about yourself, the project, or course. Any helpful context could go here!\nMy name is Nick Hand, the instructor for the course. You can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2023.\nAdipisicing proident minim non non dolor quis. Pariatur in ipsum aliquip magna. Qui ad aliqua nulla excepteur dolor nostrud quis nisi. Occaecat proident eiusmod in cupidatat. Elit qui laboris sit aliquip proident dolore. Officia commodo commodo in eiusmod aliqua sint cupidatat consectetur aliqua sint reprehenderit.\nOccaecat incididunt esse et elit adipisicing sit est cupidatat consequat. Incididunt exercitation amet dolor non sit anim veniam veniam sint velit. Labore irure reprehenderit ut esse. Minim quis commodo nisi voluptate."
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html",
    "href": "analysis/Assignment2Kapuvari.html",
    "title": "Visualization Basics | Seaborn, Matploblib, Altair",
    "section": "",
    "text": "Trevor Kapuvari Voter Turnout 2018"
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#data-wrangling-and-database-framing",
    "href": "analysis/Assignment2Kapuvari.html#data-wrangling-and-database-framing",
    "title": "Visualization Basics | Seaborn, Matploblib, Altair",
    "section": "Data Wrangling and Database Framing",
    "text": "Data Wrangling and Database Framing\nvoterturnout2018df\n\nMainParties18 = voterturnout2018df[voterturnout2018df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\nMainParties17 = voterturnout2017df[voterturnout2017df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\nMainParties16 = voterturnout2016df[voterturnout2016df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\nMainParties15 = voterturnout2015df[voterturnout2015df['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])]\n\nmerged_elections = pd.concat([MainParties18, MainParties17, MainParties16, MainParties15], axis=0)\nmerged_elections\n\n\n\n\n\n\n\n\nelection\nelection_date\nprecinct_description\nprecinct_code\npolitical_party\nvoter_count\n\n\n\n\n0\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 01\n101\nDEMOCRATIC\n158\n\n\n3\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 01\n101\nREPUBLICAN\n9\n\n\n4\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 02\n102\nDEMOCRATIC\n174\n\n\n8\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 02\n102\nREPUBLICAN\n8\n\n\n9\n2018 GENERAL PRIMARY\n5/15/2018\nPHILA WD 01 DIV 03\n103\nDEMOCRATIC\n243\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n6473\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 44\n6644\nREPUBLICAN\n61\n\n\n6477\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n6645\nREPUBLICAN\n48\n\n\n6478\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n6645\nDEMOCRATIC\n67\n\n\n6482\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n6646\nREPUBLICAN\n94\n\n\n6483\n2015 MUNICIPAL PRIMARY\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n6646\nDEMOCRATIC\n113\n\n\n\n\n12959 rows × 6 columns\n\n\n\n\nmerged_elections.set_index(['precinct_code', 'election', 'political_party'], append=True)\n#setting an index better organized the table to indicate ways to tidy/pivot it to our liking\n\n\n\n\n\n\n\n\n\n\n\nelection_date\nprecinct_description\nvoter_count\n\n\n\nprecinct_code\nelection\npolitical_party\n\n\n\n\n\n\n\n0\n101\n2018 GENERAL PRIMARY\nDEMOCRATIC\n5/15/2018\nPHILA WD 01 DIV 01\n158\n\n\n3\n101\n2018 GENERAL PRIMARY\nREPUBLICAN\n5/15/2018\nPHILA WD 01 DIV 01\n9\n\n\n4\n102\n2018 GENERAL PRIMARY\nDEMOCRATIC\n5/15/2018\nPHILA WD 01 DIV 02\n174\n\n\n8\n102\n2018 GENERAL PRIMARY\nREPUBLICAN\n5/15/2018\nPHILA WD 01 DIV 02\n8\n\n\n9\n103\n2018 GENERAL PRIMARY\nDEMOCRATIC\n5/15/2018\nPHILA WD 01 DIV 03\n243\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n6473\n6644\n2015 MUNICIPAL PRIMARY\nREPUBLICAN\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 44\n61\n\n\n6477\n6645\n2015 MUNICIPAL PRIMARY\nREPUBLICAN\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n48\n\n\n6478\n6645\n2015 MUNICIPAL PRIMARY\nDEMOCRATIC\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 45\n67\n\n\n6482\n6646\n2015 MUNICIPAL PRIMARY\nREPUBLICAN\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n94\n\n\n6483\n6646\n2015 MUNICIPAL PRIMARY\nDEMOCRATIC\n05/19/2015 12:00:00 AM\nPHILA WD 66 DIV 46\n113\n\n\n\n\n12959 rows × 3 columns\n\n\n\n\ntidyElections = pd.pivot(merged_elections, columns = (\"election\", \"political_party\"), index = \"precinct_code\", values = \"voter_count\")\ntidyElections\n\n\n\n\n\n\n\nelection\n2018 GENERAL PRIMARY\n2017 MUNICIPAL PRIMARY\n2016 GENERAL PRIMARY\n2015 MUNICIPAL PRIMARY\n\n\npolitical_party\nDEMOCRATIC\nREPUBLICAN\nDEMOCRATIC\nREPUBLICAN\nREPUBLICAN\nDEMOCRATIC\nDEMOCRATIC\nREPUBLICAN\n\n\nprecinct_code\n\n\n\n\n\n\n\n\n\n\n\n\n101\n158.0\n9.0\n135.0\n4.0\n13.0\n207.0\n157.0\n3.0\n\n\n102\n174.0\n8.0\n178.0\n6.0\n38.0\n288.0\n187.0\n11.0\n\n\n103\n243.0\n16.0\n205.0\n10.0\n37.0\n318.0\n245.0\n17.0\n\n\n104\n183.0\n28.0\n147.0\n21.0\n65.0\n242.0\n170.0\n24.0\n\n\n105\n84.0\n7.0\n62.0\n8.0\n11.0\n124.0\n98.0\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6642\n49.0\n40.0\n47.0\n32.0\n95.0\n122.0\n101.0\n63.0\n\n\n6643\n84.0\n63.0\n79.0\n44.0\n176.0\n168.0\n147.0\n100.0\n\n\n6644\n98.0\n44.0\n71.0\n33.0\n122.0\n175.0\n159.0\n61.0\n\n\n6645\n31.0\n26.0\n29.0\n19.0\n100.0\n80.0\n67.0\n48.0\n\n\n6646\n58.0\n69.0\n73.0\n78.0\n164.0\n119.0\n113.0\n94.0\n\n\n\n\n1688 rows × 8 columns\n\n\n\n\nMatplotlib Chart, Data & Plotting\nHere we are comparing voter participation for the Democratic Party between the years of 2016 and 2018, specifically among their primary elections.\nMatplotlib was used for the scatter chart because of the simplisity and direct approach to plotting large sets of data. The basic approach of matplotlib creates visualizations that are easy to comprehend for viewers.\nHere, we can tell that the precincts that had more votes in 2016 also had more votes in 2018, with some notable outliers.\n\nx= tidyElections['2018 GENERAL PRIMARY']['DEMOCRATIC']\ny = tidyElections['2016 GENERAL PRIMARY']['DEMOCRATIC']\n\n\n#matplotlib\n\nfig, ax = plt.subplots()\nx = x #2018 votes\ny = y #2016 votes \nax.scatter(x, y, c='blue')\nax.set_title('Democratic Votes in General Primary, 2018  vs 2016, by Precinct')\nax.set_xlabel('Votes in 2018')\nax.set_ylabel('Votes in 2016')\nplt.ylim(0,400)\nplt.show()\n\n\n\n\nThe scatter plot displays the difference in Democratic votes in the 2018 and 2016 general primaries. Each dot represents a precinct that compares the two years. You may also notice the cluster goes off the chart when trending upward. This cut-off was done on purpose, the x and y axis are fixed to be the same to demonstrate the difference in voter participation. In 2016, a presidential election year, showed significantly more participation than in 2018, a mid-term election year. When lookinng at individual points, you will notice there is, generally, more votes counted in 2016 than in 2018."
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#altair-chart-1",
    "href": "analysis/Assignment2Kapuvari.html#altair-chart-1",
    "title": "Visualization Basics | Seaborn, Matploblib, Altair",
    "section": "Altair Chart 1",
    "text": "Altair Chart 1\n\nPie Chart of Election Participation Overall\n\nalt.Chart(Electiondf, title=\"Voting Comparisons by Election\").mark_arc(innerRadius=50).encode(\n    theta=\"Total\",\n    color=\"Election:N\",\n)\n\n\n\n\n\n\n\nThe pie chart helps us conclude the amount of voter participation in each election year.\nWe notice two major changes over time from the chart. The first is that voter turnout decreased from 2015 to 2018 generally, and peaked at 2016.\nThe second is that the largest declines in turnout were from municipal elections, specifically the years 2015 and 2017."
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#altair-2",
    "href": "analysis/Assignment2Kapuvari.html#altair-2",
    "title": "Visualization Basics | Seaborn, Matploblib, Altair",
    "section": "Altair 2",
    "text": "Altair 2\n\nVoter Participation in Elections, Visualized Through Bar Chart\n\n#altair 2\nalt.Chart(Electiondf, title=\"Voting Comparisons by Election (Bar Form)\").mark_bar().encode(\n    x=\"Election\",\n    y=\"Total\",\n).properties(\n    width=alt.Step(40),\n)\n\n\n\n\n\n\n\nThe bar graph shows the same thing as the pie chart but better compares individual years to one another. We notice here that voter turnout has been declining overall in both types of elections ever since 2016."
  },
  {
    "objectID": "analysis/Assignment2Kapuvari.html#altair-3",
    "href": "analysis/Assignment2Kapuvari.html#altair-3",
    "title": "Visualization Basics | Seaborn, Matploblib, Altair",
    "section": "Altair 3",
    "text": "Altair 3\n\nScatter Plot, Democrats vs Republicans in 9 Sampled Precincts\nHere we look at 9 sampled precincts, specifically where Republicans accumulated their largest number of total votes.\n\nElection2018 = pd.read_csv(\"./Data/voters2018top9.csv\")\nElection2018\n\n\n\n\n\n\n\n\nPrecinct\nSum of REPUBLICAN\nSum of DEMOCRATIC\n\n\n\n\n0\n3505\n69\n118\n\n\n1\n4503\n81\n118\n\n\n2\n4520\n71\n99\n\n\n3\n4524\n69\n89\n\n\n4\n5824\n77\n148\n\n\n5\n5841\n90\n142\n\n\n6\n6311\n86\n180\n\n\n7\n6520\n87\n255\n\n\n8\n6617\n81\n24\n\n\n9\n6646\n69\n58\n\n\n\n\n\n\n\n\n#altair  3\nbrush = alt.selection_interval()\nalt.Chart(Election2018).mark_point().encode(\n    x='Sum of DEMOCRATIC',\n    y='Sum of REPUBLICAN',\n    color=alt.condition(brush, 'Precinct', alt.value('grey')),\n).add_params(brush)\n\n\n\n\n\n\n\nThis chart shows precincts where Republicans accumulated the most votes in their primaries. We notice in one precinct, 6617, that there was larger participation by Republicans than by Democrats. This comparison can help us predict future election results in this parrticular precinct. Meanwhile, other precincts still have a heavy Democratic lean in terms of votes."
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/streetsandwebscrap.html",
    "href": "analysis/streetsandwebscrap.html",
    "title": "Street Networks & Web Scraping",
    "section": "",
    "text": "Part 1: Visualizing crash data in Philadelphia\nIn this section, you will use osmnx to analyze the crash incidence in Center City.\nPart 2: Scraping Craigslist\nIn this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia’s Craigslist portal."
  },
  {
    "objectID": "analysis/streetsandwebscrap.html#part-1-visualizing-crash-data-in-philadelphia",
    "href": "analysis/streetsandwebscrap.html#part-1-visualizing-crash-data-in-philadelphia",
    "title": "Street Networks & Web Scraping",
    "section": "Part 1: Visualizing crash data in Philadelphia",
    "text": "Part 1: Visualizing crash data in Philadelphia\n1.1 Load the geometry for the region being analyzed We’ll analyze crashes in the “Central” planning district in Philadelphia, a rough approximation for Center City. Planning districts can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\nhttp://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\nSelect the “Central” district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type shapely.geometry.polygon.Polygon\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport hvplot.pandas \nimport altair as alt\nimport matplotlib.pyplot as plt\nimport rasterio as rio\nimport osmnx as ox\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\nCPD = gpd.read_file(\"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\")\n#CPD = ox.geocode_to_gdf(\"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\")\nCPD = CPD[CPD[\"ABBREV\"] == \"CTR\"]\nCPD\n\n\n\n\n\n\n\n\nOBJECTID_1\nOBJECTID\nDIST_NAME\nABBREV\nShape__Area\nShape__Length\nPlanningDist\nDaytimePop\ngeometry\n\n\n\n\n3\n4\n9\nCentral\nCTR\n1.782880e+08\n71405.14345\nNaN\nNaN\nPOLYGON ((-75.14791 39.96733, -75.14715 39.967...\n\n\n\n\n\n\n\n\nPhillyox = ox.geocode_to_gdf(\"Philadelphia, PA\")\nPhillyox.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n1.2 Get the street network graph\nUse OSMnx to create a network graph (of type ‘drive’) from your polygon boundary in 1.1.\n\nax = ox.project_gdf(CPD).plot(fc=\"lightgreen\", ec=\"black\")\nax.set_axis_off()\n\n\n\n\n\nax = CPD.to_crs(epsg=2272).plot(facecolor=\"none\", edgecolor=\"black\")\nax.set_axis_off()\n\n\n\n\n\nstreets = ox.features_from_place(\"Philadelphia, PA\", tags={\"highway\": True})\n\n\nstreets.head(6)\n\n\n\n\n\n\n\n\n\nhighway\ngeometry\ntraffic_signals\ntraffic_signals:direction\nrailway\ncrossing\nref\nnoref\nnoexit\nref:left\nref:right\nold_ref\ndisused:railway\nstop\nname\ncrossing:markings\ndirection\ntraffic_calming\ntactile_paving\naccess\nsource\ncrossing_ref\nbicycle\ndescription\nfoot\nhorse\nmotor_vehicle\naddr:city\naddr:housenumber\naddr:postcode\naddr:state\naddr:street\nwebsite\nnote\ncrossing:island\nproposed:junction\nsupervised\nexcept\nproposed:highway\nford\nele\ngnis:feature_id\nbench\nbus\ncovered\nnetwork\nnetwork:wikidata\noperator\npublic_transport\nshelter\nkerb\njunction\ngate\nparking\nwheelchair\nnetwork:wikipedia\nindoor\nlevel\nfixme\nroute_ref\nlocal_ref\ndesignation\nleisure\naeroway\nmaxheight\nheritage\nheritage:operator\nref:nrhp\nmaterial\nbin\nlit\ntraffic_signals:sound\nlamp_type\ndepartures_board\ncapacity\nbutton_operated\nlayer\nopening_hours\nsurface\ntraffic_signals:vibration\ninternet_access\nroute_ref_1\nlanduse\nalt_name\nlamp_mount\ntraffic_sign\noperator:wikidata\nshort_name\nbrand\nbrand:wikidata\ntram\nmotor_vehicle:conditional\nhgv\ntourism\ntrolleybus\nnetwork:short\nman_made\nflashing_lights\nabandoned\nnot:network:wikidata\nsupport\ntraffic_signals:countdown\nhistoric\ncycleway\ninformal\nadvertising\ncheck_date:crossing\nnodes\noneway\ntiger:cfcc\ntiger:name_base\ntiger:name_type\ntiger:reviewed\ntiger:zip_left\ntiger:zip_right\nlane_markings\nlanes\ntiger:name_direction_prefix\ntiger:separated\ntiger:source\ntiger:tlid\ntiger:zip_left_1\nservice\nparking:lane:both\nsidewalk\ncycleway:right\nold_ref_legislative\nref:penndot\ntiger:name_direction_suffix\ndestination:ref\nturn:lanes\ndestination\ntoll\ndestination:ref:to\nmaxspeed\nsource:hgv\nsource:ref:penndot\nsmoothness\nbridge\njunction:ref\ndestination:lanes\nwidth\ncycleway:both\ncycleway:both:buffer\nNHS\ncycleway:right:buffer\ncheck_date:smoothness\ndestination:street\nwikipedia\nlanes:backward\nlanes:forward\nturn:lanes:backward\ntracktype\nturn\nmaxspeed:advisory\npsv\ndestination:symbol\ntunnel\nnoname\nmaxweight:signed\ntiger:name_base_1\nsource:width\nrcn_ref\nabandoned:railway\nparking:lane:left\nparking:lane:left:parallel\nparking:lane:right\ncycleway:left\nembedded_rails\nname_1\ntiger:name_base_2\ntiger:name_type_1\ndestination:ref:lanes\nold_ref:legislative\ndestination:to\ntiger:zip_left_2\ntiger:zip_right_1\ntiger:zip_right_2\ncycleway:left:buffer\ncycleway:right:lane\nsource:oneway\nsidewalk:both\nlcn\nparking:lane:right:parallel\nmaxspeed:type\nparking:both\nparking:orientation\nsidewalk:both:surface\ncentre_turn_lane\ntiger:zip_left_3\ntiger:zip_left_4\nparking:left\nparking:right\nparking:right:orientation\nsidewalk:left\nsidewalk:right\nmaxwidth\nold_name\nmaxspeed:backward\nold_railway_operator\nbicycle:backward:conditional\nbicycle:forward:conditional\nname:etymology:wikidata\noneway:bicycle\nconstruction\nwikidata\nincline\ncheck_date:surface\ndestination:street:lanes\nparking:both:orientation\naccess:hgv\nmaxweight\nFIXME\nlanes:both_ways\nturn:lanes:both_ways\nturn:lanes:forward\ntiger:zip_right_3\nsource:lanes\nrestriction:hgv\nmaxspeed:forward\ntiger:name_direction_prefix_1\nparking:lane:both:parallel\nnote:lanes\nname_2\ntiger:name_direction_prefix_2\ntiger:name_type_2\ntiger:zip_right_4\nmotorcar\ntiger:name_direction\nnote:old_railway_operator\ntoll:backward\ntoll:forward\nbicycle:forward\nparking:condition:right\nbicycle:backward\nmaxwidth:physical\ntiger:name_base_3\ntiger:name_direction_prefix_3\ntiger:name_type_3\nsource:boundary\nsegregated\nbridge:name\nstart_date\nname2\ncreated_by\nmotorroad\nfootway\nmaxspeed:variable\ntiger:county\ndestination:ref:to:lanes\narea\nhandrail\nramp\nparking:left:markings\nparking:left:orientation\nstep_count\nsac_scale\ntrail_visibility\ntrolley_wire\nvoltage\nhighspeed\ncheck_date\nbase_material_values\nbridge:structure\nembankment\ndestination:lanes:forward\ndestination:ref:to:lanes:forward\ndestination:ref:lanes:forward\ndestination:ref:to:lanes:backward\ndisused:cycleway:both\ndestination:lanes:backward\nopening_date\nlanes:bus\nmtb:scale\naccess:lanes:forward\npsv:lanes:forward\nplacement:backward\ncanoe\nsurface:condition\ndestination:ref:forward\ndestination:street:forward\ntiger:zip\nunsigned_ref\nhgv:lanes\nshoulder\nplacement\nabandoned:highway\ndestination:ref:lanes:backward\nproposed\nbicycle:lane\nloc_name\nramp:wheelchair\nmtb:scale:imba\nmtb:scale:uphill\ndisused:cycleway:right\ngoods\ncutting\ncycleway:left:oneway\nparking:condition:left\nparking:condition:left:time_interval\nski\nsnowmobile\nemergency\nsource:name\nname:en\nHFCS\nNJDOT_SRI\nbridge:movable\nhgv:state_network\nsource:hgv:state_network\nchange:lanes:backward\nchange:lanes:forward\nparking:lane:right:diagonal\nlevels\npostal_code\nlocation\nlength\nsport\nclosed\ncondition\ndanger\ndanger_1\nhazard\ncycleway:both:width\nparking:lane:left:diagonal\nPARK_NAME\nSHAPE_LENG\nTRL_LENGTH\nparking:condition:both\nproposed:lanes\nproposed:lanes:forward\nproposed:turn:lanes:forward\nproposed:lanes:backward\nproposed:turn:lanes:backward\nplacement:forward\ndestination:street:backward\ndestination:street:lanes:forward\ndestination:street:lanes:backward\nname:backward\nname:forward\ncycleway:buffer\nturn:backward\ngolf\ngauge\nparking:condition:both:time_interval\ntrain\nhorse_scale\ndog\nturn:forward\nsurface:lanes:backward\nsurface:lanes:forward\nbus:backward\nhgv:backward\nmotorcar:backward\nmotorroad:backward\nartist:website\nartist:wikidata\nartist_name\nnot:name\ngolf_cart\nsteps\ncomment\nhistoric:railway\ndisused:cycleway:left\noneway:bus\nstairs\nsidewalk:left:surface\nsidewalk:right:surface\ntruck\nstroller\nconveying\ncheck_date:handrail\nways\ntype\n\n\nelement_type\nosmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnode\n109727831\ntraffic_signals\nPOINT (-75.15328 40.02996)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727914\ntraffic_signals\nPOINT (-75.13837 40.02803)\nsignal\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727992\ntraffic_signals\nPOINT (-75.14616 40.02904)\nsignal\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727997\ntraffic_signals\nPOINT (-75.14686 40.02914)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109728089\nturning_circle\nPOINT (-74.99562 40.10542)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109728114\ntraffic_signals\nPOINT (-75.13897 40.03285)\nsignal\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nCPDoutline = CPD.squeeze().geometry\nCPDoutline\n\n\n\n\n\nG_CPD = ox.graph_from_polygon(CPDoutline, network_type=\"drive\")\n\n\nox.plot_graph(ox.project_graph(G_CPD), node_size=0);\n\n\n\n\n\n\n1.3 Convert your network graph edges to a GeoDataFrame\nUse OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network.\n\nCPD_edges = ox.graph_to_gdfs(G_CPD, edges=True, nodes=False)\n\n\nCPD_edges\n\n\n\n\n\n\n\n\n\n\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\n\n\nu\nv\nkey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n109727439\n109911666\n0\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727448\n109727439\n0\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n110034229\n0\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727507\n110024052\n0\n193364514\nTrue\nCarpenter Street\nresidential\nFalse\n53.208\nLINESTRING (-75.17196 39.93973, -75.17134 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109728761\n110274344\n0\n672312336\nTrue\nBrown Street\nresidential\nFalse\n58.270\nLINESTRING (-75.17317 39.96951, -75.17250 39.9...\n25 mph\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11176163640\n11176163648\n1\n1206065012\nFalse\nNaN\nliving_street\nFalse\n57.963\nLINESTRING (-75.16655 39.95896, -75.16633 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5808113442\n0\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n37.086\nLINESTRING (-75.16655 39.95896, -75.16662 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11176163648\n5808113443\n0\n613950538\nFalse\nAlexander Court\nliving_street\nFalse\n31.592\nLINESTRING (-75.16652 39.95909, -75.16646 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11176163640\n0\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n14.738\nLINESTRING (-75.16652 39.95909, -75.16655 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1206065012\nFalse\nNaN\nliving_street\nTrue\n57.963\nLINESTRING (-75.16652 39.95909, -75.16630 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3896 rows × 16 columns\n\n\n\n\n\n1.4 Load PennDOT crash data\nData for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n./data/CRASH_PHILADELPHIA_XXXX.csv\nYou should see three separate files in the data/ folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using pd.concat().\nThe data was downloaded for Philadelphia County from here.\n\ncrash2020 = pd.read_csv(\"./Data/CRASH_PHILADELPHIA_2020.csv\")\ncrash2021 = pd.read_csv(\"./Data/CRASH_PHILADELPHIA_2021.csv\")\ncrash2022 = pd.read_csv(\"./Data/CRASH_PHILADELPHIA_2022.csv\")\ncrashDf = pd.concat([crash2020, crash2021, crash2022])\ncrashDf\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2020036617\n1842.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n4\n2020\n1\n39.9809\n-75.2065\n1840.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n18\n1\n0\n0\nNaN\n1\n39 58:51.367\n3.0\n0\n75 12:23.366\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n0\nNaN\n2\n9\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nN\n1838\n0\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2020035717\n2000.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\nNaN\n67\n4\n2020\n1\n39.9269\n-75.1691\n2000.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n14\n1\n1\n1\nNaN\n9\n39 55:36.660\nNaN\n0\n75 10:08.677\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n0\nNaN\n1\n0\n1\n3\n3\nU\n1457\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n4.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2020034378\n1139.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n4\n2020\n4\n39.9237\n-75.1924\n1131.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n11\n1\n1\n0\nNaN\n0\n39 55:25.183\nNaN\n0\n75 11:32.758\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n1\nNaN\n1\n0\n0\n0\n0\nNaN\n1128\n1\n3\n0\n0\n0\n0\n0\n3\n4\n0\n3\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2020025511\n345.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2020\n1\n39.8826\n-75.2450\n329.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n3\n3\n2\n0\nNaN\n0\n39 52:57.717\nNaN\n0\n75 14:41.931\n3\n0\n0\n0\n67301\n0\n0\n0\nY\n0\n0\n0\n2\n68K01\n0\nNaN\n4\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nNaN\n328\n2\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8746\n2022016289\n2245.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n4\n40.0239\n-75.1425\n2242.0\n6\n0\n0\n0\n1\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n1\n1\nNaN\n0\n40 01:25.870\nNaN\n0\n75 08:33.165\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n2230\n1\n4\n0\n0\n0\n0\n0\n3\n4\n0\n4\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8748\n2022014729\n1721.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n1\n40.0189\n-75.0495\n1707.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n3\n1\n1\nNaN\n0\n40 01:08.120\nNaN\n0\n75 02:58.216\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n1701\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8749\n2022014753\n713.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\nNaN\n67\n2\n2022\n4\n39.9291\n-75.1687\n707.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n7\n1\n2\n0\nN\n0\n39 55:44.636\nNaN\n0\n75 10:07.421\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67301\n0\nNaN\n3\n1\nNaN\nN\nN\nN\n1\nNaN\n1\n0\n1\n0\n0\nNaN\n700\n2\n5\n0\n0\n0\n1\n0\n3\n4\n0\n5\n5\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n29463 rows × 100 columns\n\n\n\n\n\n1.5 Convert the crash data to a GeoDataFrame\nYou will need to use the DEC_LAT and DEC_LONG columns for latitude and longitude.\nThe full data dictionary for the data is available here\n\ncrashGDF = gpd.GeoDataFrame(crashDf, geometry=gpd.points_from_xy(crashDf[\"DEC_LONG\"], crashDf[\"DEC_LAT\"], crs=\"2272\"))\ncrashGDF\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n1\n2020036617\n1842.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n4\n2020\n1\n39.9809\n-75.2065\n1840.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n18\n1\n0\n0\nNaN\n1\n39 58:51.367\n3.0\n0\n75 12:23.366\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n0\nNaN\n2\n9\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nN\n1838\n0\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.207 39.981)\n\n\n2\n2020035717\n2000.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\nNaN\n67\n4\n2020\n1\n39.9269\n-75.1691\n2000.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n14\n1\n1\n1\nNaN\n9\n39 55:36.660\nNaN\n0\n75 10:08.677\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n0\nNaN\n1\n0\n1\n3\n3\nU\n1457\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n4.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.169 39.927)\n\n\n3\n2020034378\n1139.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n4\n2020\n4\n39.9237\n-75.1924\n1131.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n11\n1\n1\n0\nNaN\n0\n39 55:25.183\nNaN\n0\n75 11:32.758\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n1\nNaN\n1\n0\n0\n0\n0\nNaN\n1128\n1\n3\n0\n0\n0\n0\n0\n3\n4\n0\n3\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.192 39.924)\n\n\n4\n2020025511\n345.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2020\n1\n39.8826\n-75.2450\n329.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n3\n3\n2\n0\nNaN\n0\n39 52:57.717\nNaN\n0\n75 14:41.931\n3\n0\n0\n0\n67301\n0\n0\n0\nY\n0\n0\n0\n2\n68K01\n0\nNaN\n4\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nNaN\n328\n2\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.245 39.883)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8746\n2022016289\n2245.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n4\n40.0239\n-75.1425\n2242.0\n6\n0\n0\n0\n1\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n1\n1\nNaN\n0\n40 01:25.870\nNaN\n0\n75 08:33.165\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n2230\n1\n4\n0\n0\n0\n0\n0\n3\n4\n0\n4\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.142 40.024)\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.958)\n\n\n8748\n2022014729\n1721.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\nNaN\n67\n1\n2022\n1\n40.0189\n-75.0495\n1707.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n3\n1\n1\nNaN\n0\n40 01:08.120\nNaN\n0\n75 02:58.216\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n3\nNaN\n1701\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.049 40.019)\n\n\n8749\n2022014753\n713.0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\nNaN\n67\n2\n2022\n4\n39.9291\n-75.1687\n707.0\n6\n0\n0\n0\n0\n0\n2\n0\n0\nNaN\n0\n0\n0.0\n7\n1\n2\n0\nN\n0\n39 55:44.636\nNaN\n0\n75 10:07.421\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67301\n0\nNaN\n3\n1\nNaN\nN\nN\nN\n1\nNaN\n1\n0\n1\n0\n0\nNaN\n700\n2\n5\n0\n0\n0\n1\n0\n3\n4\n0\n5\n5\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.169 39.929)\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.947)\n\n\n\n\n29463 rows × 101 columns\n\n\n\n\n\n1.6 Trim the crash data to Center City\n\nGet the boundary of the edges data frame (from part 1.3). Accessing the .geometry.unary_union.convex_hull property will give you a nice outer boundary region.\nTrim the crashes using the within() function of the crash GeoDataFrame to find which crashes are within the boundary.\n\nThere should be about 3,750 crashes within the Central district.\n\nCPD_boundary = CPD_edges.geometry.unary_union.convex_hull\nCC_crashes = crashGDF[crashGDF.within(CPD_boundary)]\nCC_crashes\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n4\n1\nNaN\n67\n3\n2020\n2\n39.9700\n-75.1631\n1250.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n12\n1\n2\n1\nNaN\n0\n39 58:11.995\nNaN\n0\n75 09:47.193\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67508\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n3\nN\n1250\n2\n2\n0\n0\n0\n2\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.163 39.970)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n2\n2020\n7\n39.9523\n-75.1878\n800.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n8\n1\n2\n0\nNaN\n2\n39 57:08.258\n4.0\n0\n75 11:16.213\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67504\n0\nNaN\n6\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nN\n800\n2\n2\n0\n0\n2\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.188 39.952)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n5\n1\nNaN\n67\n3\n2020\n2\n39.9558\n-75.1484\n1024.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n1\n0.0\n10\n1\n0\n0\nNaN\n0\n39 57:21.042\nNaN\n2\n75 08:54.233\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67501\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1024\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.148 39.956)\n\n\n18\n2020000481\n1737.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n5\n1\nNaN\n67\n1\n2020\n4\n39.9534\n-75.1547\n1737.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n4\n0\n1\nNaN\n0\n39 57:12.069\nNaN\n0\n75 09:16.934\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n2\nNaN\n1735\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.155 39.953)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8719\n2022019573\n2210.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n1\n2022\n5\n39.9556\n-75.1728\n2205.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n21\n3\n1\n1\nNaN\n0\n39 57:20.163\nNaN\n0\n75 10:21.961\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n2\nNaN\n2145\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.173 39.956)\n\n\n8723\n2022037448\n252.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n2\n2022\n1\n39.9537\n-75.1816\n237.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n2\n3\n1\n0\nN\n1\n39 57:13.320\n3.0\n0\n75 10:53.702\n4\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n1\nNaN\n2\n1\n246.0\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nN\n236\n1\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.182 39.954)\n\n\n8744\n2022037464\n2225.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2022\n5\n39.9600\n-75.1798\n2210.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n0\n6\nNaN\n0\n39 57:36.017\nNaN\n2\n75 10:47.219\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n1\n0\n0\nNaN\n2205\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.180 39.960)\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.958)\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.947)\n\n\n\n\n3751 rows × 101 columns\n\n\n\n\n\n1.7 Re-project our data into an approriate CRS\nWe’ll need to find the nearest edge (street) in our graph for each crash. To do this, osmnx will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude\nWe’ll convert the local state plane CRS for Philadelphia, EPSG=2272\n\nTwo steps:\n\nProject the graph object (G) using the ox.project_graph. Run ox.project_graph? to see the documentation for how to convert to a specific CRS.\nProject the crash data using the .to_crs() function.\n\n\nG_projected = ox.project_graph(G_CPD, to_crs=2272)\nCC_crashproj = CC_crashes.to_crs(2272)\nCC_crashproj\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n4\n1\nNaN\n67\n3\n2020\n2\n39.9700\n-75.1631\n1250.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n12\n1\n2\n1\nNaN\n0\n39 58:11.995\nNaN\n0\n75 09:47.193\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67508\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n3\nN\n1250\n2\n2\n0\n0\n0\n2\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.163 39.970)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n2\n2020\n7\n39.9523\n-75.1878\n800.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n8\n1\n2\n0\nNaN\n2\n39 57:08.258\n4.0\n0\n75 11:16.213\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67504\n0\nNaN\n6\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nN\n800\n2\n2\n0\n0\n2\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.188 39.952)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n5\n1\nNaN\n67\n3\n2020\n2\n39.9558\n-75.1484\n1024.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n1\n0.0\n10\n1\n0\n0\nNaN\n0\n39 57:21.042\nNaN\n2\n75 08:54.233\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67501\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1024\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.148 39.956)\n\n\n18\n2020000481\n1737.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n5\n1\nNaN\n67\n1\n2020\n4\n39.9534\n-75.1547\n1737.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n17\n4\n0\n1\nNaN\n0\n39 57:12.069\nNaN\n0\n75 09:16.934\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n2\nNaN\n1735\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.155 39.953)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8719\n2022019573\n2210.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n1\n2022\n5\n39.9556\n-75.1728\n2205.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n21\n3\n1\n1\nNaN\n0\n39 57:20.163\nNaN\n0\n75 10:21.961\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n1\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n1\n3\n2\nNaN\n2145\n1\n2\n0\n0\n0\n0\n1\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.173 39.956)\n\n\n8723\n2022037448\n252.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n2\n2022\n1\n39.9537\n-75.1816\n237.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n2\n3\n1\n0\nN\n1\n39 57:13.320\n3.0\n0\n75 10:53.702\n4\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n1\n68K01\n1\nNaN\n2\n1\n246.0\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nN\n236\n1\n1\n0\n0\n0\n0\n0\n3\n4\n0\n1\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.182 39.954)\n\n\n8744\n2022037464\n2225.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2022\n5\n39.9600\n-75.1798\n2210.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n22\n3\n0\n6\nNaN\n0\n39 57:36.017\nNaN\n2\n75 10:47.219\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n1\n0\n0\nNaN\n2205\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.180 39.960)\n\n\n8747\n2022037461\n1706.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\nNaN\n67\n3\n2022\n4\n39.9583\n-75.1658\n1644.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n16\n1\n1\n7\nNaN\n0\n39 57:29.748\nNaN\n2\n75 09:56.922\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n68K01\n0\nNaN\n2\n1\nNaN\nN\nN\nN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1639\n1\n1\n0\n0\n0\n1\n0\n3\n4\n0\n1\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.958)\n\n\n8750\n2022014627\n1700.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\nNaN\n67\n2\n2022\n5\n39.9471\n-75.1660\n1646.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n9\n1\n1\n0\nN\n0\n39 56:49.690\nNaN\n0\n75 09:57.440\n3\n0\n0\n0\n67301\n1\n0\n0\nN\n1\n0\n0\n2\n67301\n0\nNaN\n1\n9\nNaN\nN\nN\nN\n0\nNaN\n1\n0\n0\n0\n0\nNaN\n945\n1\n2\n0\n0\n0\n0\n0\n3\n4\n0\n1\n7\n7.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.166 39.947)\n\n\n\n\n3751 rows × 101 columns\n\n\n\n\nox.plot_graph(G_projected);\n\n\n\n\n\n\n\n1.8 Find the nearest edge for each crash\nSee: ox.distance.nearest_edges(). It takes three arguments:\n\nthe network graph\nthe longitude of your crash data (the x attribute of the geometry column)\nthe latitude of your crash data (the y attribute of the geometry column)\n\nYou will get a numpy array with 3 columns that represent (u, v, key) where each u and v are the node IDs that the edge links together. We will ignore the key value for our analysis.\n\ndef find_nearest_edge(CC_crashproj, G_CPD):\n    crash_longitudes = CC_crashproj.geometry.x\n    crash_latitudes = CC_crashproj.geometry.y\n\n    nearest_edges = ox.distance.nearest_edges(G_CPD, crash_longitudes, crash_latitudes)\n    #CC_crashproj[\"nearest_edge\"] = nearest_edges\n    return nearest_edges\n\ncrashes_nearest_edge = find_nearest_edge(CC_crashproj, G_CPD)\n#crashes_nearest_edge\n\n\n\n1.9 Calculate the total number of crashes per street\n\nMake a DataFrame from your data from part 1.7 with three columns, u, v, and key (we will only use the u and v columns)\nGroup by u and v and calculate the size\nReset the index and name your size() column as crash_count\n\nAfter this step you should have a DataFrame with three columns: u, v, and crash_count.\n\nCC_crashproj.head(4)\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\nCHLDPAS_SUSP_SERIOUS_INJ_COUNT\nCOLLISION_TYPE\nCOMM_VEH_COUNT\nCONS_ZONE_SPD_LIM\nCOUNTY\nCRASH_MONTH\nCRASH_YEAR\nDAY_OF_WEEK\nDEC_LAT\nDEC_LONG\nDISPATCH_TM\nDISTRICT\nDRIVER_COUNT_16YR\nDRIVER_COUNT_17YR\nDRIVER_COUNT_18YR\nDRIVER_COUNT_19YR\nDRIVER_COUNT_20YR\nDRIVER_COUNT_50_64YR\nDRIVER_COUNT_65_74YR\nDRIVER_COUNT_75PLUS\nEST_HRS_CLOSED\nFATAL_COUNT\nHEAVY_TRUCK_COUNT\nHORSE_BUGGY_COUNT\nHOUR_OF_DAY\nILLUMINATION\nINJURY_COUNT\nINTERSECT_TYPE\nINTERSECTION_RELATED\nLANE_CLOSED\nLATITUDE\nLN_CLOSE_DIR\nLOCATION_TYPE\nLONGITUDE\nMAX_SEVERITY_LEVEL\nMCYCLE_DEATH_COUNT\nMCYCLE_SUSP_SERIOUS_INJ_COUNT\nMOTORCYCLE_COUNT\nMUNICIPALITY\nNONMOTR_COUNT\nNONMOTR_DEATH_COUNT\nNONMOTR_SUSP_SERIOUS_INJ_COUNT\nNTFY_HIWY_MAINT\nPED_COUNT\nPED_DEATH_COUNT\nPED_SUSP_SERIOUS_INJ_COUNT\nPERSON_COUNT\nPOLICE_AGCY\nPOSSIBLE_INJ_COUNT\nRDWY_SURF_TYPE_CD\nRELATION_TO_ROAD\nROAD_CONDITION\nROADWAY_CLEARED\nSCH_BUS_IND\nSCH_ZONE_IND\nSECONDARY_CRASH\nSMALL_TRUCK_COUNT\nSPEC_JURIS_CD\nSUSP_MINOR_INJ_COUNT\nSUSP_SERIOUS_INJ_COUNT\nSUV_COUNT\nTCD_FUNC_CD\nTCD_TYPE\nTFC_DETOUR_IND\nTIME_OF_DAY\nTOT_INJ_COUNT\nTOTAL_UNITS\nUNB_DEATH_COUNT\nUNB_SUSP_SERIOUS_INJ_COUNT\nUNBELTED_OCC_COUNT\nUNK_INJ_DEG_COUNT\nUNK_INJ_PER_COUNT\nURBAN_AREA\nURBAN_RURAL\nVAN_COUNT\nVEHICLE_COUNT\nWEATHER1\nWEATHER2\nWORK_ZONE_IND\nWORK_ZONE_LOC\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n3\n2020\n2\n39.9601\n-75.1794\n1343.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n13\n1\n0\n0\nNaN\n1\n39 57:36.245\n4.0\n0\n75 10:45.819\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n3\n68K01\n0\nNaN\n1\n9\nNaN\nN\nN\nNaN\n1\nNaN\n0\n0\n0\n0\n0\nN\n1332\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n7\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.179 39.960)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n4\n1\nNaN\n67\n3\n2020\n2\n39.9700\n-75.1631\n1250.0\n6\n0\n0\n0\n0\n0\n1\n0\n0\nNaN\n0\n0\n0.0\n12\n1\n2\n1\nNaN\n0\n39 58:11.995\nNaN\n0\n75 09:47.193\n8\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n4\n67508\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n3\n3\nN\n1250\n2\n2\n0\n0\n0\n2\n1\n3\n4\n0\n2\n3\nNaN\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.163 39.970)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\nNaN\n67\n2\n2020\n7\n39.9523\n-75.1878\n800.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n0\n0.0\n8\n1\n2\n0\nNaN\n2\n39 57:08.258\n4.0\n0\n75 11:16.213\n3\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67504\n0\nNaN\n6\n1\nNaN\nN\nN\nNaN\n0\nNaN\n2\n0\n0\n0\n0\nN\n800\n2\n2\n0\n0\n2\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.188 39.952)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n5\n1\nNaN\n67\n3\n2020\n2\n39.9558\n-75.1484\n1024.0\n6\n0\n0\n0\n0\n0\n0\n0\n0\nNaN\n0\n1\n0.0\n10\n1\n0\n0\nNaN\n0\n39 57:21.042\nNaN\n2\n75 08:54.233\n0\n0\n0\n0\n67301\n0\n0\n0\nN\n0\n0\n0\n2\n67501\n0\nNaN\n1\n1\nNaN\nN\nN\nNaN\n0\nNaN\n0\n0\n0\n0\n0\nNaN\n1024\n0\n2\n0\n0\n0\n0\n0\n3\n4\n0\n2\n3\n3.0\nN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.148 39.956)\n\n\n\n\n\n\n\n\nnearest_edge_column = crashes_nearest_edge\n\nNearestEdgedf = pd.DataFrame(nearest_edge_column, columns=[\"u\", \"v\", \"key\"])\n\nNearestEdgedf \n\n\n\n\n\n\n\n\nu\nv\nkey\n\n\n\n\n0\n8482829382\n7065714513\n0\n\n\n1\n109848091\n109848089\n0\n\n\n2\n1903608761\n109775193\n0\n\n\n3\n775424610\n775424581\n0\n\n\n4\n110416511\n110417392\n0\n\n\n...\n...\n...\n...\n\n\n3746\n110053344\n110053369\n0\n\n\n3747\n110453046\n110318202\n0\n\n\n3748\n8482829382\n7065714513\n0\n\n\n3749\n109791270\n109783164\n0\n\n\n3750\n110338911\n110000903\n0\n\n\n\n\n3751 rows × 3 columns\n\n\n\n\ngrouped_NearestEdgedf = NearestEdgedf.groupby([\"u\", \"v\"])\n\n\ncrash_count = grouped_NearestEdgedf.size().to_frame('size')\ncrash_count_df = crash_count.reset_index()\ncrash_count_df.rename(columns={\"size\": \"crash_count\"}, inplace=True)\ncrash_count_df\n\n\n\n\n\n\n\n\nu\nv\ncrash_count\n\n\n\n\n0\n109729474\n3425014859\n2\n\n\n1\n109729486\n110342146\n4\n\n\n2\n109729673\n109729699\n3\n\n\n3\n109729699\n109811674\n6\n\n\n4\n109729709\n109729731\n3\n\n\n...\n...\n...\n...\n\n\n875\n10270051289\n5519334546\n5\n\n\n876\n10660521817\n10660521823\n1\n\n\n877\n10674041689\n10674041689\n22\n\n\n878\n11144117753\n109729699\n3\n\n\n879\n11162290432\n110329835\n1\n\n\n\n\n880 rows × 3 columns\n\n\n\n\n\n1.10 Merge your edges GeoDataFrame and crash count DataFrame\nYou can use pandas to merge them on the u and v columns. This will associate the total crash count with each edge in the street network.\nTips: - Use a left merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge. - Use the fillna(0) function to fill in missing crash count values with zero.\n\nmerged_CPD = pd.merge(CPD_edges, crash_count_df, how=\"left\", on=[\"u\",\"v\"]).fillna(0)\nmerged_CPD\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\ncrash_count\n\n\n\n\n0\n109727439\n109911666\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n1\n109727448\n109727439\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n2\n109727448\n110034229\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3\n109727507\n110024052\n193364514\nTrue\nCarpenter Street\nresidential\nFalse\n53.208\nLINESTRING (-75.17196 39.93973, -75.17134 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n4\n109728761\n110274344\n672312336\nTrue\nBrown Street\nresidential\nFalse\n58.270\nLINESTRING (-75.17317 39.96951, -75.17250 39.9...\n25 mph\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3891\n11176163640\n11176163648\n1206065012\nFalse\n0\nliving_street\nFalse\n57.963\nLINESTRING (-75.16655 39.95896, -75.16633 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3892\n11176163640\n5808113442\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n37.086\nLINESTRING (-75.16655 39.95896, -75.16662 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3893\n11176163648\n5808113443\n613950538\nFalse\nAlexander Court\nliving_street\nFalse\n31.592\nLINESTRING (-75.16652 39.95909, -75.16646 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3894\n11176163648\n11176163640\n613950538\nFalse\nAlexander Court\nliving_street\nTrue\n14.738\nLINESTRING (-75.16652 39.95909, -75.16655 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3895\n11176163648\n11176163640\n1206065012\nFalse\n0\nliving_street\nTrue\n57.963\nLINESTRING (-75.16652 39.95909, -75.16630 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n\n\n3896 rows × 19 columns\n\n\n\n\n\n1.11 Calculate a “Crash Index”\nLet’s calculate a “crash index” that provides a normalized measure of the crash frequency per street. To do this, we’ll need to:\n\nCalculate the total crash count divided by the street length, using the length column\nPerform a log transformation of the crash/length variable — use numpy’s log10() function\nNormalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n\nNote: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero.\nAfter this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9.\n\nmerged_CPD['crash_count_real']= merged_CPD['crash_count'].where(merged_CPD['crash_count'] &gt; 0)\nmerged_CPD = merged_CPD.dropna(axis=0)\n\n\nmerged_CPD[\"crash_per_length\"] = merged_CPD['crash_count'] / merged_CPD[\"length\"]\nmerged_CPD[\"crash_index\"] = np.log10(merged_CPD[\"crash_per_length\"]) \ncrash_index = merged_CPD[\"crash_index\"]\nmerged_CPD[\"crash_index_normalized\"] = (crash_index - crash_index.min()) / (crash_index.max() - crash_index.min())\nmerged_CPD \n\nC:\\Users\\Owner\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\geopandas\\geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\ncrash_count\ncrash_count_real\ncrash_per_length\ncrash_index\ncrash_index_normalized\n\n\n\n\n14\n109729474\n3425014859\n62154356\nTrue\nArch Street\nsecondary\nFalse\n126.087\nLINESTRING (-75.14847 39.95259, -75.14859 39.9...\n25 mph\n2\n0\n0\n0\n0\n0\n0\n0\n2.0\n2.0\n0.015862\n-1.799640\n0.253014\n\n\n15\n109729486\n110342146\n[12169305, 1052694387]\nTrue\nNorth Independence Mall East\nsecondary\nFalse\n123.116\nLINESTRING (-75.14832 39.95333, -75.14813 39.9...\n0\n[2, 3]\n0\n0\n0\n0\n0\n0\n0\n4.0\n4.0\n0.032490\n-1.488255\n0.344052\n\n\n22\n109729673\n109729699\n30908050\nTrue\nNorth 5th Street\nsecondary\nFalse\n106.498\nLINESTRING (-75.14749 39.95686, -75.14748 39.9...\n0\n2\n0\n0\n0\n0\n0\n0\n0\n3.0\n3.0\n0.028170\n-1.550220\n0.325935\n\n\n25\n109729699\n109811674\n[1047787360, 424804073, 1047787359]\nTrue\nCallowhill Street\ntrunk\nFalse\n135.769\nLINESTRING (-75.14724 39.95779, -75.14739 39.9...\n35 mph\n5\n0\n0\n0\n0\n0\n0\n0\n6.0\n6.0\n0.044193\n-1.354649\n0.383113\n\n\n26\n109729709\n109811681\n12166069\nFalse\nWillow Street\nresidential\nFalse\n135.689\nLINESTRING (-75.14714 39.95860, -75.14788 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4.0\n4.0\n0.029479\n-1.530485\n0.331705\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3880\n10660521817\n10660521823\n1145560245\nFalse\n0\nresidential\nFalse\n78.888\nLINESTRING (-75.18307 39.94021, -75.18315 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1.0\n1.0\n0.012676\n-1.897011\n0.224546\n\n\n3882\n10674041689\n10674041689\n1116747275\nFalse\nArch Street\ntertiary\nFalse\n59.612\nLINESTRING (-75.17821 39.95627, -75.17826 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22.0\n22.0\n0.369053\n-0.432911\n0.652595\n\n\n3883\n10674041689\n10674041689\n1116747275\nFalse\nArch Street\ntertiary\nTrue\n59.612\nLINESTRING (-75.17821 39.95627, -75.17827 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22.0\n22.0\n0.369053\n-0.432911\n0.652595\n\n\n3886\n11144117753\n109729699\n[367831920, 50725362, 61756380, 12189165]\nTrue\nNorth 5th Street\nsecondary\nFalse\n470.874\nLINESTRING (-75.14814 39.95362, -75.14807 39.9...\n25 mph\n1\n0\n0\nyes\n0\n0\n0\n0\n3.0\n3.0\n0.006371\n-2.195783\n0.137196\n\n\n3888\n11162290432\n110329835\n30908343\nTrue\nNorth 19th Street\nresidential\nFalse\n9.734\nLINESTRING (-75.17044 39.95855, -75.17045 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1.0\n1.0\n0.102733\n-0.988291\n0.490223\n\n\n\n\n884 rows × 23 columns\n\n\n\n\n\n1.12 Plot a histogram of the crash index values\nUse matplotlib’s hist() function to plot the crash index values from the previous step.\nYou should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!\n\nfig, ax = plt.subplots()\nx = merged_CPD[\"crash_index_normalized\"]\nplt.hist(x, bins=100,),\nplt.xlabel(\"Normalized Crash Index\"),\nplt.ylabel(\"Total Number of Roads\"),\nplt.title(\"Crash Index Value\"),\nplt.grid(True),\nplt.show()\n\n\n\n\n\n\n1.13 Plot an interactive map of the street networks, colored by the crash index\nYou can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\nTip: if you use the viridis color map, try using a “dark” tile set for better constrast of the colors.\n\nimport folium\n\n\nm = merged_CPD.explore(\n    column=\"crash_index_normalized\",\n    tiles=\"Cartodb dark matter\",\n)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/streetsandwebscrap.html#part-2-scraping-craigslist",
    "href": "analysis/streetsandwebscrap.html#part-2-scraping-craigslist",
    "title": "Street Networks & Web Scraping",
    "section": "Part 2: Scraping Craigslist",
    "text": "Part 2: Scraping Craigslist\nIn this part, we’ll be extracting information on apartments from Craigslist search results. You’ll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text.\nFor reference on CSS selectors, please see the notes from Week 6.\n\nPrimer: the Craigslist website URL\nWe’ll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist.\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThere are three components to this URL.\n\nThe base URL: http://philadelphia.craigslist.org/search/apa\nThe user’s search parameters: ?min_price=1&min_bedrooms=1&minSqft=1\n\n\nWe will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n\n\nThe URL hash: #search=1~gallery~0~0\n\n\nAs we will see later, this part will be important because it contains the search page result number.\n\nThe Craigslist website requires Javascript, so we’ll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want.\n\n\n2.1 Initialize a selenium driver and open Craigslist\nAs discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n\nInitialize the selenium driver\nUse the driver.get() function to open the following URL:\n\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThis will give you the search results for 1-bedroom apartments in Philadelphia.\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\n\n\ndriver = webdriver.Chrome()\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_25652\\537431734.py:1: ResourceWarning: unclosed file &lt;_io.BufferedWriter name='nul'&gt;\n  driver = webdriver.Chrome()\n\n\n\nurl = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\"\ndriver.get(url)\n\n\n\n2.2 Initialize your “soup”\nOnce selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver’s page source\n\npropertySoup = BeautifulSoup(driver.page_source, \"html.parser\")\n\n\n\n2.3 Parsing the HTML\nNow that we have our “soup” object, we can use BeautifulSoup to extract out the elements we need:\n\nUse the Web Inspector to identify the HTML element that holds the information on each apartment listing.\nUse BeautifulSoup to extract these elements from the HTML.\n\nAt the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page.\n\nelement = propertySoup.select(\"li.cl-search-result\")\n\n\n#element\n\n\nlen(element)\n\n120\n\n\n\nprint(element[0].prettify())\nprint(len(element))\n\n&lt;li class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7678961565\" title=\"A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces.\"&gt;\n &lt;div class=\"gallery-card\"&gt;\n  &lt;div class=\"cl-gallery\"&gt;\n   &lt;div class=\"gallery-inner\"&gt;\n    &lt;a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/north-wales-fresh-take-on-living/7678961565.html\"&gt;\n     &lt;div class=\"swipe\" style=\"visibility: visible;\"&gt;\n      &lt;div class=\"swipe-wrap\" style=\"width: 6768px;\"&gt;\n       &lt;div data-index=\"0\" style=\"width: 376px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\"&gt;\n        &lt;span class=\"loading icom-\"&gt;\n        &lt;/span&gt;\n        &lt;img alt=\"A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces. 1\" src=\"https://images.craigslist.org/00Z0Z_g553W757i6Y_0ew09G_300x300.jpg\"/&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"1\" style=\"width: 376px; left: -376px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"2\" style=\"width: 376px; left: -752px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"3\" style=\"width: 376px; left: -1128px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"4\" style=\"width: 376px; left: -1504px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"5\" style=\"width: 376px; left: -1880px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"6\" style=\"width: 376px; left: -2256px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"7\" style=\"width: 376px; left: -2632px; transition-duration: 0ms; transform: translateX(376px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"8\" style=\"width: 376px; left: -3008px; transition-duration: 0ms; transform: translateX(-376px);\"&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-back-arrow icom-\"&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-forward-arrow icom-\"&gt;\n     &lt;/div&gt;\n    &lt;/a&gt;\n   &lt;/div&gt;\n   &lt;div class=\"dots\"&gt;\n    &lt;span class=\"dot selected\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n   &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/north-wales-fresh-take-on-living/7678961565.html\" tabindex=\"0\"&gt;\n   &lt;span class=\"label\"&gt;\n    A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces.\n   &lt;/span&gt;\n  &lt;/a&gt;\n  &lt;div class=\"meta\"&gt;\n   14 mins ago\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   &lt;span class=\"housing-meta\"&gt;\n    &lt;span class=\"post-bedrooms\"&gt;\n     1br\n    &lt;/span&gt;\n    &lt;span class=\"post-sqft\"&gt;\n     680ft\n     &lt;span class=\"exponent\"&gt;\n      2\n     &lt;/span&gt;\n    &lt;/span&gt;\n   &lt;/span&gt;\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   North Wales, Lansdale, Horsham, Doylestown, Ambler\n  &lt;/div&gt;\n  &lt;span class=\"priceinfo\"&gt;\n   $1,722\n  &lt;/span&gt;\n  &lt;button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n   &lt;/span&gt;\n  &lt;/button&gt;\n  &lt;button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n    hide\n   &lt;/span&gt;\n  &lt;/button&gt;\n &lt;/div&gt;\n&lt;/li&gt;\n\n120\n\n\n\n\n2.4 Find the relevant pieces of information\nWe will now focus on the first element in the list of 120 apartments. Use the prettify() function to print out the HTML for this first element.\nFrom this HTML, identify the HTML elements that hold:\n\nThe apartment price\nThe number of bedrooms\nThe square footage\nThe apartment title\n\nFor the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n\napt1 = element[0]\nspans = apt1.select('span')\nprint(f\"apartment price: {apt1.find('span',{'class' : 'priceinfo'}).text}\",end=\"\\n\")\n\nprint(f\"number of bedrooms: {apt1.find('span',{'class' : 'post-bedrooms'}).text[:1]}\",end=\"\\n\")\n\nprint(f\"square footage: {apt1.find('span',{'class' : 'post-sqft'}).text[:3]}\",end=\"\\n\")\n\nprint(f\"apartment title: {apt1.find('span',{'class' : 'label'}).text}\",end=\"\\n\")\n\napartment price: $1,722\nnumber of bedrooms: 1\nsquare footage: 680\napartment title: A fresh take on living: Explore our 1 BR, 680 Sq Ft spaces.\n\n\n\n\n2.5 Functions to format the results\nIn this section, you’ll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\nI’ve started the functions to format the values. You should finish theses functions in this section.\nHints - You can use string formatting functions like string.replace() and string.strip() - The int() and float() functions can convert strings to numbers\n\ndef format_bedrooms(bedrooms_string):\n    # Format the bedrooms string and return an int\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    \n    return int(bedrooms_string.replace(\"br\",\"\"))\n\n\ndef format_size(size_string):\n    # Format the size string and return a float\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    \n    return float(f\"{size_string[:-3]}\") \n\n\ndef format_price(price_string):\n    # Format the price string and return a float\n    # \n    # This will involve using the string.strip() function to \n    # remove unwanted characters\n    return float(price_string.replace(\"$\",\"\").replace(\",\",\"\"))\n\n\n\n2.6 Putting it all together\nIn this part, you’ll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments.\nWe can get a specific page by changing the search=PAGE part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2gallery0~0\nIn the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\nFill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment.\nAfter filling in the missing pieces and executing the code cell, you should have a Data Frame called results that holds the data for 600 apartment listings.\n\nNotes\nBe careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I’ve added a sleep() function to the for loop to wait 30 seconds between scraping requests.\nIf the for loop gets stuck at the “Processing page X…” step for more than a minute or so, your IP address is probably banned temporarily, and you’ll have to wait a few minutes before trying again.\n\nfrom time import sleep\n\n\nresults = []\n\n# search in batches of 120 for 5 pages\n# NOTE: you will get temporarily banned if running more than ~5 pages or so\n# the API limits are more leninient during off-peak times, and you can try\n# experimenting with more pages\nmax_pages = 5\n\n# The base URL we will be using\nbase_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n\n# loop over each page of search results\nfor page_num in range(1, max_pages + 1):\n    print(f\"Processing page {page_num}...\")\n\n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"#search={page_num}~gallery~0~0\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n    apts = soup.select(\"li.cl-search-result\")\n    print(\"Number of apartments = \", len(apts))\n\n    # loop over each apartment in the list\n    page_results = []\n    for apt in apts:\n\n        # YOUR CODE: the bedrooms string\n        bedrooms = format_bedrooms(apt.find('span',{'class' : 'post-bedrooms'}).text)\n\n        # YOUR CODE: the size string\n        size = format_size(apt.find('span',{'class' : 'post-sqft'}).text)\n\n        # YOUR CODE: the title string\n        title = apt.find('span',{'class' : 'label'}).text\n\n        # YOUR CODE: the price string\n        price = format_price(apt.find('span',{'class' : 'priceinfo'}).text)\n\n\n        # Format using functions from Part 1.5\n        # bedrooms = format_bedrooms(bedrooms)\n        # size = format_size(size)\n        # price = format_price(price)\n\n        # Save the result\n        page_results.append([price, size, bedrooms, title])\n\n    # Create a dataframe and save\n    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\nProcessing page 1...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 2...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 3...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 4...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 5...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\n\n\n\nresults.head(4)\n\n\n\n\n\n\n\n\nprice\nsize\nbedrooms\ntitle\n\n\n\n\n0\n1722.0\n680.0\n1\nA fresh take on living: Explore our 1 BR, 680 ...\n\n\n1\n850.0\n400.0\n1\nCozy and clean apartment\n\n\n2\n1905.0\n926.0\n2\n2 bedroom, 9' Ceilings w/ Crown Molding, West ...\n\n\n3\n1788.0\n521.0\n1\n1/bd, Conference Room, Fully Equipped Kitchens\n\n\n\n\n\n\n\n\n\n\n2.7 Plotting the distribution of prices\nUse matplotlib’s hist() function to make two histograms for:\n\nApartment prices\nApartment prices per square foot (price / size)\n\nMake sure to add labels to the respective axes and a title describing the plot.\n\nfig,ax = plt.subplots(1,1)\nax.hist(\n    results[\"price\"],\n    rwidth=0.65,\n)\nax.set_title(\"Philadelphia Rental Apartment Price Frequencies - Source: Craigslist\")\nax.set_xlabel(\"Listing Price ($)\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\n\nSide note: rental prices per sq. ft. from Craigslist\nThe histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia’s rents compare to the other most populous cities:\n\nSource\n\nfig,ax = plt.subplots(1,1)\nax.hist(\n    results[\"price\"]/results[\"size\"],\n    rwidth=0.65\n)\n\nax.set_title(\"Price per Sq. Ft. of Philadelphia Rental Apartments - Source: Craigslist\")\nax.set_xlabel(\"Listing Price ($/sq. ft.)\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\n\n\n\n2.8 Comparing prices for different sizes\nUse altair to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms.\nMake sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\nWith this sort of plot, you can quickly see the outlier apartments in terms of size and price.\n\nimport altair as alt\n\ncolormap = alt.Scale(\n    domain=[0,1,2,3,4,5],\n    range=[\n        \"steelblue\",\n        \"cornflowerblue\",\n        \"chartreuse\",\n        \"#F4D03F\",\n        \"#D35400\",\n        \"red\",\n    ])\n\n# Step 1: Initialize the chart with the data\nscatter = alt.Chart(results).mark_circle(size=50).encode(\n    x=alt.X('price:Q'),\n    y=alt.Y('size:Q'),\n    color=alt.Color('bedrooms:Q',sort=\"ascending\",scale=colormap),\n    tooltip=[\"title\", \"price\",\"size\",\"bedrooms\"],\n).properties(width=350,height=150)\n\nscatter.interactive()"
  }
]